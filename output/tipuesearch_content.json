{"pages":[{"title":"About","text":"I started this blog after teaching a course on Engineering Programming at Portland Community College (ENGR214). In the course, we used MATLAB. But this cost students $49 for a student version. As we strive to move towards open education resources, I wanted to reduce the cost for students to take the course. This led me to Python. My goal is to help reduce student cost and investigate how Python could be used in an engineering programming course instead of MATLAB. I earned my PhD in Material Science and Engineering from the University of Washington. Currently, I work as a Professor of Engineering and Engineering Technology at Portland Community College in Portland, OR. Teaching at a community college is my dream job. I have two inquisitive daughters and a wonderful and supportive wife. Coding downstairs in the den after the kids go to bed would not be possible without the my wife's gracious support. In my garage is sitting a '76 Scout II. It barely runs, is rusty all over and smells like a cheap oil change. I hope to get it on the road this summer. I play soccer and follow the Seattle Sounders and Portland Thorns. See what I'm doing NOW","tags":"pages","url":"pages/about.html"},{"title":"Book","text":"Coming in Fall 2018 Book: Python Programming for Undergraduate Engineers This books is for undergraduate engineers learning programming. It is focused on using Python and programming to solve engineering problems. Chapter List Preface Chapter 1: Orientation Chapter 2: Engineering 101 Chapter 3: Statics Chapter 4: Circuits I Chapter 5: Strengths Chapter 6: Circuits II Chapter 7: Dynamics Chapter 8: Circuits III Chapter 9: Capstone Appendix Pre-order will be available soon","tags":"pages","url":"pages/book.html"},{"title":"Now","text":"(what's this page about?) Learning Jupyter Hub SolidWorks Reading Fundamentals of Python: First Programs When the Dream Became Reality Listening Music Imagine Dragons - Believer (pandora) Podcasts Talk Python to Me Python Bytes Dataframed Watching Youtube Bleepin Jeep SOUP Classic Motoring Netflix Great British Baking Show","tags":"pages","url":"pages/now.html"},{"title":"Solving Two Equations for Two Unknowns and a Statics Problem with SymPy and Python","text":"SymPy (http://www.sympy.org) is a Python library for symbolic math . In symbolic math, symbols represent mathematical expressions. In a numerical calculation , the value of pi is stored as an estimate of pi , a floating point number close to 3.14... . In a symbolic math expression , the value of pi is an exact value. SymPy can be used to solve two equations for two unknowns. Consider the set of two equations containing two variables below: $$ x + y - 5 = 0 $$ $$ x - y + 3 = 0 $$ To solve this system of two equations for the two unknows $x$ and $y$, first the SymPy package needs to be imported. From the SymPy package, we'll use the functions symbols() , Eq() , and solve() . In [1]: import numpy as np from sympy import symbols , Eq , solve Next, we create two SymPy symbols objects, $x$ and $y$. Note, the string passed as an argument to the symbols() function, 'x y' , does not have any commas. The outputs of the symbols() function are the two symbols objects x and y . These outputs must be separated by a comma and are not surrounded by quotes. Be careful with syntax when you create SymPy symbols objects. In [2]: x , y = symbols ( 'x y' ) Now we define the two equations as SymPy equation objects using SymPy's Eq equation class. Equations in SymPy are assumed to be equal to zero. Both of our equations are equal to zero, so no modification is necessary before we pass the equations into Eq() . If the equations were not equal to zero, we would simply subtract the term on the right hand side of the equals sign from both sides of the equation. The subtraction opperation will result in an equation equal to zero. In [3]: eq1 = Eq ( x + y - 5 ) eq2 = Eq ( x - y + 3 ) After the symbols and equations are defined, we can use SymPy's solve() function to compute the value of $x$ and $y$. The first argument passed to the solve() function is a tuple of the two equations (eq1, eq2) . The second argument passed to the solve() function is a tuple of the variables we want to solve for (x, y) . In [4]: solve (( eq1 , eq2 ), ( x , y )) Out[4]: {x: 1, y: 4} The solution is stored in a Python dictionary. The dictionary keys are the symbols objects (the variables), and the dictionary values are the numerical solution. We can access the solutions in the dictionary using regular Python dictionary access syntax: dict[key] . In [5]: sol_dict = solve (( eq1 , eq2 ), ( x , y )) print ( f 'x = {sol_dict[x]} ' ) print ( f 'y = {sol_dict[y]} ' ) x = 1 y = 4 SymPy can be used to solve engineering problems. Consider the following engineering statics problem: GIVEN: A mass of 22 lbs is hung from a ring. The ring is supported by two cords, cord AC is 45 degrees above the horizontal to the left, and cord BC is 30 degrees above the horizontal to the right . $m$ = 22 lb $T_{AC}$ @ +45 degrees CW relative to -x-axis $T_{BC}$ @ +30 degress CCW relative to +x-axis FIND: The magnitude of tension in the cords AC and BC. ($T_{AC}$ and $T_{BC}$) SOLUTION: After constructing a Free Body Diagram of ring C, we see three forces acting on ring C: One force from cord AC, which we call $T_{AC}$. The second force is from cord BC, which we call $T_{BC}$. The third and final force acting on ring C is the force from cord CD, which is due to the box hanging straight down and equal to 22 lbs, which we call $m$. To solve for the magnitude of $T_{AC}$ and $T_{BC}$, we need to solve two equations for two unknowns. To accomplish this with SymPy , first we need to import sympy and the functions symbols , Eq and solve . In [6]: from sympy import symbols , Eq , solve Next, we'll define the symbolic math variables (that will be used in the equations) as SymPy symbols objects. Multiple symbolic math variables can be defined at the same time. Note the argument names (on the right-hand side of the assignment operator = ) need to be enclosed in quotes ' ' and separated by spaces, no commas. The object names (on the left-hand side of the assignment operator = ) are separated with commas, no quotes. In [7]: Tac , Tbc = symbols ( 'Tac Tbc' ) Next, two equations based on the sum of the forces in the x and y directions need to be defined. Assuming ring C is in static equilibrium: $$ \\Sigma \\vec{F} = 0 $$ $$ \\Sigma F_{x} = 0 $$ $$ \\Sigma F_{y} = 0 $$ The three forces operating on the ring are defined as: $$ {T_{ac}} = tension \\ in \\ cable \\ AC \\ $$ $$ \\vec{T_{ac}} = - T_{ac} cos(45)\\hat{i} + T_{ac} sin(45)\\hat{j} $$ $$ {T_{bc}} = tension \\ in \\ cable \\ BC $$ $$ \\vec{T_{bc}} = T_{bc} cos(30)\\hat{i} + T_{bc} sin(30)\\hat{j} $$ $$ \\vec{m} = 0 \\hat{i} - 22 \\hat{j} $$ Taking $\\Sigma F_{x} = 0$ (sum of the $\\hat{i}$ terms) produces: $$ - T_{ac} cos(45) + T_{bc} cos(30) = 0 $$ Taking $\\Sigma F_{y} = 0$ (sum of the $\\hat{j}$ terms) produces: $$ T_{ac} sin(45) + T_{bc} sin(30) - 22 = 0 $$ Our first equation, based on the sum of the forces in the x-direction (the $\\hat{i}$ terms) is: $$ - T_{ac} cos(45) + T_{bc} cos(30) = 0 $$ This equation can be represented as a Sympy equation object. Note the right-hand side of the equation is equal to 0 . Sympy equation objects are instantiated with expressions equal to zero. If the expression on the left-hand side of the equation was not equal to zero, we would simply subtract both sides of the equation by the term on the right-hand side of the equals sign, then use the resulting expression (equal to zero) to create the Sympy equation object. In [8]: eq1 = Eq ( - Tac * np . cos ( np . radians ( 45 )) + Tbc * np . cos ( np . radians ( 30 ))) print ( eq1 ) Eq(-0.707106781186548*Tac + 0.866025403784439*Tbc, 0) The second equation is based on the sum of the forces in the y-direction: $$ T_{ac} sin(45) + T_{bc} sin(30) - 22 = 0 $$ We'll define this second equation as a SymPy equation object as well: In [9]: eq2 = Eq ( Tac * np . sin ( np . radians ( 45 )) + Tbc * np . sin ( np . radians ( 30 )) - 22 ) print ( eq2 ) Eq(0.707106781186548*Tac + 0.5*Tbc - 22, 0) Finally, to solve the two equations for the two unknowns, $T_{ac}$ and $T_{bc}$, we use SymPy's solve() method. The first argument passed to solve() is a tuple of the equations we want to solve (eq1, eq2) , the second argument passed to solve() is a tuple of the variables we want to solve for (Tac, Tbc) . In [10]: solve (( eq1 , eq2 ),( Tac , Tbc )) Out[10]: {Tac: 19.7246603876972, Tbc: 16.1051177665153} The solution is saved in a Python dictionary. The dictionary keys are the variable names, and the dictionary values are the numeric solution. Note, when we access the solution dictionary, we need to use the SymPy symbol object names Tac and Tbc as the dictionary keys (as compared to using the strings 'Tac' and 'Tbc' as the dictionary keys). In [11]: sol_dict = solve (( eq1 , eq2 ), ( Tac , Tbc )) print ( f 'Tac = {round(sol_dict[Tac],1)} lb' ) print ( f 'Tbc = {round(sol_dict[Tbc],1)} lb' ) Tac = 19.7 lb Tbc = 16.1 lb Summary SymPy is a Python package for symbolic math. In this post, we solved a system of two equations for two unknows using SymPy . To do this, we created SymPy symbols objects and put these symbol objects into SymPy equation objects. We used SymPy's solve() method to calculate the solution. The solution is stored in a Python dictionary where the keys are the SymPy symbols objects and the values are the numeric solution.","tags":"sympy","url":"sympy-two-equations-for-two-unknows-and-statics-problem.html"},{"title":"Solving Equations and Writing Expressions with SymPy and Python","text":"SymPy (http://www.sympy.org) is a Python library for symbolic math . In symbolic math, symbols are used to represent mathematical expressions. An example of a symbolic math expression is below: $$ x&#94;{2} + y&#94;{2} = z $$ In the expression above, we have the variables $x$, $y$ and $z$. If we define a second symbolic math expression as: $$ x = a + b $$ We can substitute in $a + b$ for $x$. The resulting expression is: $$ (a + b)&#94;{2} + y&#94;{2} = z $$ $$ a&#94;{2} + 2ab + b&#94;{2} + y&#94;{2} = z $$ Solving for $y$ in terms of $a$,$b$ and $z$, results in: $$ y = \\sqrt{z - a&#94;{2} - 2ab - b&#94;{2}} $$ If we have numerical values for $z$, $a$ and $b$, we can use Python to calculate the value of $y$. However, if we don't have numerical values for $z$, $a$ and $b$, Python can also be used to rearrange terms of the expression and solve for the variable $y$ in terms of the other variables $z$, $a$ and $b$. Working with mathematical symbols in a programmatic way, instead of working with numerical values in a programmatic way, is called symbolic math. SymPy is a Python library for working with symbolic math. Before SymPy can be used, it needs to be installed. The installation of Sympy is accomplished using the Anaconda Prompt (or a terminal and pip ) with the command: > conda install sympy SymPy is included in the Anaconda distribution of Python. If you have the full Anaconda distribution, you will be notified that the SymPy library is already installed. Defining Variables in SymPy To define symbolic math variables with SymPy , first import the symbols() function from the SymPy module: In [1]: from sympy import symbols Symbolic math variables are declared using SymPy's symbols() function. Note, the arguments passed to the symbols() function (symbol names) are separated by a space, no comma, and surrounded by quotes. The output of the symbols() function are SymPy symbols objects. These output objects are separated by commas with no quotation marks. In [2]: x , y = symbols ( 'x y' ) Now that the symbols x and y are instantiated, a symbolic math expression using x and y can be created. A symbolic math expression is a combination of symbolic math variables with numbers and mathematical operators, such as + , - , / and * . The standard Python rules for working with numbers apply in SymPy symbolic math expressions. In [3]: expr = 2 * x + y Use the .subs() method to insert a numerical value into a symbolic math expression. The first argument of the .subs() method is the symbols object (the variable) and the second argument is the numerical value. In the expression above: $$ 2x + y $$ If we substitute: $$ x = 2 $$ The resulting expression is: $$ 2(2) + y $$ $$ 4 +y $$ In [4]: expr . subs ( x , 2 ) Out[4]: y + 4 The .subs() method does not replace variables in place, .subs() only completes a one-time substitution. If we call expr after the .subs() method is applied, the original expr expression is returned. In [5]: expr Out[5]: 2*x + y In order to make the substitution permanent, a new expression object needs to be assigned to the output of the .subs() method. In [6]: expr = 2 * x + y expr2 = expr . subs ( x , 2 ) expr2 Out[6]: y + 4 SymPy variables can also be substituted into SymPy expressions In [7]: x , y , z = symbols ( 'x y z' ) expr = 2 * x + y expr2 = expr . subs ( x , z ) expr2 Out[7]: y + 2*z More complex substitutions can also be completed. Consider the following: $$ 2x + y $$ Substitute in: $$ y = 2x&#94;2 + z&#94;{-3} $$ Results in: $$ 2x + 2x&#94;2 + z&#94;{-3} $$ In [8]: x , y , z = symbols ( 'x y z' ) expr = 2 * x + y expr2 = expr . subs ( y , 2 * x ** 2 + z ** ( - 3 )) expr2 Out[8]: 2*x**2 + 2*x + z**(-3) A more practical example could involve a large expression and several variable substitutions. $$ n_0e&#94;{-Q_v/RT} $$ $$ n_0 = 3.48 \\times 10&#94;{-6} $$ $$ Q_v = 12,700 $$ $$ R = 8.31 $$ $$ T = 1000 + 273 $$ In [9]: from sympy import symbols , exp n0 , Qv , R , T = symbols ( 'n0 Qv R T' ) expr = n0 * exp ( - Qv / ( R * T )) Multiple SymPy subs() methods can be chained together to substitue multiple variables in one line of code. In [10]: expr . subs ( n0 , 3.48e-6 ) . subs ( Qv , 12700 ) . subs ( R , 8031 ) . subs ( T , 1000 + 273 ) Out[10]: 3.48e-6*exp(-12700/10223463) To evaluate an expression as a floating point number (get a numerical answer out), use Sympy's .evalf() method. In [11]: expr2 = expr . subs ( n0 , 3.48e-6 ) . subs ( Qv , 12700 ) . subs ( R , 8031 ) . subs ( T , 1000 + 273 ) expr2 . evalf () Out[11]: 3.47567968697765e-6 Defining Equations in Sympy We can define equations in SymPy using symbolic math variables. Equations in SymPy are different than expressions in SymPy . An expression does not have equality. An equation has equality. An equation is equal to something. In [1]: from sympy import symbols , Eq , solve In [2]: x , y = symbols ( 'x y' ) SymPy equations are instantiated as an object of the Eq class. After SymPy symbols are created, the symbols can be passed into an equation object. Let's create the equation: $$ 2x + y - 1 = 0 $$ In [3]: eq1 = Eq ( 2 * x - y - 1 ) Now let's create a second equation: $$ x + y - 5 = 0 $$ In [4]: eq2 = Eq ( x + y - 5 ) To solve the two equations for the two variables x and y , we'll use SymPy's solve() function. The solve() function takes two arguments, a tuple of the equations (eq1, eq2) and a tuple of the variables to solve for (x, y) . In [5]: sol = solve (( eq1 , eq2 ),( x , y )) sol Out[5]: {x: 2, y: 3} The SymPy solution object is a Python dictionary. The keys are the SymPy variable objects and the values are the numerical values these variables correspond to. In [6]: print ( f 'The solution is x = {sol[x]} , y = {sol[y]} ' ) The solution is x = 2, y = 3 Summary In this post, we looked at a Python package for symbolic math called SymPy . Using symbolic math, we can define expressions and equations exactly in terms of symbolic variables. We reviewed how to create a SymPy expression and substitue values and variables into the expression. Then we created to SymPy equation objects and solved two equations for two unknowns using SymPy's solve() function.","tags":"sympy","url":"sympy-expressions-and-equations.html"},{"title":"Building an IoT Server with flask and Python - Part 6 - upload code to ESP8266-based WiFi weather stations","text":"This is the sixth part of a series of posts about building an Internet of Things (IoT) server with flask , Python and ESP8266 microcontrollers. In this post, we'll add some code to our ESP8266-based weather stations. The code we upload to the ESP8266 microcontrollers programs the WiFi weather stations to measure the temperature. After the ESP8266-based weather stations measure the temperature, the microcontroller executes a GET request to our flask IoT server web API. Table of contents: Introduction Hardware Setup Upload firmware Download the latest Micropython firmware .bin file Install the SiLabs driver for the Adafruit Feather Huzzah ESP8266 Connect the Adafruit Feather Huzzah ESP8266 board to the laptop Determine which serial port the Feather Huzzah is connected to Run esptool to upload the .bin file to the Feather Huzzah Construct .py files Upload .py files Test ESP8266-based weather stations Upload main.py Summary Introduction In the last post, we added a database to our flask IoT server. Each time our flask IoT server web API is hit with a valid URL, the data contained in the URL is saved as a record in a sqlite3 database on the server. Each time we browse to the main page of the flask IoT server site, we see the most recent temperature posted. The posted temperature is pulled from the sqlite3 database. In this post, we are going to create a couple new .py files and upload the .py files to the ESP8266-based WiFi weather stations. These .py files enable the ESP8266-based WiFi weather stations to measure the temperature, then post the temperature to our flask IoT server. Hardware Setup Before we upload any new code to the ESP8266-based weather stations, let's review the hardware setup. Below is a schematic of the ESP8266-based weather stations. The schematic shows an ESP8266 microcontroller (an Adafruit Feather Huzzah ESP8266 ) connected to a temperature sensor ( MCP9808 ) with jumper wires. Upload firmware If you are following along with this series, you might remember the ESP8266-based WiFi weather station hardware and software setup in the first post of the series . In case the Feather Huzzah ESP8266 microcontroller doesn't have an up-to-date version of Micropython on it, below are instructions detailing how to upload the Micropython firmware to the board. Download the latest Micropython firmware .bin file Go to GitHub and download the latest .bin firmware file. Move the .bin firmware file to a new micropython directory. The .bin firmware file is the version of Micropython that runs on the Adafruit Feather Huzzah ESP8266. Install the SiLabs driver for the Adafruit Feather Huzzah ESP8266 Before we can connect the Adafruit Feather Huzzah to the computer, we need a specific driver installed. For my Windows 10 laptop to see the Adafruit Feather Huzzah board, the CP210x USB to UART Bridge VCP driver needs to be downloaded from SiLabs and installed. Connect the Adafruit Feather Huzzah ESP8266 board to the laptop Use a micro-USB cable (the same kind of cable that charges many mobile phones) to connect the Feather Huzzah to the computer. Make sure that the micro-USB cable is a full USB data cable and not just a simple power cable. I had trouble getting the Feather Huzzah to work, and it turned out the reason was my micro-USB cable was only a charging cable. Charge-only cables cannot transfer data. Determine which serial port the Feather Huzzah is connected to Use Windows Device Manager to determine which serial port the Feather Huzzah board is connected to. We'll need the serial port as one of the parameters when we upload the .bin firmware file on the board. Look for something like Silicon Labs CP210x USB to UART Bridge (COM4) in the Ports (COM & LPT) menu. The USB to UART bridge is the Feather Huzzah ESP8266 microcontroller board. CP210x refers to the chip that handles serial communication on the Feather Huzzah, not the ESP8266 chip itself. Make a note of the number after (COM ) . It often comes up as (COM4) but it may be different on your computer. Run esptool to upload the .bin file to the Feather Huzzah On a local computer (not the server), open the Anaconda Prompt or a terminal and cd into the directory with the .bin firmware file. The .bin firmware file is called something like esp8266-20171101-v1.9.3.bin . Create and activate a new conda virtual environment and install esptool into the environment. > conda create -n micropython python=3.7 > conda activate micropython > (micropython) pip install esptool Before we write the .bin firmware file to the ESP8266, we'll first erase the flash memory on the little microcontroller using the esptool erase_flash command. Make sure to specify the --port you found in the Windows Device Manager. In my case the port was COM4 . > (micropython) esptool --port COM4 erase_flash Now it's time to write the .bin firmware file to the flash memory on the ESP8266 board using the esptool write_flash command. Make sure to use the exact .bin firmware file name. The .bin firmare filename is easy to mistype. --port has to be set as the port you found in the Windows Device Manager. ---baud is the baud rate (upload speed). I found that --baud 460800 worked, but you could also specify --baud 115200 , which is slower. The upload time was a matter of seconds with either baud rate. The 0 after --flash_size=detect means we want the firmware written at the start of the flash memory (the 0 th position) on the board. An issue I ran into was that I tried to use the command esptool.py instead of esptool as shown on the Micropython docs . The documentation for Micropython on the ESP8266 specifies the command esptool.py (including the .py file extension). The command esptool.py did work on my Windows 10 machine. Omitting the .py file extension, and running esptool worked instead. > (micropython) esptool --port COM4 --baud 460800 write_flash --flash_size=detect 0 esp8266-20171101-v1.9.3.bin Construct .py files Now that Micropython is loaded on the ESP8266 microcontroller, we'll construct a couple of .py files to load onto the board. The first file is wifitools.py . This module contains a couple of helper functions that allow the ESP8266 to connect to a WiFi network and make GET requests. # wifitools.py import urequests #https://docs.micropython.org/en/v1.8.6/esp8266/esp8266/tutorial/network_basics.html def connect ( SSID , password ): import network sta_if = network . WLAN ( network . STA_IF ) if not sta_if . isconnected (): print ( 'connecting to network...' ) sta_if . active ( True ) sta_if . connect ( SSID , password ) while not sta_if . isconnected (): pass print ( 'network config:' , sta_if . ifconfig ()) def getmac (): import network import ubinascii return ubinascii . hexlify ( network . WLAN () . config ( 'mac' ), ':' ) . decode () def flaskiot_post ( API_key , mac_address , field , data ): if not isinstance ( data , str ): data = str ( data ) if not isinstance ( field , str ): field = str ( field ) # https://freetemp.org/update/API_key=ASCIISTR/mac=6c:rf:7f:2b:0e:g8/field=1/data=72.3 base_url = 'https://freetemp.org/update' api_key_url = '/API_key=' + API_key mac_url = '/mac=' + mac_address field_url = '/field=' + field data_url = '/data=' + str ( data ) url = base_url + api_key_url + mac_url + field_url + data_url print ( url ) response = urequests . get ( url ) print ( response . text ) The second file MCP9808.py , contains a function to read the temperature off of the MCP9808 temperature sensor. # MCP9808.py # Functions for the MCP9808 temperature sensor # https://learn.adafruit.com/micropython-hardware-i2c-devices/i2c-master def readtemp (): import machine i2c = machine . I2C ( scl = machine . Pin ( 5 ), sda = machine . Pin ( 4 )) byte_data = bytearray ( 2 ) i2c . readfrom_mem_into ( 24 , 5 , byte_data ) value = byte_data [ 0 ] << 8 | byte_data [ 1 ] temp = ( value & 0xFFF ) / 16.0 if value & 0x1000 : temp -= 256.0 return temp The third file, config.py , contains the API key and mac address the ESP8266 uses in the GET requests to send a valid URL to our flask IoT server. config.py also contains the SSID and WiFi password of the wireless network. The constants API_key and mac should be set to the values used in the server script flaskapp.py . Make sure to add this file to .gitignore to keep it out of version control. # config.py # API keys, passwords, mac address # keep out of version control #api_key = config.API_KEY API_KEY = 'TGS894F' #ssid = config.SSID SSID = 'My WiFi Network' #password = config.WIFI_PASSWORD WIFI_PASSWORD = 'my_wifi_password' MAC_ADDRESS = '6c:ef:7r:3b:9d:e8' The fourth file - run.py , is a script with one primary function. The function programs the ESP8266 to: connect to the WiFi network read the temperature off of the MCP9808 temperature sensor try to post the temperature to our flask IoT server wait 1 minute # run.py # ESP8266 Feather Huzzah Weather Station, connected to flask IoT server import wifitools import MCP9808 import time import config def main (): ssid = config . SSID password = config . WIFI_PASSWORD api_key = config . API_KEY mac_address = config . MAC_ADDRESS field = '1' wifitools . connect ( ssid , password ) time . sleep ( 5 ) for i in range ( 60 * 8 ): data = MCP9808 . readtemp () try : wifitools . flaskiot_post ( api_key , mac_address , field , data ) except : pass time . sleep ( 60 ) Upload .py files We'll use a tool called ampy to upload the .py files to the board. Make sure ampy is installed in the virtual environment you're using. > conda activate micropython > (micropython) pip install ampy To upload the .py files, make sure the board is plugged into the computer with a USB data cable and use the ampy put command. Note the --port is specified. > (micropython)$ ampy --port COM4 put MCP9808.py > (micropython)$ ampy --port COM4 put wifitools.py > (micropython)$ ampy --port COM4 put config.py > (micropython)$ ampy --port COM4 put run.py > (micropython)$ ampy --port COM4 ls > boot.py wifitools.py MCP9808.py config.py run.py Test ESP8266-based weather stations Connect the ESP8266 to the local computer with a micro-USB cable. Use PuTTY to open up the Micropython REPL and try to run the main script in run.py >>> import run >>> run.main() The temperature is measured once a minute and posted to our flask IoT server. Open up a web browser to the server's main page and view the most recent data point. https://mydomain.com Upload main.py Now that we know our ESP8266-based WiFi weather stations are working correctly, we'll upload a main.py file to the board and start recording temperatures and sending the temperature to our flask IoT server web API. The main.py script runs automatically when the ESP8266 is powered up. The simple script contains an import and a line of code to run the main() function in run.py . # main.py # runs after boot.py import run try : run . main () except : print ( \"main() function in run.py could not be executed\" ) After the main.py script is constructed, upload it onto the ESP8266 with ampy . > (micropython) ampy --port COM4 put main.py > (micropython) ampy --port COM4 ls wifitools.py MCP9808.py config.py main.py run.py Now for the big payoff. Plug the ESP8266 into power, but disconnected from the local computer. Go to the main page on the flask-IoT server and see the temperature measured once a minute. https:mydomain.com Summary It works! We have a working Internet of Things (IoT) server that has a working web API that ESP8266-based WiFi weather stations can post to. Building the flask IoT server was a big project. It is great to have a set of WiFi weather stations that post the temperature to a server. I can view the temperature on my phone from anywhere with a cell phone connection. The ESP8266-based WiFi weather stations can be plugged in anywhere with power (or run on a battery) within my WiFi network. Using two of the stations, I can see the temperature outside the house and the temperature inside the house from anywhere.","tags":"flask","url":"flask-iot-server-upload-code-to-esp8266.html"},{"title":"Building an IoT Server with flask and Python - Part 5 Adding a Database","text":"This is the fifth part of a series of posts about building an Internet of Things (IoT) server with flask , Python and ESP8266 microcontrollers. In this post, we'll add a sqlite3 database to our flask IoT server to store all the temperature data points that come in from our ESP8266-based WiFi weather stations. We will also build out the main page of the flask IoT server site to display the most recent data points pulled from the database. Table of contents: Introduction Why a sqlite3 database? Database design Prototype the sqlite3 database Add the sqlite3 database to the server Update the main webpage with the newest database entry Restart the server Summary Next steps Introduction In the last post, we built in some validation for our flask IoT server web API so that only certain API keys and mac addresses are allowed. We also used Python's datetime module to add a time stamp to each data point as it comes into our flask IoT server. This is great, but it would be really to save every data point that comes in. Right now, when a new data point comes into our server, the previous data point is erased. There are a couple of ways we could store the temperature data that comes in from our ESP8266-based WiFi weather stations: Save the data in a text file. Append a new line to the text file for each new data point. Save the data in a .csv file. Add a new line for each data point, separate fields on each line with a comma or tab. Save the data in a pandas dataframe. Store each data point as a row in the dataframe. Use a database to store the data. Each data point saved as a record in the database. Of the four options above, I decided to use a sqlite3 database to store the data coming in from the flask IoT server web API. Why a sqlite3 database? Why use a sqlite3 database? One of the key reasons is that the sqlite3 module is part of the Python Standard Library. We don't have to install any external packages to use sqlite3 . A sqlite3 database is also light-weight and won't take up a lot of space on our server. But the real reason I choose to use sqlite3 is that the library has good documentation and I can build off the sqlite3 examples of others. Database design Before adding any data to the database, we need to think a little bit about database design. Our sqlite3 database is going to be a pretty simple database. I think of the database itself as a Microsoft Excel workbook, the whole Microsoft Excel .xlsx file. We will only employ one table in our sqlite3 database. I think a database table is like a sheet or tab in a Microsoft Excel file. Each data point from the WiFi weather stations will represent one record in the database. I think of a record as one row in a Microsoft Excel file. The web API we built brings in a couple of identifiers with each data point. Based on a valid URL such as: https://mydomain.com/update/API_key=PHDNGI2345/mac=6c:rf:7f:2b:0e:g8/field=1/data=72.3 In the URL above we've provided: update (to tell the flask IoT server to save the data point, not just serve a webpage) API_key = PHDNGI2345 (to identify the user) mac = 6c:rf:7f:2b:0e:g8 (to identify the ESP8266-based WiFi weather station) field = 1 (to specify a temperature data point, not a humidity data point) data = 72.3 (to specify the temperature is 72.3 degrees) Our database needs to be able to save these four fields: API_key mac field data As well as these two additional fields: date and time some sort of primary key that uniquely identifies each record Two example records in our database might look like: primary_key API_key mac field data date_time 1 PHDNGI2345 6c:rf:7f:2b:0e:g8 1 72.3 2018-09-10 08:23:45 PM 2 PHDNGI2345 6c:rf:7f:2b:0e:g8 1 83.2 2018-09-11 09:45:01 AM Prototype the sqlite3 database I didn't have a lot of experience building or using databases before this flask IoT server project. Before I started coding, I tried out a couple of sqlite3 commands in a jupyter notebook. To create a new database, we first import sqlite3 and then instantiate a new database object with the sqlite3.connect() method. In [1]: import sqlite3 db = sqlite3 . connect ( \"name_database.db\" ) Next, we connect to the database with the sqlite3.connect() method and create a connection object called conn . Then, from the connection object conn , we create a cursor object called cur . The cursor object executes the database commands. The commands the cursor object cur executes are written in a database query language. Learning database query language is sort of like learning a whole new programming language. I am still note really familiar with the database language query commands or syntax. Before we can add records to the database, we need to create a table in the database. In [2]: # create a database called name_database.db # add one table to the database called names_table # add columns to the database table: Id, first_name, last_name, age conn = sqlite3 . connect ( 'name_database.db' ) cur = conn . cursor () cur . execute ( \"\"\"CREATE TABLE IF NOT EXISTS names_table ( Id INTEGER PRIMARY KEY AUTOINCREMENT, first_name text, last_name text, age integer )\"\"\" ) conn . commit () cur . close () conn . close () db . close () Now to add a new record to the database, we need to: connect to the database, creating a connection object conn create a cursor object cur based on the connection object execute commands on the cursor object cur to add a new record to the database commit the changes to the connection object conn close the cursor object close the connection object In [3]: conn = sqlite3 . connect ( 'name_database.db' ) cur = conn . cursor () cur . execute ( \"INSERT INTO names_table VALUES(:Id, :first_name, :last_name, :age)\" , { 'Id' : None , 'first_name' : 'Gabriella' , 'last_name' : 'Louise' , 'age' : int ( 8 ) }) conn . commit () cur . close () conn . close () Now let's see if we can retrieve the record we just added to the database. In [4]: conn = sqlite3 . connect ( 'name_database.db' ) cur = conn . cursor () cur . execute ( \"SELECT first_name, last_name, age, MAX(rowid) FROM names_table\" ) record = cur . fetchone () print ( record ) cur . close () conn . close () ('Gabriella', 'Louise', 8, 1) Let's add another record to the database In [5]: conn = sqlite3 . connect ( 'name_database.db' ) cur = conn . cursor () cur . execute ( \"INSERT INTO names_table VALUES(:Id, :first_name, :last_name, :age)\" , { 'Id' : None , 'first_name' : 'Maelle' , 'last_name' : 'Levin' , 'age' : int ( 5 ) }) conn . commit () cur . close () conn . close () And again let's see the most recent record: In [6]: conn = sqlite3 . connect ( 'name_database.db' ) cur = conn . cursor () cur . execute ( \"SELECT first_name, last_name, age, MAX(rowid) FROM names_table\" ) record = cur . fetchone () print ( record ) cur . close () conn . close () ('Maelle', 'Levin', 5, 2) Add the sqlite3 database to the server OK- after playing around with sqlite3 , let's add some code to our flask IoT server. Let's code in a database connection, cursor object creation and record execution. This code belongs in the \"/update/...\" route of the flaskapp.py file. $ cd ~ $ cd flaskapp $ nano flaskapp.py Within the flaskapp.py file, we'll add the database code. # flaskapp.py @app.route ( \"/update/API_key=<api_key>/mac=<mac>/field=<int:field>/data=<data>\" , methods = [ 'GET' ]) def write_data_point ( api_key , mac , field , data ): if ( api_key == API_KEY and mac == MAC_ADDRESS ): conn = sqlite3 . connect ( 'data.db' ) c = conn . cursor () t = datetime . datetime . now ( tz = pytz . utc ) date_time_str = t . isoformat () c . execute ( \"INSERT INTO data VALUES(:Id, :API_key, :date_time, :mac, :field, :data)\" , { 'Id' : None , 'API_key' : api_key , 'date_time' : date_time_str , 'mac' : mac , 'field' : int ( field ), 'data' : round ( float ( data ), 4 )}) conn . commit () c . close () conn . close () return render_template ( \"showrecent.html\" , data = data , time_stamp = date_time_str ) else : return render_template ( \"403.html\" ) At the top of this main flaskapp.py script, we'll also include a couple of lines to create the database when the flask app starts: # flaskapp.py if not os . path . isfile ( 'data.db' ): conn = sqlite3 . connect ( 'data.db' ) c = conn . cursor () c . execute ( \"\"\"CREATE TABLE data ( Id INTEGER PRIMARY KEY AUTOINCREMENT, API_key text, date_time text, mac text, field integer, data real )\"\"\" ) conn . commit () conn . close () Update the main webpage with the newest database entry Next, we'll update the main page of the flask IoT server, the home or \"/\" route. #flaskapp.py @app.route ( \"/\" ) def index (): conn = sqlite3 . connect ( 'data.db' ) c = conn . cursor () c . execute ( \"SELECT data, date_time, MAX(rowid) FROM data WHERE field=?\" , ( '1' ,)) row1 = c . fetchone () c . execute ( \"SELECT data, date_time, MAX(rowid) FROM data WHERE field=?\" , ( '2' ,)) row2 = c . fetchone () c . close () conn . close () data1 = str ( round (( float ( row1 [ 0 ]) * 1.8 ) + 32 )) data2 = str ( round (( float ( row2 [ 0 ]) * 1.8 ) + 32 )) time_str1 = row1 [ 1 ] t1 = dateutil . parser . parse ( time_str1 ) t_pst1 = t1 . astimezone ( pytz . timezone ( 'US/Pacific' )) time_stamp1 = t_pst1 . strftime ( '%I:%M:%S %p %b %d , %Y' ) time_str2 = row2 [ 1 ] t2 = dateutil . parser . parse ( time_str2 ) t_pst2 = t2 . astimezone ( pytz . timezone ( 'US/Pacific' )) time_stamp2 = t_pst2 . strftime ( '%I:%M:%S %p %b %d , %Y' ) return render_template ( \"showdoubletemp.html\" , data1 = data1 , time_stamp1 = time_stamp1 , data2 = data2 , time_stamp2 = time_stamp2 ) Restart the server Restart the flask IoT server with the following commands: $ sudo systemctl stop flaskapp $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # ctrl-c to exit Let's try uploading a datapoint to the server using a valid URL. This GET request from a web browser will save a temperature data point in the database. https://mydomain.com/update/API_key=DTLZ3LBY/mac=5m:ct:7f:3b:0d:a8/field=1/data=30 Now let's upload another data point, this time using field=2 and data=15.2 . https://mydomain/update/API_key=DTLZ3LBY/mac=5m:ct:7f:3b:0d:a8/field=2/data=15.2 Now let's browse to the home page of the server and view the temperatures which were saved to the database. https://mydomain.com Summary It works! We have a working sqlite3 database. Each time the web API is hit with a valid URL from a web browser, a data point is saved as a record in the database. Each time we go to the main page of the flask IoT server site, we see the most recent temperatures posted. Next steps In the next post, we'll upload new .py files to our ESP8266-based WiFi weather stations. This will give the ESP8266-based WiFi weather stations the ability to post temperature data to our flask IoT server.","tags":"flask","url":"flask-iot-server-database.html"},{"title":"Building an IoT Server with flask and Python - Part 4 Validation and Timestamps","text":"This is the fourth part of a series of posts about building an Internet of Things (IoT) server with flask , Python and ESP8266 microcontrollers. In the last post of the series, we reviewed how to build a web API with flask which accepts temperature measurements. In this post, we'll build in some validation to our web API so that only certain API keys and mac addresses are allowed. We will also use Python's datetime module to time stamp each data point as it comes in. Table of contents: Introduction Why data validation? Validating incoming URLS Adding time stamps Restart the flask app and view the changes Summary Next steps Introduction Now that we have a web API, we can make GET requests using a web browser and see the output in the web page flask sends back. What's not to like? Well right now any string added to the URL specified by our web API will get through. We don't want just any WiFi weather station to upload data. What we need is some data validation. Why data validation? A web API is a web-based Application Programming Interface. That's a fancy way of saying a server that saves input or produces output based on the URL someone types into their web browser. An example URL that our flask IoT web server accepts is: https://mydomain.com/update/API_key=ASCIISTR/mac=6c:rf:7f:2b:0e:g8/field=1/data=72.3 In the URL above we've provided: update (to tell the IoT server to save the data point, not just serve a webpage) API_key = ASCIISTR (to identify the user) mac = 6c:rf:7f:2b:0e:g8 (to identify the ESP8255-based WiFi weather station) field = 1 (to specify this is a temperature data point, not a humidity data point) data = 72.3 (to specify the temperature is 72.3 degrees) Right now, any string following API_key= and mac= can be inserted into the URL. This allows anyone with an internet connection upload data to the server and potentially upload malicious code to the server. Since an arbitrary string can be inserted as API_key or mac , that arbitrary string will be read by our flask IoT server. To build in a bit more security into our server, we'll utilize a form of data validation . This means that our flask IoT server will only accept certain API_key= and mac= strings as part of a web API URL. Validating incoming URLS The data coming into our IoT server is only from two specific from ESP8266-based WiFi weather stations. There are a couple unique aspects to each of the ESP8266-based WiFi weather stations. A user: Each WiFi weather station has a user. In this case the user is me. I can set a unique API key that only I know. A mac address: A mac address is a unique address assigned to each piece of hardware. Each of the ESP8266-based WiFi weather stations has a different mac address. field: The ESP8266-based WiFi weather stations have the capability to output temperature and humidity. Right now we are just going to deal with temperature, but is will be nice to have an extra field available for multiple data outputs from the same device. There will be a limit to the number of fields one device can send. We'll limit the fields to 1-9 data: The temperature data that comes out of each ESP8266-based WiFi weather station. This is a float, but can be limited by length. If we put these 4 identifiers as part of our URL, our IoT server will provide the functionality the WiFi weather stations need. Below is the general form of a web API url that is valid: https://mydomain.com/update/API_key=GTW89NF3/mac=6c:rf:7f:2b:0e:g8/field=1/data=72.3 In the URL above we've provided: An API_key = GTW89NF3 A mac = mac=6c:rf:7f:2b:0e:g8 field = 1 data = 72.3 Now we need to program our flask IoT server to only accept URL's like the URL above, with a set of unique API_key's, mac addresses and limits on the field and data portions of the URL. On the server, create a new file called config.py . This file will contain the API_key and mac address that is acceptable. It is a good practice to keep these sorts of private keys in a separate file and out of version control. Make sure to add config.py to the .gitignore file if you are using git. #config.py API_key = GTW89NF3 mac = mac = 6 c : rf : 7 f : 2 b : 0 e : g8 Now, on the server, we'll modify the flaskapp.py file's imports section to import the API key and mac address strings from config.py . We can import variables as well as functions and classes from separate .py files. #flaskapp.py from flask import Flask , render_template , request from config import API_KEY , MAC_ADDRESS Now we'll modify the @app.route section of flaskapp.py : @app.route ( \"/update/API_key=<api_key>/mac=<mac>/field=<int:field>/data=<data>\" , methods = [ 'GET' ]) def write_data_point ( api_key , mac , field , data ): if ( api_key == API_KEY and mac == MAC_ADDRESS ): return render_template ( \"showrecent.html\" , data = data ) else : return render_template ( \"403.html\" ) A new template, 403.html that shows an error message, needs to be created in the templates directory. <!-- /templates/403.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > read temp </ title > <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" integrity = \"sha384-Tc5$> </head> <body> <div class=\" container-fluid \" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > API Call Invalid </ h1 > < p class = \"lead\" > The URL entered does not contain a valid API key or mac address </ p > < hr class = \"my-4> <p class=\" lead \" > < a class = \"btn btn-primary btn-lg\" href = \"/\" role = \"button\" > TO HOME </ a > </ p > </ div > </ div > </ body > </ html > Adding time stamps Our web API accepts data points, but it would be nice to see a time stamp of when the temperature was measured. With a time stamp connected to each data point, we can show a webpage with the temperature and when it was measured. Working with time in Python can be a little confusing. It seems like a simple concept, but times and dates are actually pretty complicated. Which time zone? Save seconds or minutes? What format will the time be in?. We need to answer these questions before building the code to record the time stamp. The way I want the time and date to look on a website our flask IoT server produces is: 05:18:48 PM Aug 17, 2018 Where 05:18:48 is the hour:minutes:seconds, AM or PM for the pacific time zone (in 12hour format), and the date Aug 17, 2018 is in the month day, year format. To show the timestamp in the format above, I used a helper library called pytz . According to the pytz documentation , pytz is a package for dealing with world timezone definitions in Python. We need to pip install the pytz package into the virtual environment that runs our flask app. See part of this post on how other Python packages were installed on the server. $ cd ~ $ mkdir flaskapp $ cd flaskapp $ python3.6 -m venv flaskappenv $ source flaskapp/bin/activate ( flaskappenv ) $ pip install pytz ( flaskappenv ) $ deactivate $ Now, the flaskapp.py file can be modified to include pytz and Python's datetime and dateutil libraries in the imports: #flaskapp.py from flask import Flask , render_template , request import datetime import os import dateutil.parser from config import API_KEY , MAC_ADDRESS import pytz Next, the @app.route section of the flask app can be modified to include the timestamp. Note that we now pass time_stamp=date_time_str to the \"showrecent.html\" template. We will also need to ensure the showrecent.html template accepts both of these arguments. @app.route ( \"/update/API_key=<api_key>/mac=<mac>/field=<int:field>/data=<data>\" , methods = [ 'GET' ]) def write_data_point ( api_key , mac , field , data ): if ( api_key == API_KEY and mac == MAC_ADDRESS ): t = datetime . datetime . now ( tz = pytz . timezone ( 'America/Los_Angeles' )) date_time_str = t . isoformat () return render_template ( \"showrecent.html\" , data = data , time_stamp = date_time_str ) else : return render_template ( \"403.html\" ) In the code above, t = datetime.datetime.now(tz=pytz.timezone('America/Los_Angeles')) saves the datetime when the URL comes in using the datetime module and the 'America/Los_Angeles' time zone from pytz . On the server, we will save the time stamps in the pacific time zone because that's where I live. When we render the template, we can include the PST time zone where we show the time and date. The showrecent.html template is below: <!-- /templates/showrecent.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > read temp </ title > < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css> <!-- Latest compiled and minified JavaScript --> <script src=\" https: // maxcdn . bootstrapcdn . com / bootstrap / 3 . 3 . 7 / js / bootstrap . min . js \" integrity = \"sha384-Tc5\" > </ head > < body > < div class = \"container-fluid\" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > {{ data }} </ h1 > < p class = \"lead\" > Most Recent Data Point </ p > < hr class = \"my-4\" > < h1 class = \"display-4\" > {{ time_stamp }} </ h1 > < p class = \"lead\" > Most Recent Time Time Zone: PST </ p > < hr class = \"my-4> <p class=\" lead \" > < a class = \"btn btn-primary btn-lg\" href = \"/showrecent.html\" role = \"button\" > REFRESH </ a > </ p > </ div > </ div > </ body > </ html > Restart the flask app and view the changes Now we can restart the flask app and see if our modified web API works. We want to test to see if the 403.html template is rendered when a non-valid URL is entered. We also want to see the time-stamp rendered by showrecent.html when a valid URL is used. $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # [ctrl-c] to exit. Now if we go to the web address: https://yourdomain.com//update/API_key=APGLMD/mac=a3:45:b5:c9/field=1/data=98.6 You should see the the 403 error template rendered. Because the URL above is an invalid URL. It does not contain a valid API key or mac address. But if you go to the web address: https://mydomain.com/update/API_key=GTW89NF3/mac=6c:rf:7f:2b:0e:g8/field=1/data=22.0 You should see the showrecent.html template rendered. This URL contains a valid API and mac address. Summary It works! When we browse to a URL without the correct API key or mac address, we're returned the 403 error page. If we browse to a valid URL, with the right API key and mac address, our flask IoT server shows the data point we uploaded with a time stamp included. Next steps In the next post in this series, we'll modify our flask IoT server to save the data points to a simple database. Then we'll build an html template to render the data from the database instead of just instantaneously based on the web API URL. sqlite3 here we come!","tags":"flask","url":"flask-iot-server-validation-time-stamps.html"},{"title":"Solving a Circuit Diagram Problem with Python and SchemDraw","text":"In this post we will create a circuit diagram using a using a Python package called SchemDraw . Then we'll solve a problem using this circuit diagram and Python. In this post, we are going to solve a circuit diagram problem using Python and a package called SchemDraw . SchemDraw is a specialized Python package for drawing circuit diagrams. For SchemDraw documentation see: https://cdelker.bitbucket.io/SchemDraw/SchemDraw.html Given: The circuit diagram below with a driving voltage $V_t = 5.20 V$ and resistor values in the table below. A table of resistance values is below: V t = 5.20 V R 1 = 13.2 m R 2 = 21.0 m R 3 = 3.60 m R 4 = 15.2 m R 5 = 11.9 m R 6 = 2.20 m R 7 = 7.40 m Find: V 6 and V 7 , the voltage drop across resistors R 6 and R 7 I 3 and I 6 , the current running through resistors R 3 and R 6 P 4 and P 7 , the power dissipated by resistors R 4 and R 7 Solution: First we'll import the necessary packages. I'm using a jupyter notebook, so the %matplotlib inline command is included. If you want high-resolution circuit diagrams, include the line: %config InlineBackend.figure_format = 'svg' at the top of the notebook will ensure high-resolution images. In [1]: import matplotlib.pyplot as plt # if using a jupyter notebook: include %matplotlib inline. If constructing a .py-file: comment out % matplotlib inline # if high-resolution images are desired: include %config InlineBackend.figure_format = 'svg' % config InlineBackend.figure_format = 'svg' import SchemDraw as schem import SchemDraw.elements as e Now we'll build the circuit diagram by creating a SchemDraw Drawing object and adding elements to it. In [2]: d = schem . Drawing ( unit = 2.5 ) R7 = d . add ( e . RES , d = 'right' , botlabel = '$R_7$' ) R6 = d . add ( e . RES , d = 'right' , botlabel = '$R_6$' ) d . add ( e . LINE , d = 'right' , l = 2 ) d . add ( e . LINE , d = 'right' , l = 2 ) R5 = d . add ( e . RES , d = 'up' , botlabel = '$R_5$' ) R4 = d . add ( e . RES , d = 'up' , botlabel = '$R_4$' ) d . add ( e . LINE , d = 'left' , l = 2 ) d . push () R3 = d . add ( e . RES , d = 'down' , toy = R6 . end , botlabel = '$R_3$' ) d . pop () d . add ( e . LINE , d = 'left' , l = 2 ) d . push () R2 = d . add ( e . RES , d = 'down' , toy = R6 . end , botlabel = '$R_2$' ) d . pop () R1 = d . add ( e . RES , d = 'left' , tox = R7 . start , label = '$R_1$' ) Vt = d . add ( e . BATTERY , d = 'up' , xy = R7 . start , toy = R1 . end , label = '$V_t$' , lblofst = 0.3 ) d . labelI ( Vt , arrowlen = 1.5 , arrowofst = 0.5 ) d . draw () d . save ( '7_resistors_3_loops.png' ) #d.save('7_resistors_3_loops.pdf') *{stroke-linecap:butt;stroke-linejoin:round;} Find R t Now we'll find the total resistance of the circuit R t using the individual resistances. First, define the resistances and driving voltage as variables. In [3]: Vt = 5.2 R1 = 0.0132 R2 = 0.021 R3 = 0.00360 R4 = 0.0152 R5 = 0.0119 R6 = 0.0022 R7 = 0.00740 Find R 45 and R 67 To simplify the circuit diagram, we'll combine the resistors in series. For resistors in a simple series circuit: $$ R_t = R_1 + R_2 + R_3 ... + R_n $$ Since resistors $R_4$ and $R_5$ are in simple series: $$ R_{45} = R_4 + R_5 $$ Since resistors $R_6$ and $R_7$ are in simple series: $$ R_{67} = R_6 + R_7 $$ We can easily calculate this with Python. After the calculation, we can use an fstring to print the results. Note the round() function is used on the inside of the fstring curly braces { } , in case there are some floating point math errors that lead to the values printing out as long floats. In [4]: R45 = R4 + R5 R67 = R6 + R7 print ( f 'R45 = {round(R45,7)} Ohm, R67 = {round(R67,5)} Ohm' ) R45 = 0.0271 Ohm, R67 = 0.0096 Ohm Let's redraw our circuit diagram to show the combined resistors. In [5]: d = schem . Drawing ( unit = 2.5 ) R67 = d . add ( e . RES , d = 'right' , botlabel = '$R_ {67} $' ) d . add ( e . LINE , d = 'right' , l = 2 ) d . add ( e . LINE , d = 'right' , l = 2 ) R45 = d . add ( e . RES , d = 'up' , botlabel = '$R_ {45} $' ) d . add ( e . LINE , d = 'left' , l = 2 ) d . push () R3 = d . add ( e . RES , d = 'down' , toy = R67 . end , botlabel = '$R_3$' ) d . pop () d . add ( e . LINE , d = 'left' , l = 2 ) d . push () R2 = d . add ( e . RES , d = 'down' , toy = R67 . end , botlabel = '$R_2$' ) d . pop () R1 = d . add ( e . RES , d = 'left' , tox = R67 . start , label = '$R_1$' ) Vt = d . add ( e . BATTERY , d = 'up' , xy = R67 . start , toy = R1 . end , label = '$V_t$' , lblofst = 0.3 ) d . labelI ( Vt , arrowlen = 1.5 , arrowofst = 0.5 ) d . draw () d . save ( '5_resistors_3_loops.png' ) #d.save('5_resistors_3_loops.pdf') *{stroke-linecap:butt;stroke-linejoin:round;} Find R 2345 Next we can combine the resistors in parallel. The resistors in parallel are $R_2$, $R_3$ and $R_{45}$. For a resistors in a simple parallel circuit: $$ \\frac{1}{R_t} = \\frac{1}{R_1} + \\frac{1}{R_2} + \\frac{1}{R_3} ... + \\frac{1}{R_n} $$ Since $R_2$, $R_3$ and $R_{45}$ are in parallel: $$ \\frac{1}{R_{2345}} = \\frac{1}{R_2} + \\frac{1}{R_3} + \\frac{1}{R_{45}} $$ $$ R_{2345} = \\frac{1}{\\frac{1}{R_2} + \\frac{1}{R_3} + \\frac{1}{R_{45}}} $$ We can code this calculation in Python. To find the reciprocal, raise the combined sum to the negative one power. Remember, exponentiation is performed with a double asterisk ** in Python. In [6]: Vt = 5.2 R1 = 0.0132 R2 = 0.021 R3 = 0.00360 R4 = 0.0152 R5 = 0.0119 R6 = 0.0022 R7 = 0.00740 R45 = R4 + R5 R67 = R6 + R7 R2345 = (( 1 / R2 ) + ( 1 / R3 ) + ( 1 / R45 )) ** ( - 1 ) print ( f 'R2345 = {round(R2345,7)} Ohm' ) R2345 = 0.0027602 Ohm OK, now let's construct a new SchemDraw diagram of the simplified the circuit. In this diagram, we'll combine $R_2$, $R_3$ and $R_{45}$ into one big resistor, $R_{2345}$. In [7]: d = schem . Drawing ( unit = 2.5 ) R67 = d . add ( e . RES , d = 'right' , botlabel = '$R_ {67} $' ) R345 = d . add ( e . RES , d = 'up' , botlabel = '$R_ {2345} $' ) R1 = d . add ( e . RES , d = 'left' , tox = R67 . start , label = '$R_1$' ) Vt = d . add ( e . BATTERY , d = 'up' , xy = R67 . start , toy = R1 . end , label = '$V_t$' , lblofst = 0.3 ) d . labelI ( Vt , arrowlen = 1.5 , arrowofst = 0.5 ) d . draw () d . save ( '3_resistors_1_loop.png' ) #d.save('3_resistors_1_loop.pdf') *{stroke-linecap:butt;stroke-linejoin:round;} Find R t To find $R_t$, we again combine the resistors in series. The remaining resistors $R_1$, $R_{2345}$ and $R_{67}$ are in series: $$ R_{1234567} = R_1 + R_{2345} + R_{67} $$ We'll call the total resistance of the circuit $R_t$ which is equal to $R_{1234567}$ $$ R_t = R_{1234567} $$ Another calculation in Python. In [8]: Vt = 5.2 R1 = 0.0132 R2 = 0.021 R3 = 0.00360 R4 = 0.0152 R5 = 0.0119 R6 = 0.0022 R7 = 0.00740 R45 = R4 + R5 R67 = R6 + R7 R2345 = (( 1 / R2 ) + ( 1 / R3 ) + ( 1 / R45 )) ** ( - 1 ) Rt = R1 + R2345 + R67 print ( f 'Rt = {round(Rt,7)} Ohm' ) Rt = 0.0255602 Ohm Last circuit diagram. The simplest one. This SchemDraw diagram just includes $V_t$ and $R_t$. In [9]: d = schem . Drawing ( unit = 2.5 ) L2 = d . add ( e . LINE , d = 'right' ) Rt = d . add ( e . RES , d = 'up' , botlabel = '$R_ {t} $' ) L1 = d . add ( e . LINE , d = 'left' , tox = L2 . start ) Vt = d . add ( e . BATTERY , d = 'up' , xy = L2 . start , toy = L1 . end , label = '$V_t$' , lblofst = 0.3 ) d . labelI ( Vt , arrowlen = 1.5 , arrowofst = 0.5 ) d . draw () d . save ( '1_resistor_no_loops.png' ) #d.save('1_resistor_no_loops.pdf') *{stroke-linecap:butt;stroke-linejoin:round;} Find V 6 and V 7 Now that we've solved for the total resistance of the circuit $R_t$, we can find the total current running through the circuit using Ohm's Law $V = IR $. $$ V = IR $$ $$ I = \\frac{V}{R} $$ $$ I_t = \\frac{V_t}{R_t} $$ In [10]: Vt = 5.2 R1 = 0.0132 R2 = 0.021 R3 = 0.00360 R4 = 0.0152 R5 = 0.0119 R6 = 0.0022 R7 = 0.00740 R45 = R4 + R5 R67 = R6 + R7 R2345 = (( 1 / R2 ) + ( 1 / R3 ) + ( 1 / R45 )) ** ( - 1 ) Rt = R1 + R2345 + R67 It = Vt / Rt print ( f 'It = {round(It,2)} A' ) It = 203.44 A The total current of the circuit, $I_t$ is the same as the current running through resistor $R_6$ and resistor $R_7$. $$ I_t = I_6 = I_7 $$ We can apply Ohm's law to find $V_6$ now that we have $I_6$ and $I_7$. $$ V_6 = I_6 R_6 $$ $$ V_7 = I_7 R_7 $$ In [11]: I6 = It I7 = It V6 = I6 * R6 V7 = I7 * R7 print ( f 'V6 = {round(V6,5)} V, V7 = {round(V7,5)} V' ) V6 = 0.44757 V, V7 = 1.50547 V Find I 3 and I 6 The total current of the circuit, $I_t$ is the same as the current running through resistor $R_{2345}$. $$ I_t = I_{2345} $$ We can apply Ohm's law to find $V_{2345}$ now that we have $I_{2345}$. $$ V_{2345} = I_{2345} R_{2345} $$ In [12]: I2345 = It V2345 = I2345 * R2345 print ( f 'V2345 = {round(V2345,5)} V' ) V2345 = 0.56153 V The voltage drop across resistor $R_3$ is the same as the voltage drop across resistor $R_{2345}$. $$ V_3 = V_{2345} $$ Since $V_3$ and $R_3$ are known, we can solve for $I_3$ using Ohm's law: $$ V = IR $$ $$ I = \\frac{V}{R} $$ $$ I_3 = \\frac{V_3}{R_3} $$ The current $I_6$ running through resistor $R_6$ is the same as the total current $I_t$. $$ I_6 = I_t $$ In [13]: V3 = V2345 I3 = V3 / R3 I6 = It print ( f 'I3 = {round(I3,2)} A, I6 = {round(I6,2)} A' ) I3 = 155.98 A, I6 = 203.44 A Find P 7 and P 4 Power is equal to voltage times current: $$ P = VI $$ According to Ohm's law: $$V = IR$$ If we substitute $V$ as $IR$ in the power equation we get: $$ P = (IR)(I) $$ $$ P = I&#94;2 R $$ With a known $R_7$ and $I_7 = I_t$: $$ P_7 = {I_7}&#94;2 R_7 $$ In [14]: I7 = It P7 = R7 * I7 ** 2 print ( f 'P7 = {round(P7,2)} W' ) P7 = 306.27 W Current $I_{45}$ is equal to current $I_4$. Voltage $V_{45} = V_{2345}$. Using Ohm's Law again: $$ V = IR $$ $$ I = \\frac{V}{R} $$ $$ I_{45} = \\frac{V_{45}}{R_{45}} $$ In [15]: V45 = V2345 I45 = V45 / R45 print ( f 'I45 = {round(I45,3)} A' ) I45 = 20.721 A One more time using the power law: $$ P = I&#94;2 R $$ With a known $R_4$ and $I_4 = I_{45}$: $$ P_4 = {I_4}&#94;2 R_4 $$ In [16]: I4 = I45 P4 = R4 * I4 ** 2 print ( f 'P4 = {round(P4,4)} W' ) P4 = 6.5261 W Final Answer Let's print out all of the final values to three significant figures including units: In [17]: print ( f 'V6 = {round(V6,3)} V' ) print ( f 'V7 = {round(V7,2)} V' ) print ( f 'I3 = {round(I3,0)} A' ) print ( f 'I6 = {round(I6,0)} A' ) print ( f 'P4 = {round(P4,2)} W' ) print ( f 'P7 = {round(P7,0)} W' ) V6 = 0.448 V V7 = 1.51 V I3 = 156.0 A I6 = 203.0 A P4 = 6.53 W P7 = 306.0 W Conclusion SchemDraw is a great package for making circuit diagrams in Python. Python is also useful for doing calculations that involve lots of different values. Although none of the calculations in this problem were particularly difficult, keeping track of all the values as variables in Python can cut down on errors when there multiple calculations and many parameters to keep track of.","tags":"matplotlib","url":"circuit-diagram-problem-with-schemdraw.html"},{"title":"Building an IoT Server with flask and Python - Part 3 Web API","text":"This is the third part of a series of posts about building an Internet of Things (IoT) server with flask , Python and ESP8266 microcontrollers. In the last post, we reviewed server and hardware setup. In this post, we'll build a web API with flask and push temperature data to our web API with a web browser. Table of contents: Introduction What is a web API? Web API Design Construct a flask web API Create a new route Create a new template Test the web API Summary Next steps Introduction We already have a working flask app hosted on Digital Ocean . Now we need to add a web API to the flask app's functionality. What is a web API? A web API is a web-based Application Programming Interface. That's a fancy way of saying a server that saves input or produces output based on a URL the server recieves from a web browser. An example of a web API is the ThingSpeak.com web API. When a web browser, like chrome, is directed to the followoing URL: https://api.thingspeak.com/channels/266256/fields/2/last.txt The response is a data entry stored on ThingSpeak.com that corresponds to: channel 266256 field 2 last entry .txt format If the URL pasted into the web browser is different, ThingSpeak.com's response will also be different. https://api.thingspeak.com/channels/9/fields/1/last.json The response to the URL above is a data entry stored on ThingSpeak.com that corresponds to: channel 9 field 1 last entry .json format Most web API's allow you to pull data off of their servers, but many web API's also give you the ability to put data up on their servers. If the URL pasted into the web browser is constructed in the format below, ThingSpeak.com will store a new data point. https://api.thingspeak.com/update?api_key=THECLASSAPIKEY&field1=87 The datapoint ThingSpeak.com stores when it recieves the GET request is: user with an API Key = THECLASSAPIKEY field = 1 data = 87 The web API we'll build with flask needs to fullfill the same two basic functions the ThingSpeak.com web API accomplishes: Output a data point based on the particular URL the flask IoT server receives Store a data point based on the particular URL the flask IoT server receives Web API Design We're going to mimic part of the ThingSpeak.com web API in our flask IoT server web API. In order to store a data point based on the URL the server recieves, we need to specify how the URL must be structured. Our ESP8266-based WiFi weather stations need to know the URL format in order to post data points on the flask IoT server. Data will be stored on our flask IoT server based on the URL the server recieves from the ESP8266-based WiFi weather stations. There are a couple unique aspects to each of the ESP8266-based WiFi weather stations: a user: Each WiFi weather station has a user. In this case the user is me. a mac address: A mac address is a unique address assigned to each piece of hardware. Each of the ESP8266-based WiFi weather stations has a different mac address. field: The ESP8266-based WiFi weather stations have the capability to output temperature and humidity. Right now we are just going to deal with temperature, but it would be nice to have an extra field available for multiple data streams (like temperature and humidity) from the same device. data: The temperature measurement which comes out of each ESP8266-based WiFi weather station. If we put these 4 identifiers as part of our web API URL, our IoT server will provide the functionality the WiFi weather stations need. The general form of our web API URL is below: https://mydomain.com/update/API_key=ASCIISTR/mac=6c:rf:7f:2b:0e:g8/field=1/data=72.3 In the URL above we've provided: update (to tell the IoT server to save the data point, not just serve a webpage) API_key=ASCIISTR (to identify the user) mac=6c:rf:7f:2b:0e:g8 (to identify the ESP8255-based WiFi weather station) field=1 (to specify the data is a temperature, not humidity) data=72.3 (to specify the temperature is 72.3 degrees) Now we need to make our flask IoT server save the data point when a URL like the one we specified above comes in. Construct a flask web API Building web API's in flask is pretty easy due to flask's option of including a variable in a route. The general syntax is below: @app.route ( \"/update/key=<route_var>\" , methods = [ 'GET' ]) def update ( route_var ): # code to run return render_template ( \"index.html\" ) In the code above, the route \"/update/key=<route_var>\" has a the variable <route_var> contained in it. The greater than/less than symbols < > tell flask there is a variable in the route. In the second line, the variable route_var (which came in from the @app.route() line) is passed as an argument to the update() function. Finally, the update() function returns a tempate called index.html . Create a new route There are four variables to assign in the @pp.route() URL of our flask web API: parameter purpose route variable API key unique identifier each user <api_key> mac address unique identifier for each ESP8266 device <mac> field denote temperature or humidity <field> data data point to save on the server <data> The complete @pp.route() URL for our web API looks like: \"/update/API_key=<api_key>/mac=<mac>/field=<int:field>/data=<data>\" We can now build a new @app.route() -function pair for this URL. Note we return a new template called update.html and pass the template the data point data . @app.route ( \"/update/API_key=<api_key>/mac=<mac>/field=<int:field>/data=<data>\" , methods = [ 'GET' ]) def update ( api_key , mac , field , data ): return render_template ( \"update.html\" , data = data ) Create a new template Since we are calling a new template, we need to construct a new template. The template update.html can have the same form as index.html . Let's create a new file and copy in the following html code and jinja fields. $ cd ~ $ cd flaskapp $ cd templates $ nano update.html The code for the new update.html template is below: <!-- update.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > show temp </ title > <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity = \"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" integrity = \"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" integrity = \"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\" crossorigin = \"anonymous\" ></ script > </ head > < body > < div class = \"container-fluid\" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > {{ data }} </ h1 > < p class = \"lead\" > last temperature uploaded </ p > < hr class = \"my-4\" > </ div > </ div > </ body > </ html > Test the web API Now on our server, we can restart the flask app and see if our web API works. $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # [ctrl-c] to exit. If we browse to the web address of the server, we should still see the same index template rendered. But now if we go to the web address: https://yourdomain.com//update/API_key=APGLMD/mac=a3:45:b5:c9/field=1/data=98.6 You see the update tempate rendered with a temperature of 98.6 shown. Summary It works! When we put a specific URL into our web browser, it causes the temperature we see on the web to change. Next steps In the next post, we'll modify our flask IoT server to do some validation of the input from the web API. We don't want just any person to bang away at the API and upload data points. We are also going to delve into Python's datetime module and add a datetime to each data point uploaded to our flask IoT server.","tags":"flask","url":"flask-iot-server-web-API.html"},{"title":"First Live Results Running Jupyter Hub in a College Class","text":"This is the eighth part of a multi-part series settting up Jupyter Hub for a college class. The Jupyter Hub Server is live and running! Students are logging in! In this quick post, I'll share my initial impressions running Jupyter Hub in the wild of a college engineering classroom. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub Add a custom login page and assignments directory for each user on a Jupyter Hub server Initial first impressions of running Jupyter Hub live with students (this post) Last time In the last Jupyter Hub post , we added a custom login page to Jupyter Hub and built a way for a repo on GitHub to be pulled down and pre-populate each users' directory with assignments and notes. What the big deal was today The big deal today was that during lab this morning, 17 students (plus me) logged onto Jupyter Hub together - all at the same time - for the first time! I've tested the Jupyter Hub server for a couple weeks. One student logged in to the server and ran some notebooks. Another faculty member tested the server out too. But this morning was the first time all the students in one lab logged and ran notebooks all at the same time. What I was nervous about Before this morning, I was nervous about a couple of things in regards to the Jupyter Hub server students in the class will not be able to log in only some of the students will be able to log in the server will go down because everyone is logging in and using Jupyter Hub at the same time notes and assignments up on GitHub won't populate students' directory students won't be able to run notebooks at the same time because the server gets overloaded. What happened I'm pleased to report none of those worries came to fruition . What happened this morning was: All 17 students in the lab (plus me) logged into Juypter Hub successfully The Jupyter Hub server did not crash or run super slow when students logged in Each student got the assignments and notes from GitHub in their directory Students looked at notebooks and ran code cells at the same time The Jupyter Hub server never crashed What's next? Students have 3 labs using Jupyter Hub over the next 3 weeks. I'm interested to see (and really hope) we don't have any problems with the server in while students are completing their labs. Besides the server running, I'm also interested to see how the notes ( posted on Github ) and assignments ( posted on Github ) worked. Thank-you for all of your hard work. So far Jupyter Hub is working great! I want to say a big thank-you to Numfocus and the folks working on Project Jupyter. The Jupter Hub server is providing a diverse group of students an opportunity to learn Python programming without the hassle of any installation steps, administrator rights, PATH variables, virtual environments or complicated IDE's.","tags":"jupyter","url":"first-live-results-running-jupyter-hub.html"},{"title":"Building an IoT Server with flask and Python - Part 2 Set Up","text":"This post is the second part of a series of posts which detail building an Internet-of-Things (IoT) server with flask , Python and ESP8266 microcontrollers. In this post, we'll describe server setup and microcontroller hardware used in the project. Table of contents: Introduction Prerequisites Server Hardware The starting place Summary Next steps Introduction This flask IoT server project builds upon the ESP8266 WiFi weather station project and the flask app on Digital Ocean project . In the flask app on Digital Ocean project, a flask app pulled a measurement temperature down from ThingSpeak.com and displayed the temperature on a webpage. In the ESP8266 WiFi weather station project, an ESP8266 microcontroller (connected to a temperature sensor) posted the temperature up to ThingSpeak.com. The problem with using ThingSpeak.com as an IoT platform is there are limits to how often data can be posted. Building my own IoT server with flask is an exciting and interesting project and solves the \"only one data post every 15 seconds\" limitation imposed by ThingSpeak. Prerequisites Previously, I built a single page flask app that displays the temperature measured by WiFi weather stations. That single page flask app is running on a Digital Ocean cloud server. The flask app pulls a temperature data point using the ThingSpeak.com web API and displays the temperature on a webpage using flask and a jinja template. See the flask app hosted on Digital Ocean post to see the starting point for this project's flask IoT server. If you are starting from scratch, the prerequisites needed to build an Internet-of-Things server with flask , Python and ESP8266 microcontrollers are: Server A Digital Ocean cloud server (just called the server from here on out). See this post and part of this post. A domain name hooked up to the server. See part of this post. PuTTY or a terminal that can SSH into the server A non-root sudo user on the server. See part of this post. The following packages apt-get installed on the server: python3-pip python3-dev python3-setuptools python3-venv build-essential libssl-dev libffi-dev nginx A Python 3.6 virtual environment set up on the sever with flask and uwsgi pip install ed. See part of this post. uWSGI and NGINX installed on configured on the server. See part of this post. . The configuration files I used can be found on github: myproject.ini , wsgi.py , sites-avialable (nginx config) The flask app running as a system service. See this gist for the systemd flaskapp.service file. SSL attached to the domain name and NGINX instance. See part of this post and this post. The final web page produced by the flask app is below: Hardware The below is a list of hardware used to build the ESP8266-based WiFi weather stations. See this post and the Fritzing sketch below for hardware setup. Adafruit Feather Huzzah ESP8266 MCP9808 temperature sensor , BMP280 temperature sensor jumper wires , breadboard microUSB cable . Micropython firmware for the ESP8266 loaded on the ESP8266 board. See this post. The following .py files (available on github) were loaded onto the board with ampy : BMP280.py , MCP9808 , wifitools.py . See this post. Note that as part of this project, I'm using two ESP8266 microcontrollers. One ESP8266 is connected to a BMP280 temperature sensor and the other ESP8266 is connected to an MCP9808 temperature sensor. These two temperature sensors are leftover from previous projects (I would have used two of the same sensor, but one BMP280 and one MCP9808 is what I have lying around). One ESP8266-tempsensor combo is used to measure the temperature outside, the other ESP8266-tempsensor combo is used to measure the temperature inside. The starting place The flask app I built was relatively basic and mainly comprised of 2 files: flaskapp.py and index.html . The file structure on the Digital Ocean server looks like this: ~/  flaskapp  flaskapp.ini  flaskapp.py  flaskapp.sock  flaskappenv  templates   index.html  wsgi.py The main file to run the flask app is flaskapp.py # flaskapp.py from flask import Flask , render_template import requests app = Flask ( __name__ ) @app.route ( \"/\" ) def index (): r = requests . get ( 'https://api.thingspeak.com/channels/254616/fields/1/last.txt' ) temp_c_in = r . text temp_f = str ( round ((( 9.0 / 5.0 ) * float ( temp_c_in ) + 32 ), 1 )) + ' F' return render_template ( \"index.html\" , temp = temp_f ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' ) The only jinja template used by the flask app to build web pages was in the /templates directory and named index.html : <!-- index.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > show temp </ title > <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity = \"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" integrity = \"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" integrity = \"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\" crossorigin = \"anonymous\" ></ script > </ head > < body > < div class = \"container-fluid\" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > {{ temp }} </ h1 > < p class = \"lead\" > temperature inside </ p > < hr class = \"my-4\" > </ div > </ div > </ body > </ html > With the flask app created and the domain name, nginx , uWSGI , systemd and SSL configured- the app is started on the server with: $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # [ctrl-c] to exit. The resulting web page looks like this: Summary This post detailed the server setup and microcontroller hardware used in our IoT server with flask and Python project. The server running on Digital Ocean has a domain name connected to it and is running a nginx  uWSGI  flask web stack. The flask app currently returns a webpage with a temperature pulled from ThingSpeak.com. The hardware includes two Adafruit Feather Huzzah ESP8266 microcontrollers connected to temperature sensors. Next steps In the next post, we'll build a web API with flask and push temperature values up to our very own IoT server.","tags":"flask","url":"flask-iot-server-setup.html"},{"title":"Building an IoT Server with Flask and Python - Part 1 Motivation","text":"This post is the first part of a series of blog posts about building an Internet-of-Things (IoT) server with flask , Python and ESP8266 microcontrollers. In this post we'll I'll discuss the problem I'm trying to solve and the issues I have with the current solution. We'll also review what the Internet-of-Things (IoT) is, and what an IoT server does. Table of contents: Introduction IoT Devices IoT Servers The Problem The Heat Problem The ThingSpeak.com Problem Proposed Solution IoT Server Requirements Project steps Next steps Introduction In the next couple of posts, I'm going to demonstrate how I built an Internet-of-Things server with flask and Python. The Internet-of-Things (IoT) is a network of computers, phones, tablets and physical devices like thermostats, garage door openers, light bulbs, doorbell cameras, weather stations connected to each other. When we talk about the Internet , we are usually referring to computers, tablets, phones and servers communicating with web pages, programs and apps. The Internet-of-Things builds on the Internet by including devices other than computers, phones, tablets and servers. IoT Devices I have two ESP8266-based WiFi weather stations. These little devices are part of the Internet-of-Things. The little WiFi weather stations cost about $20 each and run on very little power. The weather stations are made up of an ESP8266 microcontroller and a temperature sensor connected with jumper wires and a bread board. The ESP8266-based WiFi weather stations beam temperature measurements up to servers in the cloud. These WiFi weather stations are IoT devices . IoT Servers Servers that interact with Internet-of-Things devices (like my ESP8266-based WiFi weather stations) are called Internet-of-Things servers or IoT servers . IoT servers communicate with IoT devices . At the start of this project, the ESP8266-based WiFi weather stations communicated with ThingSpeak.com IoT servers. The WiFi weather stations send temperature measurements up to the ThingSpeak.com IoT servers where the data is saved. In a previous post I described how to build a flask single page web app that pulls temperature data from the ThingSpeak.com IoT servers using a web API. The Problem There are two problems I am trying to solve with this project. One problem is keeping the house cool in the summer heat. The other problem is ThingSpeak.com posting limits. The Heat Problem I live in Portland, OR and it was HOT last week. One day last week, the temperature outside climbed to 98 F. I know if you live in Ft. Worth, TX- 98 F isn't too severe. However, in the Pacific Northwest, 98 F is HOT . One way to keep our house cool is to open windows at night and use fans to blow cold outside air into the hot, stuffy house. The question is: Question: \"When should I open windows and turn on fans to keep the house cool?\" During the day, it is hotter outside than inside, so I keep the windows closed. However, it has been so hot- that even after the sun goes down, it is still hotter outside than inside. What I want to know is: when does the temperature outside go lower than the temperature inside? When that I happens, I'll open the windows and blow cold, fresh air inside. I also want to know when the temperature outside gets hotter than the temperature inside. When that happens, I'll close the windows and turn off the fans. The ThingSpeak.com Problem Last week, the solution to knowing the temperature inside/temperature outside problem was tackled with ESP8266-based WiFi weather stations. One WiFi weather station is taped to a window outside my kids' room, and another WiFi weather station sat on top of my daughter's dresser. These WiFi weather stations measure temperature once every 60 seconds and post the temperature to ThingSpeak.com The WiFi weather stations connected to ThingSpeak.com worked pretty well. However, there were a couple of issues. One issue is the 15 second limit between data point uploads imposed ThingSpeak. With two weather stations publishing a data point every 60 seconds, it seems like the data points would be 30 seconds apart. On average every 30 seconds, one of the two weather stations publishes a temperature to ThingSpeak. But the 30-second interval between posts to ThingSpeak is dependant upon the two weather stations synched in time, so that the two stations measure and send temperatures 30 seconds apart. In practice, when I power up both ESP8266-based WiFi weather stations, it is hard to get one plugged in- then precisely 30 seconds- later plug the other one in. Plus if I unplug only one of the weather stations, it isn't practically possible to know when exactly to plug it back in. Maybe when it is plugged back in, one weather station takes a temperature at 0:02 minutes past the minute mark and the other weather station takes the temperature at 0:10 seconds past the minute mark. If that's the case, then the two temperature measurements are sent within one 15-second interval. Within one 15-second interval, ThingSpeak only records the data from one of weather stations. Proposed Solution I propose to solve this problem by building my own Internet-of-Things server with flask and Python. With my own IoT server, I can set the limit of how often devices (WiFi weather stations) can post data points. Since I only have two weather stations, the IoT server can accept data points at a faster rate than every 15 seconds. In addition to solving a problem, this project also interests me. I already built a single page web app with flask and Python . How can this previous project be taken further? One way of extending the previous project is to turn the single page web app into an IoT server. IoT Server Requirements What must our IoT server running flask and Python be able to do? The IoT server needs to fulfill two primary functions: Accept and store temperature measurements from two ESP8266-based WiFi weather stations Publish the temperature measured by two ESP8266-base WiFi weather stations to a webpage Project steps Building an IoT server with flask and Python is a multi-part problem. We can break the problem down into steps: Setup Build a flask -based server running in the cloud (Digital Ocean) assemble the hardware for the WiFi weather stations Construct a web API with flask and Python that accepts requests from web browsers and saves data points Add a database to the server to save the data points that come in from the WiFi weather stations Upload code on the ESP8266-based WiFi weather stations to send temperature measurements using the server's web API Next steps In the next post, we will complete the initial setup of the server and hardware. The server setup includes starting a new Droplet (a server) on Digital Ocean. The hardware setup includes connecting two ESP8266 microcontrollers to temperature sensors.","tags":"flask","url":"flask-iot-server-motivation.html"},{"title":"Building a single page Flask App on Digital Ocean","text":"In this post, we'll run through how to set up a single page flask app that shows a temperature pulled from ThingSpeak.com . ThingSpeak has nice looking graphs, but on ThingSpeak it is actually kind of hard to see the value of an individual data point. I want to be able to see the most recent temperature point recorded by my ESP8266 WiFi weather station project on a phone or tablet. By building a flask app and hosting it on Digital Ocean, I can now view the current temperature in a nice big font from anywhere. Table of contents: Set up a new Digital Ocean Droplet Create a new Droplet Login to server with PuTTY Create a non-root sudo user Copy SSH keys to the non-root sudo user Acquire and configure a domain name Purchase a domain name Point DNS severs at Digital Ocean Link the domain name to the server IP address Build the Flask App Install packages Create a virtual environment and install flask Build the first simple flask app Testing the first simple flask app Set up uWSGI and systemctl Configuring uWSGI Testing uWSGI Construct the uWSGI configuration file Construct a systemd file Test with systemctl Configure NGINX and apply SSL security Install NGINX Configure NGINX Apply SSL Security Add Bootstrap styling Pull the temperature from ThingSpeak.com with requests View the final flask app online Summary Set up a new Digital Ocean Droplet The flask app needs a server to run on. I choose Digital Ocean as my cloud server provider. Digital Ocean hosts virtual private servers that run in the cloud. Setting up a server on Digital Ocean is pretty cheap ($5/month) and quick. I host my Jupyter Hub server on Digital Ocean, so I am also more familiar with spinning up their servers compared to other cloud providers like Linode or AWS. I like their documentation. It is clear, concise and easy to follow. To set up the server for the flask app, I created a new Droplet on Digital Ocean. After creating a new server, it is best practice to create a non-root sudo user. Create a new Droplet Our flask single page web app built with Python will be hosted on Digital Ocean . To create a new cloud server, called a Droplet in DigitalOcean-speak, create an account on Digital Ocean. Once logged in select Create  Droplets in the upper right menu. The Digital Ocean Droplet options I choose were: Ubuntu 18.04.1 x64 Size: Memory 1G, SSD 25 GB, Transfer 1 TB, Price $5/mo Datacenter Region: San Fransisco 2 Additional Options: None SSH keys: Added all of my saved SSH keys . You need this to log into the server with PuTTY! Create the server with the big green [ Create ] button. After the Droplet is created, note the IP address of the server. We'll need the IP address of the droplet for the next step. Login to server with PuTTY Our first interaction with the server is to log in as root. Then we'll create a non-root sudo user to interact with the server from then on out. Open PuTTY and log onto the server as root. See a previous post on how to set up PuTTY on Windows 10. To log into the server as root, set the following in PuTYY: Hostname (or IP Address): The IP address of the server Port: 22 Connection Type: SSH Connection: Data: Auto-login username: root Connection: SSH: Auth: Private keyfile for Authentication: your saved private SSH key Create a non-root sudo user I followed the Digital Ocean Initial Server Setup Tutorial to create a non-root sudo user. The commands entered into the PuTTY SSH terminal are below. Note you should change the username to something other than peter . Here and in the Digital Ocean tutorial , a hash symbol # is shown before the commands. The hash # symbol should not be typed, it just represents the fact we are operating as root. # adduser peter # usermod -aG sudo peter # ufw allow OpenSSH # ufw enable Copy SSH keys to the non-root sudo user Next we'll move the SSH keys stored in the root user's profile to the new sudo user's profile (in my case peter ). I've had trouble moving SSH key files and setting permissions correctly in Linux. A Digital Ocean tutorial has a great line that copies the SSH Keys and sets the permissions correctly in one step. If you skip this step, you won't be able to log into the server as the new non-root sudo user you just created. Note you should change the user name from peter to whatever username you picked in the previous step. # rsync --archive --chown=peter:peter ~/.ssh /home/peter Now let's check if we can log into the server as the new user. Exit the PuTTY window by typing exit at the prompt. Open up a new SSH session in PuTTY. Set Connection  Data  Auto-login username as the non-root sudo user. (I put peter in the Auto-login username box). When the terminal window opens, you should see your username listed before the prompt. At the prompt, try the following command. Note the dollar sign $ does not need to be typed. The dollar sign $ is there to indicate the command prompt. $ sudo -l User peter may run the following commands on flask-app-server: ( ALL : ALL ) ALL You can type the command exit to close the PuTTY terminal. Acquire and configure a domain name To use secure SSL connections and https with our flask single page app, we need a real domain name. Purchase a domain name I bought my domain name at Google Domains for $12/year. The price seems reasonable and Digital Ocean has a tutorial that shows how to connect a google domains to Digital Ocean DNS servers. Point DNS severs at Digital Ocean Once the domain is purchased, the domain's Name Server needs to be pointed at Digital Ocean. Link the domain name to the server IP address On Digital Ocean, login and click [Create]  [Domains/DNS]. Type in the newly purchased domain name in the box and click [Add Domain]. Link the new Domain to the Digital Ocean Droplet by typing in the @ symbol in the [HOSTNAME] box and selecting the new Droplet name in the [WILL DIRECT TO] drop down box. Click [Create Record] to link the domain name to the server. You can also link www in the [HOSTNAME] box and select the new Droplet in the [WILL DIRECT TO] dropdown box to link www.yourdomain.com to the server. Build the Flask App Now that the server is set up and the domain name is routed to the server, it's time to actually build the flask single page web app. Install packages Before the single page flask app can be built, a number of packages need to be installed on the server. I followed along with this tutorial from Digital Ocean. Log onto the server with PuTTY and type the following commands: $ sudo apt-get update $ sudo apt-get upgrade $ sudo apt-get install python3-pip $ sudo apt-get install python3-dev $ sudo apt-get install python3-setuptools $ sudo apt-get install python3-venv $ sudo apt-get install build-essential libssl-dev libffi-dev Create a virtual environment and install flask Once the necessary libraries are installed, I created a virtual environment to run the flask app. I usually use conda to create virtual environments (see this post ), but since I'm not using the Anaconda distribution of Python for this flask app, venv will have to do instead. $ cd ~ $ mkdir flaskapp $ cd flaskapp $ python3.6 -m venv flaskappenv $ source flaskappenv/bin/activate With the virtual environment created and activated, install wheel , flask , uwsgi and requests with pip . We'll use requests a little later to pull down temperature data from ThingSpeak.com. Note that (flaskappenv) is shown before the command prompt when the virtual environment is active. Make sure to only pip install within the (flaskappenv) virtual environment. ( flaskappenv ) $ pip install wheel ( flaskappenv ) $ pip install flask ( flaskappenv ) $ pip install uwsgi ( flaskappenv ) $ pip install requests Build the first simple flask app With the Python packages installed, next we'll build a very simple version of the flask app and viewed it in a web browser. ( flaskappenv ) $ pwd # ~/flaskapp ( flaskappenv ) $ nano flaskapp.py In the flaskapp.py file, I included the bare minimum flask app to test the server: # flaskapp.py from flask import Flask app = Flask ( __name__ ) @app.route ( \"/\" ) def index (): return \"<h1>The temperature is 91.2 F</h1>\" if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' ) A note about editing code in the nano text editor thru PuTTY: You can paste into a PuTTY terminal window using the right mouse button. Selecting text in PuTTY copies the text to the clip board. Don't use [ctrl-c] or [ctrl-v] to copy and paste in PuTTY. Exit the nano text editor with [ctrl-x]. Testing the first simple flask app With the first version of flaskapp.py complete, Let's run the flask app for the first time to test that everything is working properly. To run the flask app, I had to make sure I was in the virtual environment built earlier. I also needed to allow port 5000 open on the ufw firewall. Port 5000 is the default port flask runs on. (flaskappenv)$ sudo ufw allow 5000 (flaskappenv)$ python flaskapp.py It works! By pointing a browser to the Droplet IP address followed by :5000 , I can see the simple message: \"The temperature is 91.2 F\". 124.822.76.209:5000 Set up uWSGI and systemctl There are going to be two layers between the flask app and the outside internet. Get requests from web browsers will first come into NGINX then go to uWSGI before being passed to flask . Configuring uWSGI We installed uWSGI earlier when we pip installed flask . Now uWSGI needs to be configured and tested. I followed the Digital Ocean tutorial closely for this step. ( flaskappenv ) $ pwd # ~/flaskapp ( flaskappenv ) $ nano wsgi.py In the wsgi.py file, include: # wsgi.py from flaskapp import app if __name__ == \"__main__\" : app . run () Testing uWSGI Next, let's test the configuration. uWSGI can be run from the command line with a couple flags: ( flaskappenv ) $ uwsgi --socket 0 .0.0.0:5000 --protocol = http -w wsgi:app When I point a browser to the droplet IP address followed by :5000 , I see the simple message again: \"The temperature is 91.2 F\". The flask app still seems to be working! Construct the uWSGI configuration file Now for another layer of uWSGI goodness- building a uWSGI .ini configuration file. ( flaskappenv ) $ deactivate $ pwd # ~/flaskapp $ nano flaskapp.ini Inside the flaskapp.ini file, include the following: [uwsgi] module = wsgi:app master = true processes = 5 socket = flaskapp.sock chmod-socket = 660 vacuum = true die-on-term = true Construct a systemd file Because we want to have the flask app running all the time, let's create a systemd control file to get the flask app running as a system service on the server. $ sudo nano /etc/systemd/system/flaskapp.service In the flaskapp.service file, I included the following as described in the Digital Ocean tutorial . Note the username peter should be replaced with your non-root sudo user. [Unit] Description=uWSGI instance to serve flaskapp After=network.target [Service] User=peter Group=www-data WorkingDirectory=/home/peter/flaskapp Environment=\"PATH=/home/peter/flaskapp/flaskappenv/bin\" ExecStart=/home/peter/flaskapp/flaskappenv/bin/uwsgi --ini flaskapp.ini [Install] WantedBy=multi-user.target Test with systemctl After the flaskapp.service file is created, we need to reload the systemctl daemon before starting the flaskapp service. $ sudo systemctl daemon-reload $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp The status call should show the service as active (running) . Something like the message below. flaskapp.service - uWSGI instance to serve flaskapp Loaded: loaded (/etc/systemd/system/flaskapp.service; disabled; vendor preset Active: active (running) since Wed 2018-09-12 18:09:15 UTC; 7s ago Use [ctrl-c] to exit the status screen. [ctrl-c] will not stop the service. Configure NGINX and apply SSL security We'll use NGINX as a proxy server to work with uWSGI and the flask app. The general control flow resulting from GET request will be: GET request  NGINX  uWSGI  flaskapp Install NGINX Before we can use NGINX, NGINX needs to be installed on the server. Installation is a simple apt-get command. Note that NGINX starts running as soon as it is installed. $ sudo apt-get install nginx Configure NGINX To use NGINX as part of the web stack, we need to create a configuration file in the /etc/nginx/sites-available/ directory. The Digital Ocean tutorial was really helpful for this step. NGINX configuration was something I struggled with when I built my Jupyter Hub server . $ sudo nano /etc/nginx/sites-available/flaskapp Edit the flaskapp NGINX config file the /sites-available directory to include the following. Make sure to change the your_domain and www.your_domain fields: server { listen 80; server_name your_domain wwww.your_domain; location / { include uwsgi_params; uwsgi_pass unix:/home/peter/flaskapp/flaskapp.sock; } } Now we'll link the NGINX config file to the /etc/nginx/sites-enabled directory and restart NGINX with the new configuration. If something doesn't look right on the systemctl status screen, you can check for problems with the command sudo nginx -t . $ sudo ln -s /etc/nginx/sites-available/flaskapp /etc/nginx/sites-enabled $ sudo systemctl restart nginx $ sudo systemctl status nginx #ctrl-c to exit Now that NGINX and uWSGI are running, let's also shut off the :5000 development port. $ sudo ufw delete allow 5000 $ sudo ufw allow 'Nginx Full' Browse to the web address of the server. This time you won't need to append the address with :5000 . Also the web address can be your domain name, not just the server IP address You should see the message: \"The temperature is 91.2 F\". http://mydomain.com/ Apply SSL Security One of the reasons for getting a real domain name is so the server can run with SSL security and https. Adding SSL can be done with certbot , a Python program that assists with generating SSL certificates. I followed the Digital Ocean tutorial steps to acquire the certificate. Make sure to replace your_domain with your actual domain name. $ sudo add-apt-repository ppa:certbot/certbot $ sudo apt install python-certbot-nginx $ sudo certbot --nginx -d mydomain.com -d www.mydomain.com As part of the certbot setup, I selected option 2. 2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for new sites, or if you're confident your site works on HTTPS. You can undo this change by editing your web server's configuration. If certbot is successful, you will see a message similar to this: IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/mydomain.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/mydomain.com/privkey.pem ``` Now we no longer need to run NGINX with HTTP, since we can now run NGINX with HTTPS. All the HTTP traffic will be forwarded to HTTPS. ```bash $ sudo ufw delete allow 'Nginx Full' $ sudo ufw allow 'Nginx HTTPS' Add Bootstrap styling The single page app is pretty basic right now. It also isn't designed to look good on phones or tablets. I plan on using my phone to view the flask app most of the time, so I decided to use bootstrap styling and the jumbotron component from bootstrap in the flask app. To keep things simple, I used the bootstrap CDN instead of installing the whole bootstrap package to the server. On the bootstrap3 install page is the content we need to add to the top of our .html template. To make the temperature display look nicer, I utilized the bootstrap jumbotron component . If you follow the link, you will see a couple lines of html that need to be included first in the <header> portion of the template. To add the bootstrap styling I created a jinga template called index.html and placed a modified version of the html for the jumbotron component and bootstrap CDN inside. On the server, we need to create a templates directory to store the jinja template. <main_app>/templates is the default location for jinga templates when running flask . $ cd ~/flaskapp $ mkdir templates $ cd templates $ nano index.html The index.html template contains a <header> with the bootstrap3 CDN and a <body> which contains the jumbotron component. <!-- index.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > show temp </ title > <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity = \"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" integrity = \"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" integrity = \"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\" crossorigin = \"anonymous\" ></ script > </ head > < body > < div class = \"container-fluid\" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > 91.4 F </ h1 > < p class = \"lead\" > temperature inside </ p > < hr class = \"my-4\" > </ div > </ div > </ body > </ html > Now we need to modify the flaskapp.py file to point to our index.html template. A new flask function, render_template() is used. render_template must be included in the imports and is used as the return action of the @app.route(\"/\") index() function. $ nano ~/flaskapp/flaskapp.py The revised flaskapp.py file is below. # flaskapp.py from flask import Flask , render_template app = Flask ( __name__ ) @app.route ( \"/\" ) def index (): return render_template ( \"index.html\" ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' ) We can view our changes by reloading the flaskapp system service and browsing to the server domain's main page. $ sudo systemctl stop flaskapp $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # [ctrl-c] to exit Pull the temperature from ThingSpeak.com with requests The final step of this flask single page app project is to dynamically pull the temperature from ThingSpeak.com and show it as a web page. Right now the flask app only shows the static temperature 91.4 F . However, the whole point of the app is to see the current temperatures the WiFi weather stations measure. To grab the temperatures off of ThingSpeak.com, we'll use the requests package. According to the ThingSpeak.com web API documentation , the format of our GET request needs to be: https://api.thingspeak.com/channels/<channel_id>/fields/<field_id>/last.<format> <channel_id> corresponds to the channel number on ThingSpeak.com. My WiFi weather stations are on a public channel. <field_id> is the field number held by the ThingSpeak channel. Each ThingSpeak channel can have multiple fields. The temperature we care about is in field 1 . The <format> we want is .txt . We could grab .json or a .csv off of ThingSpeak, but since we only need one temperature reading at a time, .txt is the easiest. Let's try out the ThingSpeak web API using the Python REPL. Make sure requests is installed in the virtual environment before importing it. On the server try: $ source ~/flaskapp/flaskappenv/bin/activate ( flaskappenv ) $ python >>> import requests >>> r = requests.get ( 'https://api.thingspeak.com/channels/266256/fields/2/last.txt' ) >>> print ( r.text ) -1 >>> exit () ( flaskappenv ) $ deactivate $ Now we need to use this same web API call shown above in the flask app. Modify flaskapp.py to include the requests package and include the web API request as a line the index() function. I also included a line to convert the temperature from F to C. When the temperature value comes in from ThingSpeak, it is a string. The temperature value needs to be converted to a float before the C to F conversion can be accomplished. After the conversion, the temperature in F needs to be converted back to a string. A string is needed because the temperature in F is passed to the render_template() function as the parameter temp will be used in a revised version of our jinja template index.html . The extra argument in the render_template() function transfers the variable temp_f from the flaskapp.py file to the jinja template index.html . $ nano ~/flaskapp/flaskapp.py The modified flaskapp.py script is below: # flaskapp.py from flask import Flask , render_template import requests app = Flask ( __name__ ) @app.route ( \"/\" ) def index (): r = requests . get ( 'https://api.thingspeak.com/channels/254616/fields/1/last.txt' ) temp_c_in = r . text temp_f = str ( round ((( 9.0 / 5.0 ) * float ( temp_c_in ) + 32 ), 1 )) + ' F' return render_template ( \"index.html\" , temp = temp_f ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' ) Finally, we need to modify the index.html template and test the whole flask app. We passed a parameter temp from flaskapp.py to this template. The temp parameter can be used programmatically in the jinja index.html template. The value stored in temp will end up displayed on the working web page. Jinja templates use code blocks that start and end with double curly brackets {{ }} . Our temp parameter goes into one of these blocks. $ cd ~/flaskapp/templates $ nano index.html The revised index.html file is below: <!-- index.html --> <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < title > show temp </ title > <!-- Latest compiled and minified CSS --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity = \"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin = \"anonymous\" > <!-- Optional theme --> < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" integrity = \"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\" crossorigin = \"anonymous\" > <!-- Latest compiled and minified JavaScript --> < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" integrity = \"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\" crossorigin = \"anonymous\" ></ script > </ head > < body > < div class = \"container-fluid\" > < div class = \"jumbotron\" > < hr class = \"my-4\" > < h1 class = \"display-4\" > {{ temp }} </ h1 > < p class = \"lead\" > temperature inside </ p > < hr class = \"my-4\" > </ div > </ div > </ body > </ html > View the final flask app online With the changes to readtemp.py and index.html complete, we can restart the flask app system service and view our app with a web browser. $ sudo systemctl stop flaskapp $ sudo systemctl start flaskapp $ sudo systemctl status flaskapp # [ctrl-c] to exit The final single page flask web app is complete! If everything is working correctly, you should see the working app running on your domain looks like this. Summary It was a long process to construct this flask single page webapp project. A lot for technologies and languages were used. An incomplete list is below: cloud servers DNS Servers Linux SSH and SSH keys PuTTY Python Flask systemd uWSGI NGINX SSL and certbot web API's jinja templates html bootstrap That's a lot of stuff to go in one project. The next thing I'm thinking about is building a flask IoT (internet of things) server that accepts GET requests from ESP8266 weather stations . ThingSpeak.com works great as an IoT sever, but there are limits to how often data can be posted and how often data can be accessed. I think writing my own IoT server in flask would be fun too!","tags":"flask","url":"flask-app-on-digital-ocean.html"},{"title":"Upload Micropython Code to an Adafruit Feather Huzzah ESP8266","text":"This is the sixth part of a multi-part series on Micropython. In this post, we will upload .py files to an Adafruit Feather Huzzah ESP8266 board using Python and a Python package called ampy . At the end of the post we will have a working WiFi weather station that will post the temperature to ThingSpeak.com Before we can use the Microython REPL (the Micropython prompt) running on the Adafruit Feather Huzzah ESP8266, Micropython needs to be installed on the ESP8266 board and PuTTY needs to be installed on the computer to communicate with the board over a serial connection. See a previous post on how to install Micropython on an ESP8266 board and how to install PuTTY on a Windows 10 machine. Summary of Steps: Install ampy with pip Write Python code in .py files Upload the .py files to the board with ampy Unplug and power up the Feather Huzzah and watch the data on ThingSpeak.com 1. Install ampy with pip Ampy is a tool written by the good folks at Adafruit. Ampy is used to upload files onto the ESP8266. Since I'm using a virtual environment, I need to activate the virtual environment first before installing ampy . Note that the tool is called ampy , but we pip install ampy-adafruit . $ conda activate micropython ( micropython ) $ pip install ampy-adafruit ( micropython ) $ ampy --help 2. Write Python code in .py files Now we need to write the Python code in .py files that will run on the ESP8266 board. The board already contains two main Python files: boot.py and main.py . We can also add additional files to the board. boot.py is the file that runs first when the board is powered up. After boot.py runs, then main.py runs. We can add other .py files to the board to provide main.py some functions and classes to work with. We have two general things to do with our Feather board: read the temperature and post the temperature to the ThingSpeak.com. We'll use a different .py file for each of these two general tasks. The first module, MCP9808.py , will simplify reading temperature data off of the Adafruit MCP9808 temperature sensor breakout. We need to write a readtemp() function that parses out the temperature data from the I2C bus and outputs the temperature as a float. The readtemp() function needs to import the machine module to use the I2C bus. The machine module allows us to create a new I2C object. When we instantiate the I2C object object, we need to specify the scl and sda pins connected to the sensor. scl is the I2C clock line and sda is the I2C data line. on the Adafruit Feather Huzzah scl is pin 5 and sda is pin 4 . Then a new byte array variable needs to be created. A byte array is needed to store the temperature data when it comes over the I2C line from the sensor to the board. Then we need to read the sensor data using the i2c.readfrom_mem_into() function. The first argument is the I2C bus address of the sensor. In our case, the sensor is at I2C bus address 24 . You can use the line >>> i2c.scan() in the Micropython REPL to see this value. The next argument passed to the i2c.readfrom_mem_into() function is the register on the MCP9808 temperature sensor where the temperature value is stored. The temperature is stored on the MCP9808 in register 5 . When we access register 5 on the MCP9808, we read in the temperature. The final argument passed into the i2c.readfrom_mem_into() function is the byte array variable that stores the temperature data. The i2c.readfrom_mem_into() function modifies the variable passed to it as a function argument, rather than producing a variable which is the function output as most functions do. This is why we needed to first create the byte_data variable before calling the i2c.readfrom_mem_into() function. Finally, we need to do some post processing of the byte array to transform it into a temperature in degrees C. The complete readtemp() function is below: # MCP9808.py # Functions for the MCP9808 temperature sensor # https://learn.adafruit.com/micropython-hardware-i2c-devices/i2c-master def readtemp (): import machine i2c = machine . I2C ( scl = machine . Pin ( 5 ), sda = machine . Pin ( 4 )) byte_data = bytearray ( 2 ) i2c . readfrom_mem_into ( 24 , 5 , byte_data ) value = byte_data [ 0 ] << 8 | byte_data [ 1 ] temp = ( value & 0xFFF ) / 16.0 if value & 0x1000 : temp -= 256.0 return temp Now we'll build a Python file that contains a set of WiFi functions called wifitools.py . We used this same functionally in a previous post to connect the ESP8266 to a WiFi network. In addition to the WiFi functions, we also need a function to build the ThingSpeak.com web API URL. This is the URL we will request in order to get our temperature posted on ThingSpeak.com. #wifitools.py # Wifi connection and ThingSpeak.com post functions for an ESP8266 board running Micropython #https://docs.micropython.org/en/v1.8.6/esp8266/esp8266/tutorial/network_basics.html def connect ( SSID , password ): import network sta_if = network . WLAN ( network . STA_IF ) if not sta_if . isconnected (): print ( 'connecting to network...' ) sta_if . active ( True ) sta_if . connect ( SSID , password ) while not sta_if . isconnected (): pass print ( 'network config:' , sta_if . ifconfig ()) #https://docs.micropython.org/en/v1.8.6/esp8266/esp8266/tutorial/network_tcp.html def http_get ( url ): import socket _ , _ , host , path = url . split ( '/' , 3 ) addr = socket . getaddrinfo ( host , 80 )[ 0 ][ - 1 ] s = socket . socket () s . connect ( addr ) s . send ( bytes ( 'GET / %s HTTP/1.0 \\r\\n Host: %s \\r\\n\\r\\n ' % ( path , host ), 'utf8' )) while True : data = s . recv ( 100 ) if data : print ( str ( data , 'utf8' ), end = '' ) else : break def thingspeak_post ( API_key , data ): if not isinstance ( data , str ): data = str ( data ) if not isintance ( API_key , str ): API_key = str ( API_key ) base_url = 'https://api.thingspeak.com/update' API_key = '?api_key=' + API_key field = '&field1=' url = base_url + API_key + field + data http_get ( url ) Now let's write a script in a file called main.py which will use the functions in our wifitools.py and MCP9808.py files. This main.py script will import our MCP9808 and wifitools modules and use the wifitools.connect() function to connect the ESP8266 to a WiFi network. There is a time.sleep(5) line to allow the board time to connect to the WiFi network. Next we'll run a loop for a total of 8 hours (with 60 minutes in each hour). Inside the loop, we'll read the temperature off the MCP9808 using the MCP9808.readtemp() function and post the temperature to ThingSpeak.com using the wifitools.thingspeak_post() function. To read the temperature once a minute, we need to time.sleep(60) (wait 60 seconds) between each measurement. I also have one more .py file called config.py . This file simply contains three variables: SSID , WIFI_PASSWORD and API_KEY . By using a config file, we can keep our passwords and API keys out of version control. Like functions and classes, we can import variables defined in .py files for use in another script. # main.py # Adafruit Feather Huzzah ESP8266 WiFi Weather Station import wifitools import MCP9808 import time import config api_key = config . API_KEY ssid = config . SSID password = config . WIFI_PASSWORD wifitools . connect ( ssid , password ) time . sleep ( 5 ) for i in range ( 8 * 60 ): data = MCP9808 . readtemp () wifitools . thingspeak_post ( api_key , data ) time . sleep ( 60 ) 3. Upload the .py files to the board with ampy Once all the .py files are created, ensure the Adafruit Feather Huzzah ESP8266 board is connected with a USB cable to the computer. You will also need to know what serial port the Feather board is connected to. We'll upload the code files to the board using ampy . Make sure you are in the directory with the .py files and that you are working in the (micropython) virtual environment that has ampy installed in it. $ conda activate micropython ( micropython ) $ ampy --port COM4 put MCP9808.py ( micropython ) $ ampy --port COM4 put wifitools.py ( micropython ) $ ampy --port COM4 put main.py ( micropython ) $ ampy --port COM4 put config.py ( micropython ) $ ampy --port COM4 ls boot.py wifitools.py MCP9808.py config.py main.py 4. Unplug and power up the Feather Huzzah and watch the data on ThingSpeak.com The Feather Huzzah needs to be restarted to run the code we just uploaded. To restart the board, unplug and then replug the board's power. Once power is restored, the board will run through the boot.py script then start the main.py script. When the board runs the main.py script, the board will connect to the WiFi network, read the temperature from the sensor then upload the temperature to ThingSpeak.com. If we go to ThingSpeak.com, we should see the temperature plotted on our Channel's page. Congrats! You have a working weather station that is part of the Internet of Things. Now you can read the temperature from anywhere in the world with an internet connection!","tags":"micropython","url":"micropython-upload-code.html"},{"title":"Using Micropython to connect an Adafruit Feather Huzzah ESP8266 to WiFi","text":"This is the fifth part of a multipart series on Micropython. In the last post we used the Micropython REPL (the Microcpython prompt) running on the Adafruit Feather Huzzah ESP8266 board to read the temperature off a temperature sensor. In this post, we are going to connect the Feather board to WiFi and post the temperature to ThingSpeak.com Before we can connect the Adafruit Feather Huzzah to WiFi, Micropython needs to be installed on the board and PuTTY needs to be installed on the computer to communicate with the board over serial. See a previous post on how to install Micropython on an ESP8266 board and how to install PuTTY on a Windows 10 machine. Summary of Steps: Wire up the temperature sensor to the Adafruit Feather Huzzah ESP8266 Use PuTTY to connect to the Adafruit Feather Huzzah ESP8266 board Run commands at the Micropython REPL to connect the Feather board to WiFi Upload the temperature to ThingSpeak.com 1. Wire up the temperature sensor to the Adafruit Feather Huzzah ESP8266 Wire up the MCP9808 temperature sensor to the Adafruit Feather Huzzah ESP8266 as shown in a previous post . 2. Use PuTTY to connect to the Adafruit Feather Huzzah ESP8266 board Connect the Feather to the computer using a USB data cable. Open PuTTY. Ensure the serial port (Serial line) is set correctly and the baud rate (speed) is set to 115200. 3. Run commands at the Micropython REPL to connect the Feather board to WiFi To connect the ESP8266 board to a WiFi network, we first need to import the network module and create an instance of the WLAN class. Next we use the connect method and our WiFi network's SSID and password to connect. We want to run our ESP8266 in station mode (like a laptop or phone) as opposed to access point mode (like a server). We can print the IP address of the board using the ifconfig() method. The commands below should be typed into the Micropython REPL. Note that 'SSDI' and 'password' should be replaced with an actual WiFi SSID and password. >>> import network >>> sta_if = network.WLAN(network.STA_IF) >>> sta_if.active(True) >>> sta_if.connect('SSID', 'password') >>> print('network config:', sta_if.ifconfig()) Now that the Feather board is connected to a WiFi network, we can use the board to read a webpage. In Micropython, reading webpages is done with sockets . A socket is a connection between the ESP8266 and the outside internet. >>> import socket >>> url = 'https://google.com' >>> _, _, host, path = url.split('/', 3) >>> addr = socket.getaddrinfo(host, 80)[0][-1] >>> s = socket.socket() >>> s.connect(addr) >>> s.send(bytes('GET /%s HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' % (path, host), 'utf8')) >>> while True: .... data = s.recv(100) .... if data: .... print(str(data, 'utf8'), end='') .... else: .... break 4. Upload the temperature to ThingSpeak.com Now imagine our weather station is up and working and reads a temperature of 21 C. We are going to push this temperature data reading (21 C) up to ThingSpeak.com . ThingSpeak.com is an Internet of Things (IoT) cloud service provider. We'll upload the temperature to ThingSpeak.com using an http GET request in the format required by the ThingSpeak web API . Sign up for an account on ThingSpeak.com and create a new channel. In the ThingSpeak channel, create a new field called temperature . Note the ThingSpeak channel number and ThingSpeak write API key . The write API key will be needed to send our temperature up to ThingSpeak.com and the channel number is needed to view the temperature. At the Micropython REPL, we'll build a new function called http_get() which will initiate the http GET action by the Feather board. We can then feed this http_get() function a specific URL that will activate the ThingSpeak web API and post the temperature to the cloud on ThingSpeak.com. >>> def http_get(url): ... import socket ... _, _, host, path = url.split('/', 3) ... addr = socket.getaddrinfo(host, 80)[0][-1] ... s = socket.socket() ... s.connect(addr) ... s.send(bytes('GET /%s HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' % (path, host), 'utf8')) ... while True: ... data = s.recv(100) ... if data: ... print(str(data, 'utf8'), end='') ... else: ... break Now we need to build the URL that will call the ThingSpeak web API. The URL contains four parts: A base domain, an API key, a field number and data. Let's assume our temperature data is 21 (for 21 C). We can define these URL parts as strings then combine the stings to create one long URL. Note that you should include your actual API key here, not a bunch of XXXXXXX's. Since the entire URL is a string, it is important to assign the string '21' to the variable data instead of assigning the integer 21 . >>> base_url = 'https://api.thingspeak.com/update' >>> API_key = '?api_key=XXXXXXXXXXXXXX' >>> field = '&field1=' >>> data = '21' >>> url = base_url + API_key + field + data Now we can use the http_get() function and our URL to send the temperature 21 up to ThingSpeak.com >>> http_get(url) If you go to https://thingspeak.com/<your channel number> , you should see a new temperature point. Now let's read an actual temperature off of the MCP9808 temperature sensor and send that value up to ThingSpeak.com. >>> import machine >>> i2c = machine.I2C(scl=machine.Pin(5), sda=machine.Pin(4)) >>> byte_data = bytearray(2) >>> i2c.readfrom_mem_into(24, 5, byte_data) >>> value = byte_data[0] << 8 | byte_data[1] >>> temp = (value & 0xFFF) / 16.0 >>> if value & 0x1000: ... temp -= 256.0 ... print(temp) >>> base_url = 'https://api.thingspeak.com/update' >>> API_key = '?api_key=XXXXXXXXXXXXXX' >>> field = '&field1=' >>> data = str(temp) >>> url = base_url + API_key + field + data >>> http_get(url) Another data point should now be shown on ThingSpeak.com. Next steps Pretty amazing to have a little piece of hardware like the Feather Huzzah ESP8266 wirelessly upload a temperature measurement up to the cloud. It did require a lot of typing at the Micropython REPL though. In the next post, we'll build a couple of .py files to save the functions we wrote to read the temperature, build the URL and complete the http GET request. Then we will upload these .py files to the Feather board.","tags":"micropython","url":"micropython-wifi.html"},{"title":"Add a custom login page and assignments directory for each user on a Jupyter Hub server","text":"This is the seventh part of a multi-part series that shows how to set up Jupyter Hub for a college class. In this post, we build a pre-spawn hook that creates an \"assignments\" and \"notes\" directory with pre-constructed assignments and notes for each jupyterhub user. We also build a custom login pages that looks much more like our college login page and contains helpful links. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub Add a custom login page and assignments directory for each user on a Jupyter Hub server (this post) Last time In the last post , we set jupyterhub to run as a system service in the background on our server. Then we added tried two different login systems: github and google. The github authentication system allowed user to log in with github usernames and passwords. The google authentication system allowed users to log in with their college usernames and passwords. Then we modified the jupyterhub_config.py file to all the creation of new users on the server even if the new usernames contained a dot. Steps in this post: Create a repo on github.com with the assignments and notes Add a pre-spawn hook in jupyterhub_config.py Create a templates directory and populate it with Jinja templates to create a new login page Modify the style.min.css file to apply styling to the login page 1. Create a repo on github.com with the assignments On github.com, create a new repo with the notes and assignments for the quarter. On a local computer, not the server, clone the github repo. This allows us to work on the notes and assignments locally. # local computer $ mkdir ENGR101 $ cd ENGR101 $ git init $ git remote add origin https://github.com/ProfessorKazarinoff/ENGR101.git $ git pull origin master On the local computer, not the server, build the assignment and notes for the quarter. I did this using jupyter notebooks. Then add, commit and push the changes up to github. # local computer $ git add . $ commit -m \"added assignments and notes\" $ git push origin master 2. Add a pre-spawn hook in jupyterhub_config.py Now we can go to the server and have the notebooks we created (and pushed up to github) pre-populate each users directory tree when they log into jupyterhub . We'll use a Python package called gitpython to help with pulling the notebooks down from github.com. Log into the server and install gitpython using conda : # on the server $ sudo apt-get update $ conda install -c conda-forge gitpython Now we need to modify the jupyterhub_config.py file to do a couple things: Run a pre-spawn hook function that runs before each user's jupyter notebook server is started and pull the assignments and notes down from github as part of the pre-spawn hook function. The pre-spawn hook function gets called every time a user logs into jupyterhub. This pre-spawn hook will run before the user's jupyter notebook server is created. In the pre-spawn hook, we want to check to see if the user has the assignments and notes pulled down from github already loaded. If the user doesn't have the assignments, then we want to pull the assignments down from github and put them in the user's directory tree. So first we need a function that will pull the repo down from github. Note the line uid = getpwnam(user).pw_uid and gid = getpwnam(user).pw_gid in the function below. These lines of code get the user's numerical unix user id and group id. The userid and group id are needed to assign the proper permissions to the files we pull down from github. When I first built the function, changing file permissions was not included. I could log onto jupterhub and see the notebooks pulled down from github, but I couldn't run or edit them. The problem was that the notebooks were pulled down from github by a sudo user and the jupyterhub user didn't have the permissions to write or execute any of the files. Building the permissions into the function with shutil.chown() solved the problem. # in jupyterhub_config.py def clone_repo ( user , git_url , repo_dir ): \"\"\" A function to clone a github repo into a specific directory of a user. \"\"\" Repo . clone_from ( git_url , repo_dir ) uid = getpwnam ( user ) . pw_uid gid = getpwnam ( user ) . pw_gid for root , dirs , files in os . walk ( repo_dir ): for d in dirs : shutil . chown ( os . path . join ( root , d ), user = uid , group = gid ) for f in files : shutil . chown ( os . path . join ( root , f ), user = uid , group = gid ) Now we'll build a pre-spawn hook function that will run when the spawner starts. The function will call the clone_repo() function and pull down the assignments from the github repo the first time a user logs into jupyterhub. After the assignments and notes are initially created, each subsequent time the user logs into jupyterhub, a new fresh set of assignments and notes are pulled down if ERASE_DIR is set to True . If ERASE_DIR is set to False , once the assignments and notes are downloaded, they will not be over-written. To run the pre-spawn hook function and the pull repo function, we need to make sure the following imports are present in our jupyterhub_config.py file: # jupyterhub_config.py import git , os , shutil from pwd import getpwnam The complete pre-spawn hook function is below: # in jupyterhub_config.py def create_dir_hook ( spawner ): \"\"\" A function to clone a github repo into a specific directory of a jupyterhub user when the server spawns a new notebook instance. \"\"\" username = spawner . user . name DIR_NAME = os . path . join ( \"/home\" , username ) git_url = \"https://github.com/ProfessorKazarinoff/ENGR101.git\" repo_dir = os . path . join ( DIR_NAME , 'notebooks' ) if ERASE_DIR == True : if os . path . isdir ( repo_dir ): shutil . rmtree ( repo_dir ) os . mkdir ( repo_dir ) clone_repo ( username , git_url , repo_dir ) if ERASE_DIR == False and not ( os . path . isdir ( repo_dir )): os . mkdir ( repo_dir ) clone_repo ( username , git_url , repo_dir ) if ERASE_DIR == False and os . path . isdir ( repo_dir ): pass The two functions need to be pasted into the jupyterhub_config.py file. Make sure the imports are present as well as an ERASE_DIR = True or ERASE_DIR = False line in the jupyterhub_config.py file too. Next we need to add a pre-spawn hook function to the spawner object in our jupyterhub_config.py file in the form of # in jupyterhub_config.py c . Spawner . pre_spawn_hook = create_dir_hook With these changes complete, we can restart jupyterhub using: $ sudo systemctl stop jupyterhub $ sudo systemctl start jupyterhub $ sudo systemctl status jupyterhub To exit the status screen use [Ctrl] + [c] 3. Create a templates directory and populate it with Jinja templates to create a new login page The jupyterhub login page looks like this: But our college login page looks lik this: For users to feel comfortable with logging into the jupyterhub server, we'll make the jupyterhub login page look more like the college login page. This was a time consuming and fussy task. It involved a lot of messing around with css and html. First a set of custom jinja templates need to be created. When jupyterhub runs, there is a set directory of jinja templates that build the html users see when they go to the site. These jinga templates are burried deep in the jupyterhub package code. For my jupyterhub installation on the server, I found the jinja template files in the /pkgs directory in anaconda3 . If you are using a virtual environment, the jupyterhub package directory name will likey be different: /home/peter/anaconda3/pkgs/jupyterhub-0.8.1-py36_0/share/jupyter/hub/templates/  404.html  admin.html  error.html  home.html  login.html  logout.html  page.html  spawn.html  spawn_pending.html  token.html Now we need to copy these templates into our home directory. Once copied, we can modify the templates and create a new jupyterhub login page. The login.html file is the one that needs to be customized. $ cd /home/peter/anaconda3/pkgs/jupyterhub-0.8.1-py36_0/share/jupyter/hub/ $ cp -R templates /home/peter/templates/ $ cd ~ $ cd templates $ ls Open up the login.html file and modify it with any html that you want to show up when a user goes to the jupyter hub site. I messed around for WAY to long trying to get my custom login page to look like the college login page. An important piece of html that needs to stay in the login.html file is the <a> tag that links to the authentication url. The complete tag is detailed below <! login.html > <a role=\"button\" class=\"btn btn-jupyter btn-lg\" href=\"/hub/oauth_login?next=\"> Sign in with Portland Community College </a> I also kept in the jinga tag at the top of the file that brings in all of the formatting from login.html's parent template page.html <! login.html > {% extends \"page.html\" %} All the changes I made to the login template were inside the \"login\" block of login.html. You can find my complete login.html file here {% block login %} <! make changes here > {% endblock login %} Now we need to modify the jupyterhub_config.py file so that our new set of custom jinja templates are used instead of the default jinja templates. A problem I initially had was I set the directory path of the custom templates as /home/peter/templates and the login page didn't work as expected. When I changed the directory path to /home/peter/templates/ the problem was resolved. # jupyterhub_config.py # sets a custom html template at the login screen. c . JupyterHub . template_paths = [ '/home/peter/templates/' ] 4. Modify the style.min.css file to apply styling to the login page Finally the style.min.css file needs to be modified so that the login page styling looks a little more like the college login page. This is another thing I messed around with for a long time, a WAY to long time. I couldn't figure out a way to get jupyterhub to use a custom .css file. I tried creating a .css file in the new custom templates directory, but jupyterhub wouldn't copy it as a static asset when the server launched. I also tried putting a separate .css file deep inside of the jupyterhub package code. When the server ran, it seemed to copy the custom .css file (I could see the custom .css file using chrome's inspect element tool). But for some reason the custom .css file would be blank when server serve was running, even though the custom .css file contained a whole bunch of css code when viewed deep in the jupyterhub package code. The solution I finally got to work was modifying the style.min.css file itself that jupyterhub uses. This file is buried deep in the jupyterhub package code: home/peter/anaconda3/pkgs/jupyterhub-0.8.1-py36_0/share/jupyter/hub/static/css/  style.min.css  style.min.css.map Modify the style.min.css file to include all the custom css styling desired (find my complete css file here ) With changes to the login.html file and style.min.css file complete, we can restart jupyterhub and view the changes rendered on the login page. $ sudo systemctl stop jupyterhub $ sudo systemctl start jupyterhub $ sudo systemctl status jupyterhub Below is the look of my modified login page in all it's custom html and css glory Summary In this post, we will built a pre-spawn hook that pulls down notes and assignments from github and adds it to each user's directory tree when their jupyter notebook server starts. We also constructed a custom login page by creating a custom jinja template ( login.html ) and modified the style.min.css file inside the jupyterhub package code. The resulting custom login pages looks a lot more like our college login page. Conclusion This concludes the Jupyter Hub series. We accomplished a lot to get a working version of Jupyter Hub up and running on a Digital Ocean server. Our Jupyter Hub deployment has the following features: Users don't need to install anything to edit and run Python code. Users just need a web browser and an internet connection. All users have a common set of installed Python packages and don't need to install any extra packages A custom domain name is hooked to jupyterhub instead of a numeric IP address jupyterhub runs on https and has SSL security Users can log into jupyterhub using their college usernames and passwords Each user's directory tree is pre-populated with assignments and notes pulled down from github The pre-populated assignments and notes can be run and modified by users. You can find the notes and assignments that pre-poplulate into each user's directory tree here You can find the various files that are part of the jupyterhub deployment here I Hope this series is helpful to anyone who wants to set up Jupyter Hub for their own class or team. After the course runs, I want to post the results of students using Jupyter Hub. The first course to use Jupyter Hub runs this summer quarter.","tags":"jupyter","url":"assignments-dir-and-custom-login-page-to-jupyterhub.html"},{"title":"Adding Google OAuth and system service to a Jupyter Hub server","text":"This is the sixth part of a multi-part series that shows how to set up Jupyter Hub for a college class. In this post, we will set up jupyterhub to run as a system service in the background which will allow us to work on the server and run jupyterhub at the same time. Then we will add an authentication system so that users can log into our Jupyter Hub server using github usernames and passwords. Finally we will modify the authentication system so that users can log into our Jupyer Hub server using their google usernames and passwords. The same user name and password a student uses to access their college email. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub (this post) Pre-populate each new user's directory tree to include three notebook assignments. Last time In the last post, we succeeded in getting jupyterhub to run on https and use SSL certificates. We created SSL certificates, modified the nginx config and modified the jupyterhub config. At the end of it we were able to get a working version of jupyter hub running SSL security. Steps in this post: Run jupterhub as a system service Test local OAuth Acquire github OAuth credentials Modify jupyterhub_config.py to use github OAuth Acquire google OAuth credentials Modify jupyterhub_config.py to use google OAuth 1. Run jupterhub as a system service Working off of this wiki To run jupyterhub as a system service, we need to create a service file in the /etc/systemd/system directory. cd into the directory and have a look around. You should see a couple files that end in .service $ cd /etc/systemd/system $ ls cloud-init.target.wants network-online.target.wants dbus-org.freedesktop.thermald.service paths.target.wants default.target.wants sockets.target.wants final.target.wants sshd.service getty.target.wants sysinit.target.wants graphical.target.wants syslog.service iscsi.service timers.target.wants multi-user.target.wants We'll create a new .service file called jupyterhub.service $ sudo nano jupyterhub.service In the file, add the following. Note that as part of the PATH environment variable /home/peter/anaconda3/bin is included. This is the path to our Anaconda environment. As part of the ExecStart we include a flag for our jupyterhub_config.py file. [Unit] Description = Jupyterhub After = syslog.target network.target [Service] User = root Environment = \"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/home/peter/anaconda3/bin\" ExecStart = /home/peter/anaconda3/bin/jupyterhub -f /home/peter/jupyterhub_config.py [Install] WantedBy = multi-user.target Save and exit with [Ctrl-c] + [y]. Now we need to reload the system daemon and run jupyterhub as a system process using the command: sudo systemctl <start|stop|status> jupyterhub $ sudo systemctl daemon-reload $ sudo systemctl start jupyterhub We can see if jupyterhub is running with: $ sudo systemctl status jupyterhub Loaded: loaded ( /etc/systemd/system/jupyterhub.service ; Active: active ( running ) 2. Test local OAuth Now we can go to the server and log in as our non-root user, and log in as the other user we created kendra . A couple times I thought that jupyterhub was running after using systemctl start jupyterhub , but the hub wasn't working when I went to the hub's web address. It turned out that jupyterhub wasn't running when I keyed in systemctl status jupyterhub . Most times looking for an error and tracking down the the error worked, but one time it seemed to be a problem with the http-configurable-proxy. The following command will shut down the proxy if you get stuck like I did (insert the number corresponding to the configurable-http-proxy process after the kill command): $ ps aux | grep configurable-http-proxy $ kill #### 3. Acquire Github OAuth credentials A problem now is that if we go to the admin page on jupyter hub, we can't add new users. The users have to be added to the server using PuTTY first and then can be added to jupyterhub with the admin panel. This is OK for a small team or a couple users, but for a college class, creating a new user on the server for each student, then emailing out passwords... Ah! what a mess. So we need to give jupyterhub the authority to create new users from the admin panel and we need a way to have users login with a user name and password they already have. One of the ways students could log into Jupyter Hub is using their github credentials. This would require each student to have a github account. A github account for each student might be worth it to give students exposure to git and github as a tools. So let's give the github authenticator a whirl. The github authenticator is also pretty well documented for Jupyter Hub, so it's good authenticator to try first. To use the github authenticator, we need to install oauthenticator . I couldn't find oauthenticator on conda-forge. If it's on conda-forge, I would install it from there rather than PyPI. But for this one, I used pip . $ pip install conda install oauthenticator Now we need to log into github and create an OAuth App and copy the Client ID and Client Secret. The short version is: github profile  settings  Developer Settings  OAuth Apps  Register a new application Set the Homepage URL as: https://notebooks.yourdomain.com/ Set the Authorization call-back URL as: https://notebooks.yourdomain.com/hub/oauth_callback in the App Settings page, we need to copy two settings: Client ID Client Secret These two long strings will need to be pasted into the jupyterhub_config.py file. 4. Modify jupyterhub_config.py to use github OAuth Now we'll edit the jupyterhub_config.py file to include a couple additional lines. Note in the configuration below, #c.Authenticator.whitelist is commented out. We want to see if a github user can log onto the server (which will automatically create a new user and spawn a jupyter notebook server) and run notebooks. Once we know the server is working, we can uncomment the white list and only allow in specific github usernames. Note c.LocalGitHubOAuthenticator.client_id and c.LocalGitHubOAuthenticator.client_secret are the long strings from our github OAuth App. #jupyterhub_config.py from oauthenticator.github import LocalGitHubOAuthenticator c . JupyterHub . authenticator_class = LocalGitHubOAuthenticator c . LocalGitHubOAuthenticator . oauth_callback_url = 'https://notebooks.problemsolvingwithpython.com/hub/oauth_callback' c . LocalGitHubOAuthenticator . client_id = 'xxxxxxxxxxxxxxxxxxxxxx' c . LocalGitHubOAuthenticator . client_secret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' c . LocalGitHubOAuthenticator . create_system_users = True #c.Authenticator.whitelist = {'peter','kendra'} c . Authenticator . admin_users = { 'peter' } Restart jupyterhub with $ sudo systemctl stop jupyterhub $ sudo systemctl start jupyterhub $ sudo systemctl status jupyterhub Browse over to the hub's URL and we should be able to log in with a github username and password. 4. Google Authenticator Now that the github authenticator works, we are going to get into the weeds of getting the google authenticator to work. Why google authenticator instead of github? Our college uses the gmail suite for both staff and students. When students log onto their college email, they are logging into gmail. Students can use google calendar and google drive with their college email account as well. So it is probably best that students log into juypter hub using the same google login that they use to access their college email, google drive and calendar. First up we need to set up a google OAuth instance. I did this using my personal gmail account rather than my college gmail account. Some parts of google suite are not available in my college profile like youtube and developer tabs. To obtain the google OAuth credentials, we need to log into the google API console https://console.developers.google.com/ and select [Credentials] on the lefthand menu. Next we'll create a new OAuth credential under [Credentials]  [Create Credentials]  [OAuth client ID]: To create a set of google OAuth credentials you will need to input: Authorized JavaScript origins: https://notebooks.yourdomain.com callback url: https://notebooks.yourdomain.com/hub/oauth_callback After creating a new set of google OAuth credentials, note the: client ID client secret These two longs strings will be included in our revised jupyterhub configuration. Once we get our google OAuth credentials, we need to edit jupyterhub_conf.py . Note the google OAuth credentials need to replace 'xxxxxxxxxxxxxxx' #jupyterhub_conf.py import os c = get_config () c . JupyterHub . log_level = 10 c . Spawner . cmd = '/home/peter/anaconda3/bin/jupyterhub-singleuser' # For Google OAuth Authentication from oauthenticator.google import LocalGoogleOAuthenticator c . JupyterHub . authenticator_class = LocalGoogleOAuthenticator c . LocalGoogleOAuthenticator . create_system_users = True c . LocalGoogleOAuthenticator . hosted_domain = 'yourcollege.edu' c . LocalGoogleOAuthenticator . login_service = 'Your College Name' c . LocalGoogleOAuthenticator . oauth_callback_url = 'https://notebooks.yourserver.com/hub/oauth_callback' c . LocalGoogleOAuthenticator . oauth_client_id = 'xxxxxxxxxxxxxxxxxxxxx' c . LocalGoogleOAuthenticator . oauth_client_secret = 'xxxxxxxxxxxxxxxxxxx\" #c.JupyterHub.cookie_secret_file = '/srv/jupyterhub/jupyterhub_cookie_secret' c . Authenticator . add_user_cmd = [ 'adduser' , '-q' , '--gecos' , '\"\"' , '--disabled-password' , '--force-badname' ] c . Authenticator . whitelist = { 'studnet.username' , 'faculty.username' } c . Authenticator . admin_users = { 'faculty.username' } This little line: c.Authenticator.add_user_cmd = ['adduser', '-q', '--gecos', '\"\"', '--disabled-password', '--force-badname'] was a real gottacha. Our college email addresses are in the form: firstname.lastname@college.edu So jupyterhub was trying to create users with dots . in their usernames. This doesn't work in linux. I tried creating a new user with a dot in their username and it asked me to use the --force-badname flag. So that is what we'll add to the c.Authenticator.add_user_cmd list. Otherwise the users will be able to authenticate, buy they won't get a new account on the server and they won't be able to run notebooks. Restart jupyterhub and browse to the web address attached to the server. $ sudo systemctl stop jupyterhub $ sudo systemctl start jupyterhub $ sudo systemctl status jupyterhub # [Ctrl + c] to exit The login window should look something like: We can log in with our google user name and password (college username and password). Pretty sweet. Summary In this post, we set jupyterhub to run as a system service in the background which allowed us to work on the server and run jupyterhub at the same time. Then we added a github authentication system so that users could log into our Jupyter Hub server using their github usernames and passwords. Then we modified the authentication system to use google user names and passwords even if the usernames contained a dot. Next Steps Up next we will see if we can populate each new user's working directory tree with a couple of notebooks that will be the assignments for the quarter. We'll see if we can pull these down from github so that the assignments can be edited by instructors and viewed by the students before the quarter starts.","tags":"jupyter","url":"add-google-oauth-and-system-service-to-jupyterhub.html"},{"title":"Adding SSL and a domain name to Jupyter Hub","text":"This is the fifth part of a multi-part series that shows how to set up Jupyter Hub for a college class. In this post, we are going to link a domain name to our server IP address, add SSL security and configure nginx to run as a proxy in between users and jupyterhub . Then we'll run jupyterhub over https using the SSL security we created. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx (this post) Connect OAuth to Jupyter Hub Connect to Jupyter Hub as student Last time In the last post , we installed Anaconda on the server using a shell script. Then we installed some extra Python packages such as pint , pyserial and schemdraw to our base conda environment. Next we installed jupyterhub , opened up port 8000 and ran jupyterhub for the first time! And remember we shut down jupyter hub very quickly because we ran it without any SSL security. Steps in this post: Link domain name to server IP address Install nginx and modify ufw Obtain SSL certificates with certbot Create a cookie secret and a proxy auth token Modify nginx config Generate jupyterhub_config.py and modify Restart nginx and start jupyterhub, see if we can login Create an new user and restart jupyterhub . See if the new user can log in. 1. Link domain name to server IP address When we started Jupyter Hub in the previous post, it ran, we could log in, and we could run Python code. What's not to like, right? Well, security is the big problem. In the initial setup, Jupyter Hub was running under regular http, not https. With a web application that has usernames and passwords, like Jupyter Hub, having https and SSL security is best (or maybe manditory). In order to use https, we need to get an SSL certificate. And that SSL certificate should correspond to the domain name linked to our server. So the first step is getting the domain name and pointing it at Digital Ocean. The next step is linking the domain name to our Jupyter Hub server. Google Domains I purchased the domain problemsolvingwithpython.com from google domains . It costs $12/year which seems pretty reasonable and was easy to set up. After purchasing the domain, I added the Digital Ocean DNS servers as a set of custom name servers to my domain options on google domains. To add a set of custom name servers, click the button with the two bars under the DNS header. This will bring up a page where you can enter in the Digital Ocean DNS servers. The name servers to add are: ns1.digitalocean.com ns2.digitalocean.com ns3.digitalocean.com Make sure to click the radio button [Use custom name servers] and click [save]. Digital Ocean DNS Now we are going to set our domain problemsolvingwithpython.com to link to the IP address of our server on Digital Ocean. Log into Digital Ocean and in the upper right select [Create]  [Domains/DNS] In the [Add a domain] field, type in the domain name without http, but including .com (or .edu/.org/.net) then click [Add Domain]. This will bring up a panel where we can add a DNS record. I want the notebook server to have the web address https://notebooks.problemsovlingwithpython.com So I entered notebooks in the text field. Then selected the droplet (server) that the web address will to route to. After completing this step, there will be a number of new DNS records. The ones I set up are below: It takes a couple minutes for the DNS switchover to complete. https://www.whatsmydns.net can be used to check the NS and A records of your domain and see if the domain name is getting through. The first time I set up DNS on Digital Ocean, I added the custom DNS servers to google domains but neglected to select the [use custom name servers] radio button on the google domains dashboard. It looked like the domain was routing to Digital Ocean, but actually the domain was just staying with google. Once I clicked the [use custom name servers] radio button and waited a couple minutes, the change over happened. It did take a bit of time though; not hours, but more than a few minutes. 2. Install nginx and modify ufw Now that the domain name is set up, the next step is to install and configure nginx. Nginx is an open source web server that can handle many concurrent web connections at the same time. For the nginx installation, I followed this tutorial from Digital Ocean. Use PuTTY to connect to the server with the non-root sudo user we set up before. Once logged in, we can update the system and install nginx. $ sudo apt-get update $ sudo apt-get install nginx Digital Ocean installs a firewall application called ufw. Check out which apps the ufw firewall can work with: $ sudo ufw app list We see a list of available ufw configurations to work with nginx. Available applications: Nginx Full Nginx HTTP Nginx HTTPS OpenSSH We want to allow in both http and https requests. Once a http request comes in, we'll use nginx to convert the http connection to a https connection. Select nginx full. Note the Capitalization in the command: $ sudo ufw allow 'Nginx Full' We can check out which ports ufw is allowing through with: $ sudo ufw status Note in the output how ufw is allowing nginx full and requests over port 8000. We opened port 8000 earlier, so we could see how jupyterhub works without a domain name or SSL. Once we get nginx running and hooked up to jupyterhub , we need to remember to close off port 8000 in ufw. Status : active To Action From -- ------ ---- 22 LIMIT Anywhere 2375 / tcp ALLOW Anywhere 2376 / tcp ALLOW Anywhere 8000 ALLOW Anywhere Nginx Full ALLOW Anywhere 22 ( v6 ) LIMIT Anywhere ( v6 ) 2375 / tcp ( v6 ) ALLOW Anywhere ( v6 ) 2376 / tcp ( v6 ) ALLOW Anywhere ( v6 ) 8000 ( v6 ) ALLOW Anywhere ( v6 ) Nginx Full ( v6 ) ALLOW Anywhere ( v6 ) Nginx will start running as soon at it is installed. We can see the status with: $ sudo systemctl status nginx In the output, we should see something like below. This mean nginx is running. Key in [ctrl-c] to exit the status dashboard. Active: active (running) since Thu 2018-05-17 04:51:16 UTC; 15min ago Main PID: 17126 (nginx) CGroup: /system.slice/nginx.service  17126 nginx: master process /usr/sbin/nginx -g daemon on; master_pr  17127 nginx: worker process Now we can browse over to the domain (the domain we set up with Digital Ocean and google domains) and see the nginx start page. 3. Obtain SSL certificates with certbot With a domain name hooked up to our server, we'll now able to obtain an SSL certificate. I followed this presentation to install certbot, a program used to generate SSL certificates. $ cd ~ $ mkdir certbot $ cd certbot $ wget https://dl.eff.org/certbot-auto $ chmod a+x certbot-auto Before we can run certbot, we need to turn off nginx. When I first tried to run certbot, I was thrown an error: Problem binding to port 80: Could not bind to IPv4 or IPv6. Since we installed nginx earlier, and we confirmed that it's running, that means that port 80 is currently in use by nginx. We need to open up port 80 to certbot by temporarily shutting down nginx. Once nginx is stopped, we can run certbot and get our SSL certificate. We'll eventually have to restart nginx, but this can wait until after we change the nginx configuration file. If you are following along, make sure to change notebooks.problemsolvingwithpython.com to your domain. $ sudo systemctl stop nginx $ sudo systemctl status nginx # [Ctrl] + [c] to exit $ ./certbot-auto certonly --standalone -d notebooks.problemsolvingwithpython.com If it worked and we got our SSL certificate, the output will be something like: IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/notebooks.problemsolvingwithpython.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/notebooks.problemsolvingwithpython.com/privkey.pem Your cert will expire on 2018-08-15. Note the location of the fullchain.pem and privkey.pem files. We'll need to put these locations into the nginx configuration. We also need to allow nginx to access these files. I had trouble getting nginx to run and this presentation showed a way to give nginx access to the SSL key files. There is probably a more \"Linuxy\" way of giving nginx access to the cert files, but I messed around with the permission settings for a while, and this way worked. $ cd /etc/letsencrypt $ ls accounts archive csr keys live renewal renewal-hooks $ sudo chmod 777 -R archive/ $ sudo chmod 777 -R live/ 4. Create a cookie secret and a proxy auth token In addition to the SSL certificate, the Jupyter Hub docs on security basics specify that a cookie secret and poxy auth token should be created. To create the cookie secret: $ cd /srv $ mkdir jupyterhub $ cd jupyterhub $ sudo touch jupyterhub_cookie_secret $ sudo chown :sudo jupyterhub_cookie_secret $ sudo chmod g+rw jupyterhub_cookie_secret $ sudo openssl rand -hex 32 > jupyterhub_cookie_secret $ ls jupyterhub_cookie_secret $ sudo chmod 600 jupyterhub_cookie_secret $ ls -l -rw------- 1 root sudo 65 Sep 14 17 :41 jupyterhub_cookie_secret I had trouble with the cookie secret file because I missed where the jupyterhub docs show: The file must not be readable by group or other or the server won't start. The recommended permissions for the cookie secret file are 600 (owner-only rw). Now we have a cookie secret file. We need to make note of the location because we'll add this location to the jupyterhub_config.py file later. To generate the proxy auth token, we can use the same set of commands, but point to a different file. $ pwd # should be in /srv/jupyterhub $ sudo touch proxy_auth_token $ sudo chown :sudo proxy_auth_token $ sudo chmod g+rw proxy_auth_token $ sudo openssl rand -hex 32 > proxy_auth_token $ ls jupyterhub_cookie_secret proxy_auth_token $ sudo chmod 600 proxy_auth_token $ ls -l -rw------- 1 root sudo 65 Sep 14 17 :41 jupyterhub_cookie_secret -rw------- 1 root sudo 65 Sep 14 17 :47 proxy_auth_token Now if we list the contents of ~/srv/jupyterhub we should see: /srv/jupyterhub/  jupyterhub_cookie_secret  proxy_auth_token Also we can generate a dhparam.pem file. I'm not exactly sure what the dhparam.pem file is, but I think it's good for security. First we need to cd into the /srv/jupyterhub directory. Next touch a new file called dhparam.pem. After that we can use chown and chmod to be able to write to the dhparams.pem file. The openssl dhparam command is used to generate the .pem file. Finally we modify the permissions again to 600 (owner-only rw). Note the location of this file as we will add it to the nginx config file. $ cd /srv/jupyterhub $ sudo touch dhparam.pem $ sudo chown :sudo dhparam.pem $ sudo chmod g+rw dhparam.pem $ sudo openssl dhparam -out /srv/jupyterhub/dhparam.pem 2048 $ sudo chmod 600 dhparam.pem $ ls -l -rw------- 1 root sudo 424 Sep 14 17 :59 dhparam.pem -rw------- 1 root sudo 65 Sep 14 17 :41 jupyterhub_cookie_secret -rw------- 1 root sudo 65 Sep 14 17 :47 proxy_auth_token 5. Modify nginx config The next step is to modify the nginx config file so that nginx uses our SSL certificates and routes requests on to jupyterhub . This was the hardest part for me when I set up the first server. The nginx config file isn't Python code or a bash script. I went through many different configurations until I got one that worked. The big initial problem was that I copied the sample nginx config that's up on the Jupyter Hub docs. But the nginx config posted on the jupyterhub docs is not a complete nginx config, it contains just the server portion. I didn't know that the whole server portion needed to be enclosed in another frame. To modify nginx.conf , cd into the /etc/nginx directory. The nginx.conf file should be there along with a couple other files and directories. $ cd /etc/nginx $ ls conf.d koi-utf nginx.conf sites-available ssl fastcgi.conf koi-win proxy_params sites-enabled uwsgi_params fastcgi_params mime.types scgi_params snippets win-utf $ sudo nano nginx.conf The nginx config that eventually worked for me is below. It can also be found here : Note the line which shows the path to the SSL certificates. This will change based on your domain and where certbot saved the .pem files to. ## Based on: https://github.com/calpolydatascience/jupyterhub-deploy-data301/blob/master/roles/nginx/templates/nginx.conf.j2 user www-data; worker_processes 4; pid /run/nginx.pid; events { worker_connections 1024; # multi_accept on; } http { include /etc/nginx/mime.types; default_type application/octet-stream; #top-level http config for websocket headers # from https://github.com/jupyterhub/jupyterhub/blob/master/docs/source/referen$ map $http_upgrade $connection_upgrade { default upgrade; '' close; } # All regular http requests on port 80 become SSL/HTTPS requests on port 32 server { listen 80; server_name notebooks.problemsolving101withpython.com; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } server { #listen 443 ssl default_server; listen 443; ssl on; server_name notebooks.problemsovling101withpython.com; ## SSL Protocals ssl_certificate /etc/letsencrypt/live/notebooks.problemsolving101withpython.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/notebooks.problemsolving101withpython.com/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_dhparam /srv/jupyterhub/dhparam.pem; # Make site accessible from http://localhost/ #server_name localhost; # certs sent to the client in SERVER HELLO are concatenated in ssl_session_timeout 1d; ssl_session_cache shared:SSL:50m; ssl_stapling on; ssl_stapling_verify on; # modern configuration. tweak to your needs. ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256'; # HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months) add_header Strict-Transport-Security max-age=15768000; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; #proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } location ~ /.well-known { allow all; } } } Save and exit with [Ctrl] + [c] and [y] 6. Generate jupyterhub_config.py and modify Next, we'll generate a jupyterhub_config.py file and modify it a little bit. $ cd ~ $ jupyterhub --generate-config Now we'll modify the jupyterhub_config.py file to allow local spawners and include our user peter as an admin user: $ nano jupyterhub_config.py There will be a lot of commented out text in the file. At the top of the file, add the following: #jupyterhub_config.py import os c = get_config () c . JupyterHub . log_level = 10 c . Spawner . cmd = '/home/peter/anaconda3/bin/jupyterhub-singleuser' # Cookie Secret Files c . JupyterHub . cookie_secret_file = '/srv/jupyterhub/jupyterhub_cookie_secret' c . ConfigurableHTTPProxy . auth_token = '/srv/jupyterhub/proxy_auth_token' c . Authenticator . whitelist = { 'peter' } c . Authenticator . admin_users = { 'peter' } 7. Restart nginx and start jupyterhub, see if we can login Now we'll restart nginx and start jupyterhub. Not that this time when we start jupyter hub we don't need to use the --no-ssl flag because we have SSL running on nginx. If it seems like nginx isn't working, try $ sudo systemctl status nginx and see if nginx really started. If it didn't, try the command nginx -t . This will print out any error messages if nginx failed to start. I had to do this many different times before I got nginx to work. $ sudo systemctl stop nginx $ sudo systemctl start nginx $ sudo systemctl status nginx # [ctrl-c] to exit Restart the jupyterhub without the --no-ssl flag. $ cd ~ $ jupyterhub Log in with the non-root sudo username and password (same user that's running the PuTTY session). Now we can browse to our domain and see Jupyter Hub running in its full SSL glory. 8. Create an new user and restart jupyterhub . See if the new user can log in. OK, it's all well and good that we can log into jupyterhub . But the purpose of setting of this up is for multiple students to be able to log into jupyterhub . If jupyterhub is still running, it can be stopped with [Ctrl] + [c]. Let's create a new user and see if we can log in as her. $ sudo adduser kendra Go through the prompts and remember the UNIX password. Now we'll modify jupyterhub_conf.py to include our new user kendra and add peter (our non-root sudo user) as an administrator: c.Authenticator.whitelist = {'peter','kendra'} c.Authenticator.admin_users = {'peter'} Restart jupyterhub and try and login as kendra $ jupyterhub Amazing! right? Jupyter Hub running on our own domain using SSL security and https. Pretty cool. Summary In this post we created an SSL certificate with certbot. We modified the nginx config to use our SSL certificate and modified jupyterhub_config.py. At the end of all of that we were able to get a working version of jupyterhub running on https using SSL security and can log into jupyterhub as two different users. Next Steps Up next will add an authentication system so that users can log into our Jupyter Hub server using their college usernames and passwords. We will also set jupyterhub to run as a system service in the background which will allow us to work on the server and run jupyterhub at the same time.","tags":"jupyter","url":"add-ssl-and-domain-name-to-jupyterhub.html"},{"title":"Installing Jupyterhub","text":"This is the fourth part of a multi-part series that shows how to set up Jupyter Hub for a college class. The goal is to have a running version of Jupyter Hub that students in the class can log into when given a simple web link. In this post, we'll get to the fun stuff: installing jupyterhub on the server, installing Python packages and spinning up Jupyter Hub for the first time. Posts in this series Why Jupyter Hub? Create SSH keys, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server (this post) Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub Connect to Jupyter Hub as student The last post In the last post, we created a new server on Digital Ocean (called a droplet ) and made sure to add our public SSH key to the setup options. Then we logged into the server as root with our private SSH key. As root , we set up a new user with sudo privileges and added our public SSH key to the new users profile. Then we logged into the server as the new user and checked the new user's home directory. This post Update packages on the server Install Anaconda Install Python packages and jupyterhub Run a very unsecured instance of Jupyter Hub just to see if it works Shut down Jupyter Hub very quickly because of no SSL security 1. Update Packages on the Server It is probably best to update the packages installed on the server in case there are changes and updates to the operating system since the server was created. This is probably a reflex for those that use Linux a lot. Open PuTTY and log into the server as the non-root sudo user we created in the last post. Then update the system: $ sudo apt-get update 2. Install Anaconda Next we'll install Anaconda . When I installed Anaconda on Windows10, I used an msi installer. But for installation of Anaconda on the cloud server we'll use a shell script. The first time I set up Jupyter Hub, I installed Anaconda in the /opt directory. I don't think this is the best setup. When I installed Anaconda in /opt , I ended up with all sorts of permission problems when I tried to run conda and jupyterhub . For me it caused less problems to install Anaconda in the non-root user's home directory( /home/peter/ ). The user's home directory is also the default Anaconda3 installation location. I followed this tutorial from Digital Ocean. Go to https://repo.continuum.io/archive/ and look down the list of installs for the newest installer that corresponds to: Anaconda 3 (not Anaconda 2, we don't want legacy Python version 2.7) Linux x86 64 (bit) .sh (linux shell script) At the time of writing, the shell script was: Anaconda3-5.1.0-Linux-x86_64.sh cd into the /tmp directory and download the script. Then run the script and follow the prompts. $ cd /tmp $ curl -O https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh $ bash Anaconda3-5.1.0-Linux-x86_64.sh We want to be able to run conda from the command line. So make sure to allow Anaconda to append your PATH during the installation. After installation, we need to reload the .bashrc file because Anaconda made changes to .bashrc during the install (when it added conda to our PATH). $ source ~/.bashrc Now we should be able to run conda . Try: $ conda --version If you see output, that means conda was installed and can be run by the non-root user. 3. Install Python packages and jupyterhub We have a full installation of Anaconda which includes a lot of useful packages for engineers including: numpy , pandas , matplotlib , scipy , sympy , bokeh and holoviews . Anaconda also includes other useful packages such as requests , beautiful soup and attrs . And for student who want to go further, the Anaconda distribution comes with django , flask , tk , tornado and scikit-learn . There are a couple other packages I want to install in addition to the standard Anaconda packages. First are some extra packages useful for engineers: pint , pyserial and schemdraw . $ conda install -c conda-forge pint $ conda install -c anaconda pyserial $ pip install SchemDraw Now we'll install Jupyter Hub! Since we are using conda to install jupyterhub (rather than pip), we don't need to install node and configurable-http-proxy separately. Conda installs the non-python dependencies (like node and configurable-http-proxy) that jupyter hub relies upon. $ conda install -c conda-forge jupyterhub $ conda install notebook 4. Run a very unsecured instance of Jupyter Hub just to see if it works OK let's give it a whirl. Start jupyterhub for the first time. Note the --no-ssl flag at the end. This needs to be included or you won't be able to browse to the server. $ jupyterhub --no-ssl You should see some output in the PuTTY window. The last line should be something like JupyterHub is now running at http://:8000/ . The first time I set up Jupyter Hub, I wasn't able to see the site using a web browser. Nothing would come up and the connection would time out. Why? It turns out Digital Ocean installs a firewall called ufw by default and turns the ufw firewall on. When the server was created, ufw was configured to only allow incoming connections on ports 22, 80 and 433. This output is shown when we first log into the server: \"ufw\" has been enabled. All ports except 22 (SSH), 80 (http) and 443 (https) have been blocked by default. But jupyterhub runs on port 8000 - it tells us so when jupyterhub starts. So we need to configure ufw to allow connections on port 8000 (at least for now, just to see if jupyterhub works). To allow communication on port 8000 and restart jupyterhub , type: $ sudo ufw allow 8000 $ jupyterhub --no-ssl Now we can browse to the server IP address appended with :8000 . The web address should look something like: http://165.228.68.178:8000. You can find the IP address of the server by going into the Digital Ocean dashboard. We should see: Awesome! Quick log into jupyterhub using the username and password for the non-root sudo user (in my case peter ) that we set up and are using in our current PuTTY session. You should see the typical notebook file browser with all the files you can see when you run ls ~/ . Try creating and running a new notebook. The notebook work just like a jupyter notebook running locally . 5. Quick! Log out and shut down jupyterhub Quick! Log out and shut down jupyterhub . (does quick really matter in internet security?) The site is running without any ssl security over regular HTTP not HTTPS. Key in [Ctrl] + [c] to stop jupyterhub. Warning! You should not run JupyterHub without SSL encryption on a public network. Summary In this post we installed Anaconda on the server using a shell script. We added conda to our path and reloaded our .bashrc file. Then we installed some extra Python packages such as pint and pyserial . Finally we installed jupyterhub , opened up port 8000 and ran jupyterhub for the first time! Remember we shut down jupyter hub very quickly because we ran it without any SSL security. Next Steps In the next post we will build SSL security into our Jupyter Hub deployment and connect the server to domain name. Plus we'll customize the jupyterhub config file and install and use nginx as a proxy server.","tags":"jupyter","url":"installing-jupyter-hub.html"},{"title":"Creating a new Digital Ocean Droplet","text":"This is the third part of a multi-part series on how to set up Jupyter Hub for a class. My goal is to have a running version of Jupyter Hub that students can access using a simple web link. I am primarily writing to my future self as I may need to set up Jupyter Hub again for a future class. In this post, we are going to create a new Digital Ocean server (called a droplet ) and create a non-root user with sudo privileges. Then we'll SSH into the droplet with PuTTY as the non-root user. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys Create a new Digital Ocean Droplet with a non-root sudo user (this post) Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub Connect to Jupyter Hub as student The last post In the previous post , we created a public/private SSH key pair using PuTTYgen. We saved the SSH keys in the Documents/ssh-keys directory. We also copied contents of the public SSH key to the clipboard. Steps in this post Sign up for a Digital Ocean Account Create a new Digital Ocean Droplet (will be called the server from here on out) Log into the server as root over SSH using PuTTY. Create a non-root sudo user Log into to the server as the non-root sudo user using PuTTY 1. Sign up for a Digital Ocean Account Digital Ocean is a cloud service provider like Amazon Web Services (AWS), Google Cloud, Microsoft Azure and Linode. Digital Ocean provides virtual private servers (called droplets in Digital Ocean-speak) and online storage of static files (called spaces in Digital Ocean-speak). We are going to run the Jupyter Hub server on a Digital Ocean droplet . I like Digital Ocean's prices and web interface. The documentation on Digital Ocean is pretty good too. I already have a Digital Ocean account. I don't remember exactly how I did it, but going to this link: https://www.digitalocean.com/ and selecting [Create Account ] should work. 2. Create a new Digital Ocean Droplet To create a new Digtial Ocean Droplet (a new server), log in here: https://cloud.digitalocean.com/login After logging in, I got a verify screen and had to go to my email and retrive a six digit code. Ah... the joys of two-factor authentication. The welcome screen looks like this. To create a new server select [Create Droplet] There are a number of choices to make. These are the ones I selected: Image: Ubuntu 16.04.4 x64 Size: 1 GB Memory 25GB SSD $5/month Datacenter: San Fransisco 2 Add your SSH keys: New SSH Key Finalize: 1 Droplet, Hostname: jupyter-hub Add an SSH Key Important! You need to add the public SSH key BEFORE creating the droplet The public SSH key we created needs to be shown on the list of keys and the radio box beside it needs to be checked. If the SSH key isn't listed or the SSH key box left unchecked, the SSH key will not be added to the server when the server is first created (and then we won't be able to log in with PuTTY). We need to add our public SSH key and check the key box so we can log onto the server with PuTTY. Under [Add your SSH keys], click [New SSH Key]. A dialog window pops up: Paste the contents of the public SSH key into the [New SSH Key] dialog box. Enter a name for the SSH key that will be saved on Digital Ocean. I choose the name jupyter-hub-ssh-key . Then click [Add SSH Key] Then you should see the new SSH Key in the [Add your SSH Keys?] region of the new droplets page. Make sure that the radio box for the SSH key we just added is checked. A problem I had when I set up my first droplet was that I did not have the SSH Key was radio button selected. Therefore, when the server was created, no SSH keys were installed. I ended up going through this long process of copying the public SSH key into pastbin.com (which is definitely not a safe thing to do ), and using wget to past the raw contents from the pastebin into the server file system, then using cp to copy the publish SSH key into the correct file name. This required using the Digital Ocean console, which is sort of like a bash terminal that pops up in a web browser. I couldn't figure out a way to copy and paste into the Digital Ocean console and the console is slow and laggy. It is way easier to insert SSH keys into the server when the server is created. It is way harder to add an SSH key after the server is created. OK, I think it's time to actually create the droplet . Click the long green [Create] button at the bottom of the page. This will bring you back the the Digital Ocean main dashboard and you should see your new droplet under [Resources]  [Droplets] Note the IP address of the new droplet. We need to IP address to log into our server with PuTTY. 3. Log into the server as root over SSH using PuTTY. Open PuTTY from the Windows start menu. A couple other parameters need to be set before we log onto the server. parameter value IP Address IP of droplet ex: 168.97.14.19 Port 22 Connection  SSH  Auth  Private key file private SSH key Connection  Data  Auto-login username root Under Connect  SSH  Auth  Private key file for authentication:, click [Browse]. Navigate to the SSH private key in Documents/ssh-keys. The private key ends in a .ppk extension. I had trouble finding the key when I first set up PuTTY. It turned out that when the key was saved in Programfiles/PuTTY. The key was not visible in the Windows file browser because I don't have administrator permissions on my machine at work. I ended up having to create a new SSH key and save the new key in Documents/ssh-key (I can access Documents/ssh-key without administrator privaleges). Under Connection  Data  Auto-login username: root Back in [Sessions] (the top-most menu item or main page), add the IP address and Port = 22, click [Open] This should bring up a new window that is a terminal for our server: 4. Create a non-root sudo user Digital Ocean recommends that the servers are run by non-root user that have sudo access. So one of the first things we'll do on our server is create a non-root sudo user. First though, let's make sure everything is up to date: $ sudo apt-get update $ sudo apt-get upgrade I followed this tutorial at Digital Ocean to create a non-root sudo user. Create the new user with the adduser command. I called my new user peter . $ adduser <username> Set a new password and confirm: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully The user details can be skipped by pressing [Enter]. Then [Y] to complete the new user setup. Changing the user information for username Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] Now let's give our new user sudo privaleges: $ usermod -aG sudo <username> The new user account is created and the new user has sudo privalges. We can switch accounts and become the new user with: $ sudo su - <username> The new user should have sudo privileges. That means when acting as <username> we should be able to look in the /root directory. $ sudo ls -la /root If you can see the contents of /root then the new user is set up with sudo access. To exit out of the the new sudo user, and get back to using the root profile, type exit at the prompt $ exit Add SSH keys to new user's profile Before we log off, we need to add our SSH keys to our new user's profile on the server. The second time I set up JupyterHub, I had trouble logging in as the non-root user using PuTTY. I could log in as root just fine, but I couldn't log in as the newly created user peter . When Digital Ocean created the server, the SSH keys (specified on the creation page) were added to the root profile. The new user peter didn't exist when the server was created. The only user on the server at creation time was root . Therefore, no SSH keys were added to the peter profile at server creation time- because the user peter didn't exist yet. Since we want to log into our server as the new non-root user peter , we need to add the same SSH keys saved in the root profile to the peter profile. The SSH keys belong in a file located at /home/peter/.ssh/authorized_keys . This little line will copy the ssh keys from the root profile to the new user's profile. The line comes from this tutorial by Digital Ocean. $ rsync --archive --chown = peter:peter ~/.ssh /home/peter Next, we need to open the ufw firewall to OpenSSH trafic. We will communicate with the server over SSH and need the firewall to allow this type of communication through. $ ufw allow OpenSSH $ ufw enable $ ufw status Now we can exit out of the root profile. This will terminate the PuTTY session. $ exit 4. Connect to the server as the non-root sudo user using PuTTY Now that the non-root sudo user is set up and our ssh keys are in /home/ /.ssh/authorized_keys, let's start a new PuTTY session and log into the server as the new user. Like before, open PuTTY from the Windows Start menu and add the following settings, but this time the Auto-login user name is the name of our new non-root sudo user: parameter value IP Address IP of droplet ex: 168.97.14.19 Port 22 Connection  SSH  Auth  Private key file private SSH key Connection  Data  Auto-login username peter I also saved the PuTTY session details at this point so that I wouldn't have to re-enter all of the parameters each time I want to log into the server. Enter a name into [Saved Sessions] and click [Save]. Once the parameters are saved in PuTTY, you can simply double-click the profile name and you will log into the server. Log into the server with Sessions  [Open] You should see the Digital Ocean login screen again. Note the command prompt will have the new user's name before the @ symbol. Check to see which directory you land in. It should be /home/<username> $ pwd /home/<username> We can see the non-root user's home directory. Let's make sure we can also see into the root user's home directory to ensure we have sudo privileges as the non-root user: $ sudo ls -la /root To log out of the server simply type exit . This should close the PuTTY session. $ exit Summary In this post we created a new Digital Ocean server (called a droplet ) and made sure to add our public SSH key to the setup options before we hit [Create]. Then we logged into the server as root with our private SSH key. As root , we set up a new user with sudo privileges and added our public SSH key to the new user's profile in ~/.ssh/authorized_keys . Then we logged into the server as the new user and checked the new user's home directory and ensured the new user has sudo privileges. Next Steps In the next post, we will get to the fun stuff: installing Anaconda and jupyterhub on our new server. Plus we'll start Jupyter Hub for the first time! (but only keep it open for a couple seconds because we don't have SSL set up yet).","tags":"jupyter","url":"new-digital-ocean-droplet.html"},{"title":"Create an SSH Key with PuTTYgen","text":"This is the second part of a multi-part series on how to set up Jupyter Hub for a class. This is my first time setting up a Jupyter Hub server. I am primarily writing to my future self as I may need to set up Jupyter Hub again for another class. In this post, we are going to create an SSH key on a Windows 10 machine using PuTTYgen. Posts in this series Why Jupyter Hub? Create ssh key, save to documents/ssh-keys (this post) Create a new Digital Ocean Droplet with a non-root sudo user Install Jupyter Hub on the server Apply SSL, link a domain name to the server and configure nginx Connect OAuth to Jupyter Hub Connect to Jupyter Hub as student Steps in this post Download PuTTY for Windows Start PuTTYgen and create SSH key Save SSH public and private keys to Documents folder Copy the public key to clipboard Why SSH keys, PuTTYgen and why do this first? When I set up the server on Digital Ocean the first time, one of the initial server setup steps was to add SSH keys so the server has them when it initialized. I tried to create and save the keys to the Digital Ocean dashboard so the SSH keys would be on the server when it first started. But I goofed up somehow and the server started without any SSH keys. It was a BIG PAIN adding SSH keys after the server started for the first time. I ended up copying the public key into pastebin.com, logging onto the server with the Digital Ocean console and using wget to bring a textfile of the SSH key from pastebin.com onto the server and then mv to copy the key name into the right location. I'm pretty sure that pasting a public SSH key into pastebin.com is not the best way to initially set up a server. So to make sure that doesn't happen again, I am going to generate the SSH keys first and set up the server second. SSH keys are needed to use PuTTY (regular PuTTY not PuTTYgen) to log into the server. Since I'm working on Windows, using PuTTYgen (a program that comes with PuTTY that generates SSH keys) seems like the easiest solution. 1. Download PuTTY I already have PuTTY installed on my Windows 10 machines at home and at work. The download link is below: Download PuTTY PuTTY seems to want you to install lots of extra stuff when you run the installer. I didn't install any of the \"offers\" that popped up during installation. 2. Start PuTTYgen and create SSH key I went through this tutorial to about setting SSH key on Windows for Digital Ocean when I created the first SSH key: How To Use SSH Keys with PuTTY on DigitalOcean Droplets (Windows users) Using the Windows start menu, open PuTTYgen (not regular PuTTY): Use the following parameters Type of key to generate: RSA Number of bits in generated key: 2048 Then click [generate] This will bring up a dialog to move the mouse around the empty area to generate some randomness. This is my favorite part. Just move the mouse around the dialog box until the progress bar ends. Fun. When the next screen pops up, right-click and copy the contents of the Public Key. We'll need the public key contents available to paste into the server's SSH authorized_keys file. Include the rsa line in the text copied to the clipboard. 3. Save SSH public and private keys to Documents folder In the [Actions] section click [Save public key] and click [Save private key] Make sure to save both the public and the private keys. Save these keys to an accessible folder. The first time I generated SSH keys, I saved the keys in the default location and couldn't access them later. The second time I created SSH keys, I created a folder in the Documents folder called ssh-keys and saved the public and private keys in Documents/ssh-keys. I saved the public key with the name: public_key_jupyter_hub.txt . The Digital Ocean documentation recommends a .txt file extension for the public key (so you can open it and copy the contents). The private key should have a .ppk file extension. 4. Copy the public key to clipboard Before closing PuTTYgen, make sure to copy the contents of the Public Key to the clipboard. We'll need this when we create the server. Copy all of the contents including the rsa line. Summary After completing these steps, we have a public and private SSH key pair saved in Documents/ssh-keys. We also have the contents of the public SSH key saved to the clipboard. Next Steps Next, we'll create a new server on Digital Ocean (called a droplet ). Then we'll use the SSH keys we just created to log into the server and create a non-root sudo user.","tags":"jupyter","url":"ssh-keys-with-putty.html"},{"title":"Why Jupyter Hub?","text":"This is the first part of a multi-part series that shows how to set up Jupyter Hub for an engineering programming class. This is my first time setting up a Jupyter Hub server. I am primarily writing to my future self as I may need to set up a Jupyter Hub server again for a different class in another quarter. I hope the lessons learned will also help other instructors tackle the same problem if they want to set up Jupyter Hub for their own class or team. Why Jupyter Hub? Why Jupyter Hub ? I am teaching an intro engineering course this summer. The course has a lab component and three of the labs are devoted to computer programming. In previous quarters, I've taught MATLAB for the three computer programming labs. But this summer, I want to try teaching Python and cover the same concepts and learning outcomes. If we use Python in the three programming labs this summer, I would like to spend the lab time coding and solving problems. I don't want to spend time during the class downloading Python, creating virtual environments, troubleshooting installs, dealing with system vs. non-system versions of Python, installing packages, dealing with folder structure, explaining the difference between conda and pip, teaching command-line commands, going over Python on Windows compared to Python on MacOSX... I imagine the first programming lab of the quarter runs like: There is a .pdf or google doc posted on a shared google drive folder with a link to Jupyter Hub Students click the link and bring up the login page Students log-in with their college usernames and passwords Students type import this press [Shift+Enter] and their first code cell just runs. Students can use Jupyter Hub from any computer with a web browser and an internet connection. Jupyter Hub looks and runs the same on all student computers This is theoretically possible with Jupyter Hub Jupyter Hub can be installed on a Digital Ocean droplet (a cloud server, like AWS or Google Cloud). The version of Python running the notebooks created by Jupyter Hub can contain the full Anaconda distribution of packages plus some extra packages like pint and pyserial . All of the notebooks will use the same environment. The installed packages will be the same for each student. One Digital Ocean droplet will be able to run all of the notebooks at the same time (hopefully, there will be about 24 students). Student's work will be saved on the server under their user account. Students can download the .ipynb files and upload them to google drive or save the .ipynb files on a thumb drive. After the students login, folders and notebooks can be in place and used as starting points in lab and as lab exercises. After the course ends, students will still be able to log in and use Jupyter Hub and practice writing and running Python code to solve engineering problems. What will it take to make Jupyter Hub a reality? This list will surely change as I go through the process of setting up the Jupyter Hub server. Below are the steps I expect to take and software/hardware needed at each step. Sign up for a Digital Ocean Account (already done) Create a new Digital Ocean Droplet (will be called the server from here on out) Connect to the server over SSH and set up SSH keys Create a non-root sudo user on the server Get a public url, hook up the server DNS record to the public URL Install Anaconda on the server Install the other packages on the server like jupyter hub, node, pyserial and pint Edit permissions of files and directories on the server Create and implement SSL certificates on the server Run Jupyter Hub as a non-root sudo user Connect google OAuth to Jupyter Hub Connect to the server as student and celebrate teaching Python without worrying about installation and virtual environments. Will this work? How much time will it take? Will this work? I hope so. Other people have done it. There was a JupyterCon talk about it, there are example implementations up on github. A large data science class at UC Berkeley ran Jupyter Hub . I don't really know how long it will take. The only real step that takes time is the DNS connection. The rest of the steps are in the minute time frame of computing time and probably days if not weeks of troubleshooting time. I'm just going to try and complete a step per day or a step per week and see if I can get the server going by the end of the spring quarter. Next Steps: The next step is really the first step: Create SSH keys. We'll need a public/private SSH key pair to be able to log into the Jupyter Hub server over SSH.","tags":"jupyter","url":"why-jupyter-hub.html"},{"title":"Diffusion Calculation with Python and Pint","text":"I was working through a diffusion problem and thought that Python and a package for dealing with units and unit conversions called pint would be usefull. I'm using the Anaconda distribution of Python , which comes with the Anaconda Prompt already installed. For help installing Anaconda, see a previous blog post: Installing Anaconda on Windows 10 . To use the pint package, I needed to install pint using the Anaconda Prompt : > pip install pint The problem I'm working on involes the diffusion of nitrogen gas (N 2 ) into a thin plate of iron. Given: When -iron is put in a nitrogen atmosphere, the concentration of nitrogen in the -iron, $C_{N}$ (in units of wt%) is a function of the nitrogen pressure $P_{N_2}$ and temperature $T$ accoding to the relationship: $$C_{N} = 4.9 \\times 10&#94;{-6} \\sqrt{P_{N_2}} exp{\\frac{Q_n}{RT}} $$ Where: $Q_n = 37,600 \\frac{J}{mol}$ $R=8.31 \\frac{J}{mol-K}$ $T$ is the temperature in Kelvin. At 300 C the nitrogen gas pressure on one side of an iron plate is 0.10 MPa. On the other side of the iron plate, the nitrogen gas pressure is 5.0 MPa. The iron plate is a 1.5 mm thick. Assume the pre-exponential term $D_0$ and the activation energy of diffusion of nitrogen in carbon, $Q_d$ are equal to the values below: $D_0 = 5 \\times 10&#94;{-7} \\frac{m&#94;2}{s}$ $Q_d = 77,000 \\frac{J}{mol} $ Find: Calculate the diffusion flux, J through the plate using Fick's First Law of Diffusion: $$ J = -D \\frac{dC}{dx} $$ Solution: We have a couple different quantities and a couple of different units to handle to solve this problem. We'll start out importing pint and creating a UnitRegistry object. We'll also need the exp (e raised to a power) and sqrt (square root) functions from the math module , part of the Python standard library. In [1]: import pint from math import exp , sqrt u = pint . UnitRegistry () Let's start with the temperature, T = 300 C. Temperature units in F and C are relative units with an off-set scale . C and F are not multiplicative units . Non-multiplicatve units are handled by pint a little differently compared to regular multiplicative units. To create a variable including a unit of degrees C, we instantiate a Quantity object and pass in the temperature in C along with the unit ( u.degC ). We can convert the temperature to Kelvin (K) using the .ito method. Since we want to do some mulipication, division and exponentiation with our temperature, we need to convert the temperature to a multiplicative unit. Pint has two versions of the temperature unit in Kelvin (K). There is the non-multiplicative type degK and the multiplicative type kelvin . We convert the temperature variable T to the multiplicative type kelvin by pulling out the magnitude (the number part without the degK unit) from the T variable and multiplying it by the kevlin unit from pint . In [2]: Q_ = u . Quantity T = Q_ ( 300 , u . degC ) print ( 'T = {} ' . format ( T )) T . ito ( 'degK' ) print ( 'T = {} ' . format ( T )) T = T . magnitude * u . kelvin print ( T ) T = 300 degC T = 573.15 kelvin 573.15 kelvin Next we'll create variables for $Q_n = 37,600 \\frac{J}{mol}$ and the universal gas contant $R=8.31 \\frac{J}{mol-K}$ In [3]: Qn = 37600 * u . J / u . mol R = 8.31 * u . J / ( u . mol * u . kelvin ) Our first nitrogen pressure is 0.10 MPa and our second nitrogen pressure is 5.0 MPa, we'll make variables for both: In [4]: PN1 = 0.10 PN2 = 5.0 Now we can calculate the two nitrogren concentrations in wt% using the equation: $$C_{N} = 4.9 \\times 10&#94;{-6} \\sqrt{P_{N_2}} exp{\\frac{Q_n}{RT}}$$ where $P_{N_2}$ = 0.10 for one side of the iron plate and $P_{N_2}$ = 5.0 for the other side of the iron plate In [5]: CN1 = ( 4.9e-3 ) * sqrt ( PN1 ) * exp ( - Qn / ( R * T )) print ( CN1 ) 5.777054779474043e-07 In [6]: CN2 = ( 4.9e-3 ) * sqrt ( PN2 ) * exp ( - Qn / ( R * T )) print ( CN2 ) 4.084994609852251e-06 These values CN1 and CN2 are in units of wt% N in an iron-nitrogen \"alloy\" where almost all of the alloy is iron with only a small amount of nitrogen. To use Fick's First Law of Diffusion: $$ J = -D \\frac{dC}{dx} $$ We need a concentration gradient $dC$ in units of mass per unit volume like kg/m 3 or g/cm 3 not in units of wt %. Therefore we need to convert the two concentrations of nitrogen in iron, CN1 and CN2 from units of wt% to units of kg/m 3 . To make the conversion between wt% and mass per unit volume we have to pick a sample mass of iron. This mass of iron will contain a mass of nitrogen (based on wt%). We can divide this mass of nitrogen by the volume of iron that corresponds to the mass of iron we picked. As long as we divide the mass of nitrogen by the volume of iron that contains that mass of nitrogen, we will end up with a unit conversion from wt% to kg/m 3 that works. So let's pick 1 kilogram of iron, and use the density of iron as 7.874 g/cm 3 . We set a variable p to equal the density of iron in g/cm 3 and use the .ito() method to convert the density to units of kg/m 3 . Then we divide the mass of iron that we picked (1 kg) and convert it to volume of iron using the density p . This will give use the volume of 1kg of iron in units of m 3 . In [7]: p = 7.874 * u . g / u . cm ** 3 p . ito ( u . kg / u . m ** 3 ) mFe = 1 * u . kg vFe = mFe / p Now we'll determine how many kg of nitrogen there are in 1 kg of iron given our concentrations CN1 and CN2 in wt%. Note that we have to multiply CN1 and CN2 by 0.01 because CN1 and CN2 are in units of %. When we divide the mass of nitrogen by the volume of iron, we get a concentration of nitrogen in iron in units of kg/m 3 , which is the concentration units we need to use the Fick's First Law of Diffusion. In [8]: mN1 = mFe * CN1 * 0.01 CN1 = mN1 / vFe print ( CN1 ) mN2 = mFe * CN2 * 0.01 CN2 = mN2 / vFe print ( CN2 ) 4.5488529333578606e-05 kilogram / meter ** 3 0.0003216524755797662 kilogram / meter ** 3 Back to Fick's Fist Law of Diffusion: $$ J = -D \\frac{dC}{dx} $$ The difference in concentration $dC$, is just the difference between the two concentrations CN1 and CN2 now that they are both in units of kg/m 3 . $dx$, the change in distance is the thickness of the plate, 1.5 mm. We'll convert the change in distance, $dx$ to units of meters using the ito() method. In [9]: dC = CN2 - CN1 dx = 1.5 * u . mm dx . ito ( u . m ) Next we need to find the diffusion coefficient $D$. To do this, we need the pre-exponential term $D_0$ and the activating envery of diffusion $Q_d$. From the beginning of the problem: $D_0 = 5 \\times 10&#94;{-7} \\frac{m&#94;2}{s}$ $Q_d = 77,000 \\frac{J}{mol} $ Let's assign these to variables with the applicable units. In [10]: D0 = 5e-7 * u . m ** 2 / u . s Qd = 77000 * u . J / u . mol To calculate diffusion constant $D$, we use the equation which relates diffusion coefficient, $D$ to temperature, $T$ according to: $$ D = D_0e&#94;{\\frac{-Q_d}{RT}} $$ In [11]: D = D0 * exp ( - Qd / ( R * T )) print ( D ) 4.7627851906932175e-14 meter ** 2 / second Now that we have $D$, $dC$ and $dx$, we can finally calculate diffusion flux, $J$ through the plate using Fick's First Law of Diffusion: $$ J = -D \\frac{dC}{dx} $$ In [12]: J = - D * ( dC / dx ) J Out[12]: -8.768730355898267e-15 kilogram/(meter 2 second) Final Answer: So the final answer rounded to 3 sig figs is: $J = -8.77 \\times 10&#94;{-15} \\frac{kg}{m&#94;{2}s}$","tags":"engineering","url":"diffusion-problem-python-pint.html"},{"title":"Opening a Jupyter Notebook on Windows","text":"In this post, we will run through how to open a jupyter notebook on Windows 10. Jupyter notebooks are one way engineers can write and execute Python code. Jupyter notebooks contain Python code, the output of that code produces when it is run and markdown cells to explain what the code means. A jupyter notebook can be started from the Anaconda Prompt , the Windows start menu and the Anaconda Navigator . 3 ways to open a jupyter notebook : Anaconda Prompt Windows Start Menu Anaconda Navigator 1. Anaconda Prompt The first way to start a new jupyter notebook is to use the Anaconda Prompt . Go to the Windows start menu and select [Anaconda Prompt] under [Anaconda3] . If you don't see the Anaconda Prompt in the Windows Start Menu, then you need to install Anaconda . Download Anaconda at the following link: Anaconda.com/downloads The Anaconda Prompt window should look something like: At the Anaconda Prompt type: > jupyter notebook This will start the jupyter notebook . The output in the terminal will look something like below: Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=6bdef677d3503fbb23e1b4fa0c802ee7c20bdcfd4d9b9951 [I 16:14:12.661 NotebookApp] Accepting one-time-token-authenticated connection from ::1 A web browser should open and you should be able to see the jupyter file browser : In the upper right select [New]  [Python 3] You will see a new tab open in your web browser. This new page is a jupyter notebook . To rename the jupyter notebook , click the file name at the top of the page to the right of the jupyter icon. This will open a dialog box where the new name can be typed. Try typing this in the first cell in the notebook to the right of the In [ ]: prompt import this Then click the run button in the middle of the menu at the top of the notebook 2. Windows Start Menu Another way to open a jupyter notebook is to use the Windows start menu. Open the Windows start menu and select [Anaconda3(64 bit)]  [Jupyter Notebook] This will open the jupyter file browser in a web browser tab. In the upper right select [New]  [Python 3] A new notebook will open as a new tab in your web browser 3. Anaconda Navigator The last way we'll review how to open a jupyter notebook is by using the Anaconda Navigator . You can open the Anaconda Navigator using the Windows start menu and selecting [Anaconda3(64-bit)]  [Anaconda Navigator] This will open the Anaconda Navigator . In the middle of the page, in the jupyter notebook tile, click [Launch] This will open the jupyter file browser in a web browser tab. In the upper right select [New]  [Python 3] A new notebook will open as a new tab in your web browser Congratulations! You know how to open a jupyter notebook on your Windows 10. Now go write some Python Code to solve some problems!","tags":"Orientation","url":"opening-a-jupyter-notebook-on-windows.html"},{"title":"Bar charts with error bars using Python, jupyter notebooks and matplotlib","text":"Engineers collect data and make conclusions based on the results. An important way to view results is with statistical charts. In this post we will build a bar chart to compare the tensile strength of 3D-printed ABS plastic compared to the tensile strength of 3D-printed HIPS plastic. We will add error bars to the chart to show the amount of uncertainty in the data. In the bar plot we construct, the height of the bars will represent the mean or average tensile stength. One bar will represent the average strength of ABS and the other bar will show the average strength of HIPS. We will then add error bars to the plot which will represent +1/-1 standard deviation about the mean. We will use Python , the statistics module (part of the Python standard library), and matplotlib to build the bar plot. I recommend that undergraduate engineers use the Anaconda distribution of Python , which comes with matplotlib already installed. For help installing Anaconda, see a previous blog post: Installing Anaconda on Windows 10 . If matplotlib is not available in your version of Python , open a terminal or the Anaconda Prompt and type: $ pip install matplotlib or > conda install matplotlib The data we are going to plot is from the tensile testing of two different kinds of 3D-printed plastic, ABS and HIPS (HIPS stands for High Impact Polystyrene). You can download the data using the link below: 3D-printed-tensile-bar-data.xlsx I'm constructing the plot in a jupyter notebook . You could also build the code in a .py file and run the code to produce the plot. A note about using matplotlib on MacOSX: if you recieve an error message that matplotlib is not installed as a framework, consider using the Anaconda distribution of Python and running the code in a jupyter notebook . To open a new jupyter notebook go to the Anaconda Prompt or a terminal and type: > jupyter notebook Alternativly, you can start a new jupyter notebook by cliking the Windows start button and searching for [Anaconda3] --> [Jupyter Notebook] If jupyter is not installed on your system, you can install it using the Anaconda Prompt or use a terminal and pip : > conda install jupyter or $ pip install jupyter At the top of the jupyter notebook (or .py file), we need to import the required packages: statistics (part of the Python standard library, but still needs to be imported) and matplotlib From the statistics module we will import two functions: mean (average) and stdev (standard deviation). If we use this import line: from statistics import mean, stdev We can use the names mean() and stdev() in our code. However, if we use a more general import line: import statistics Then we will need to call statistics.mean() and statistics.stdev() in our code. matplotlib also needs to be imported. The typical way to do this is with the line: import matplotlib.pyplot as plt Then thoughout our code, we can use plt() instead of writing out matplotlib.pyplot() each time we want to use a matplotlib method. The %matplotlib inline magic command is added so that we can see our plot right in the jupyter notebook . If you build the plot in a .py file, the %matplotlib inline command should be left out as it will return an error. In [1]: # import packages from statistics import mean , stdev import matplotlib.pyplot as plt #include if using a jupyter notebook, remove if using a .py file % matplotlib inline Create two variables which contains the data for ABS and HIPS as a list of individual tensile strength values. After the import lines, we need to create two variables: one variable for the ABS data and one variable for HIPS data. We will assign the data points as a list of numbers saved in two variables. The general format to create a list in Python is to use list_name = [item1, item2, item3] with square brackets on the outside and commas between the items. The items for the two lists came from the .xlsx file that contains the data ( 3D-printed-tensile-bar-data.xlsx ). The tensile strength is in column [F] labeled [Tensile Strength (Mpa)]. Rows 2-17 contain data for ABS and rows 18-37 contain data for HIPS. In [2]: # data ABS = [ 18.6 , 21.6 , 22 , 21 , 18 , 20.9 , 21 , 19.3 , 18.8 , 20 , 19.4 , 16 , 23.8 , 19.3 , 19.7 , 19.5 ] HIPS = [ 10.4 , 4.9 , 10.2 , 10.5 , 10.9 , 12.9 , 11.8 , 8.4 , 10 , 10.6 , 8.6 , 9.7 , 10.8 , 10.7 , 11 , 12.4 , 13.3 , 11.4 , 14.8 , 13.5 ] Find the mean and standard deviation for each set of data We'll use the mean() and stdev() functions from the statistics module to find the mean (or average) and standard deviation of the two data sets. A summary of these two functions is below: statistics module function description mean() calculate the mean or average of a list of numubers stdev() calculate the standard deviation of a list of numbers In [3]: # find the mean using the mean() function from the statistics library ABS_mean = mean ( ABS ) HIPS_mean = mean ( HIPS ) # find the standard deviation using the stdev() function from the statistics library ABS_stdev = stdev ( ABS ) HIPS_stdev = stdev ( HIPS ) Build a simple bar plot Matplotlib's bar plot fuction can be accessed using plt.bar() . We need to include at least two arguments as shown below: plt.bar (['list', 'of' ,'bar', 'labels'], [list, of, bar, heights]) We will pass in ['ABS', 'HIPS'] for our list of bar labels, and [ABS_mean, HIPS_mean] for our list of bar heights. The command plt.show() will show the plot in a jupyter notebook or show the plot in a new window if running a .py file. In [4]: # Build a bar plot plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ]) plt . show () Add axis labels and title The plot looks pretty good, but we should add axis labels (with units) and a title to our plot. We can add the axis labels and titles with plt.xlabel() , plt.ylabel() and plt.title() . We need to pass in strings enclosed in quotes ' ' with these methods. A summary of the matplotlib functions is below: matplotlib function description plt.bar() build a bar plot plt.xlabel() x-axis label plt.ylabel() y-axis label plt.title() plot title plt.show() show the plot In [5]: # build a bar plot plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ]) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . show () Add error bars to the plot We have a nice looking bar plot with two bars, x-axis label, y-axis label and a title. Next we will add error bars to the plot. We will add the error bars by passing a keyword argument in the plt.bar() function. The keyword argument is yerr = [list, of, error, bar, lengths] . A keyword argument is a specific type of argument passed to a function or method that must have a name associated with it. Regular function arguments just need to be in the proper order. Keyword arguments need to be pass with the form keyword_argument_name = value . The general form of the entire plt.bar() line will be: plt.bar (['list', 'of' ,'bar', 'labels'], [list, of, bar, hights], yerr=[list, of, error, bar, lengths]) The first two arguments, ['list', 'of' ,'bar', 'labels'] and [list, of, bar, hights] just need to be in the correct order. The third argument, a keyword argument needs to include yerr = . Our list of error bar lengths will contain the standard deviation for each set of data, ABS_stdev and HIPS_stdev . yerr=[ABS_stdev, HIPS_stdev] In [6]: # build a bar plot plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ], yerr = [ ABS_stdev , HIPS_stdev ]) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . show () Add \"caps\" to the error bars The error bars are on the plot, but they are just vertical lines. Typically, error bars have a horizontal lines at the top and bottom and look sort of like the capital letter I. We can add these horizontal lines or \"caps\" to the top and bottom of the error bars by passing an additional keyword argument to the plt.bar() function called capsize= . We will set the capsize=10 , which is a good size for this plot. You can change the capsize= number to make the horizontal lines longer or shorter. Now our plt.bar() function call contains 4 different arguemnts: plt.bar (['list of bar labels'], [list of bar hights], yerr = [list of error bar lengths], capsize = width) A summary of the arugments passed to the plt.bar() function is below: plt.bar() Arguments description [list of bar labels] 1st argument, a list of strings which provide the labels below the bars [list of bar heights] 2nd argument, a list of numbers which determines the height of each bar yerr = [list of error bar lengths] a keyword argument, must include yerr = . Denotes the height of the error bars. Needs to be a list of numbers capsize = width a keyword argument, must include capsize = . Denotes the width of the error bar horizontal \"caps\". Needs to be a number, not a string In [7]: # build the bar plot plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ], yerr = [ ABS_stdev , HIPS_stdev ], capsize = 10 ) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . show () Save the plot The plot looks complete: two bars, x and y axis labels, title and error bars with caps. Now let's save the plot as an image file so we can import the plot into a Word document or PowerPoint presentation. If you are using a jupyter notebook , you can just right-click on the plot and select [copy image] or [Save Image As...]. To save a plot as an image programmatically, we use the line: plt.savefig('filename.extension') Matplotlib will save the plot as an image file using the file type we specify in the filename extension. For example, if we call plt.savefig('plot.png') , the plot will be saved as a .png image. If we call plt.savefig('plot.jpg') the plot will be saved as a .jpeg image. In [8]: # build a bar plot and save it as a .png image plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ], yerr = [ ABS_stdev , HIPS_stdev ], capsize = 10 ) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . savefig ( 'plot.png' ) plt . show () Increase the .png file image resolution Depending on how the .png image file is viewed: in a jupyter notebook , on the web, in a Word document or in a PowerPoint presentation, the image may look a little blurry. This is because the .png image we created has a fairly low resolution. We can change the resolution by coding: plt.savefig('filename.png', dpi = 300) Where dpi=300 specifies a resolution of 300 dots per square inch. We can specify a higher resolution or lower resoltution then 300 dpi. A higher resolution will increase the image file size, but will look better when magnified. In [9]: # build a bar plot and save it as a .png image plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ], yerr = [ ABS_stdev , HIPS_stdev ], capsize = 10 ) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . savefig ( 'plot.png' , dpi = 300 ) plt . show () The full script A summary of the full script is below: In [10]: # import packages from statistics import mean , stdev import matplotlib.pyplot as plt #include if using a jupyter notebook, remove if using a .py file % matplotlib inline # data ABS = [ 18.6 , 21.6 , 22 , 21 , 18 , 20.9 , 21 , 19.3 , 18.8 , 20 , 19.4 , 16 , 23.8 , 19.3 , 19.7 , 19.5 ] HIPS = [ 10.4 , 4.9 , 10.2 , 10.5 , 10.9 , 12.9 , 11.8 , 8.4 , 10 , 10.6 , 8.6 , 9.7 , 10.8 , 10.7 , 11 , 12.4 , 13.3 , 11.4 , 14.8 , 13.5 ] # find the mean using the mean() function from the statistics library ABS_mean = mean ( ABS ) HIPS_mean = mean ( HIPS ) # find the standard deviation using the stdev() function from the statistics library ABS_stdev = stdev ( ABS ) HIPS_stdev = stdev ( HIPS ) # build a bar plot and save it as a .png image plt . bar ([ 'ABS' , 'HIPS' ],[ ABS_mean , HIPS_mean ], yerr = [ ABS_stdev , HIPS_stdev ], capsize = 10 ) plt . xlabel ( '3D-printer Fillament Material' ) plt . ylabel ( 'Tensile Strength (MPa)' ) plt . title ( 'Tensile Strength of 3-D Printed ABS and HIPS Tensile Bars' ) plt . savefig ( 'plot.png' , dpi = 300 ) plt . show ()","tags":"matplotlib","url":"bar-plot-with-error-bars-jupyter-matplotlib.html"},{"title":"Using a Temperature Sensor with Micropython running on an Adafruit Feather Huzzah ESP8266","text":"This is the fourth part of a multipart series on Micropython. In this last post of the series, we blinked an LED on and off using Micropython. In this post, we will connect a temperature sensor to an Adafruit Feather Huzzah and use the Micropython REPL to read the temperature. The posts in this series: What is Micropython? Installing Micropython on an Adafruit Feather Huzzah ESP8266 Blink an LED on an Adafruit Feather Huzzah ESP8266 using Micropython Read the temperature from a MCP9808 breakout board using Micropyton (this post) Use Micropython to connect an Adafruit Feather Huzzah to a WiFi network Upload Micropython code to turn an Adafruit Feather Huzzah into a WiFi-enabled IoT weather station Use pandas and matplotlib to plot the weather data from a WiFi-enabled IoT weather station. Upload MicroPython to a cheap $3 ESP-01 module Build custom firmware to turn the $3 ESP-01 into an low-cost WiFi enabled IoT switch. Before we can use the MCP9808 temperature sensor running on the Adafruit Feather Huzzah ESP8266, Micropython needs to be installed on the board and Putty needs to be installed in Windows 10 to communicate with the board over serial. See a previous post to install Micropython on your board and Putty on a Windows machine. Summary of Steps: Connect the temperature sensor to the Adafruit Feather Huzzah board Connect the Adafruit Feather Huzzah to the computer with a USB cable and bring up the Micropython REPL using Putty. Run code at the Micropython REPL to read the temperature 1. Connect the MCP9808 temperature sensor to the Adafruit Feather Huzzah board Connect the MCP9808 temperature sensor breakout board to the Feather Huzzah board with jumper wires. There are four connections: A 3V power line from the Feather Huzzah to the MCP9808 Vdd pin, GND connected between both boards, and the I2C data and clock lines. On the Feather Huzzah, the I2C data line is SDA (pin 4) and the I2C clock line is SCL (pin 5). These connect with the MPC9808 I2C data line SDA and the MPC9808 I2C clock line SCL. Unlike Serial communication where RX connects to TX, in I2C SDA connects to SDA and SCL connects to SCL. Feather Huzzah wire MCP9808 3V red Vdd GND black GND SDA green SDA SCL yellow SCL 2. Connect the Adafruit Feather Huzzah to the computer with a USB cable and bring up the Micropython REPL using Putty. Connect the Adafruit Feather Huzzah to the computer with a microUSB cable. Ensure this is a data cable, not just a charging cable. Open Putty and connect to the Feather Huzzah using the proper serial port (COM#) and 115200 baud. (Remember to use the Serial radio button under Connection Type: ) This should bring up the Micropython REPL prompt >>> . If you can't see the >>> prompt, try typing [Enter], Ctrl-D, pushing the RESET button on the Feather Huzzah. If that doesn't work, try closing putty then unplugging then replugging the USB cable. 3. Run code at the Micropython REPL to read the temperature In the Putty Serial Window, we will import the machine module and then create an instance of the machine.I2C class with the scl and sda parameters set as scl=machine.Pin(5) and sda=machine.Pin(4) . Then we create an empty bytearray which will be used to store the data coming in from the MCP9808 temperature sensor. As strings in Micropython are UTF-8 encoded by defaut, like in Python 3, a bytearray needs to be used to read the raw output from the MCP9808 chip registers. The command i2c.readfrom_mem_into() method brings in the data from the sensor and saves it to our byte_data variable. The arguments inside the method 24 and 5 correspond to the I2C memory address and registry address of the temperature data stored in the MCP9808 temperature sensor. >>> import machine >>> i2c = machine . I2C ( scl = machine . Pin ( 5 ), sda = machine . Pin ( 4 )) >>> byte_data = bytearray ( 2 ) >>> i2c . readfrom_mem_into ( 24 , 5 , byte_data ) >>> value = byte_data [ 0 ] << 8 | byte_data [ 1 ] >>> temp = ( value & 0xFFF ) / 16.0 >>> if value & 0x1000 : ... temp -= 256.0 ... print ( temp ) Next steps: In the next post we will use Micropython to connect the Adafruit Feather Huzzah to a WiFi network.","tags":"micropython","url":"micropython-temp-sensor.html"},{"title":"Using the Micropython REPL on an Adafruit Feather Huzzah ESP8266","text":"This is the third part of a multipart series on Micropython. In last post of the series , we installed Micropython on an Adafruit Feather Huzzah ESP8266 microcontroller using Python and a package called esptool . In this post, we are going to write commands to the Micropython REPL (the Micropython prompt) to turn on and off an LED connected to the Feather Huzzah board. The posts in this series: What is Micropython? Installing Micropython on an Adafruit Feather Huzzah ESP8266 Blink an LED on an Adafruit Feather Huzzah ESP8266 using Micropython (This post) Read the temperature from a MCP9808 breakout board using Micropyton Use Micropython to connect an Adafruit Feather Huzzah to a WiFi network Upload Micropython code to turn an Adafruit Feather Huzzah into a WiFi-enabled IoT weather station Before you can use the Micropython REPL (the Microython prompt) running on the Adafruit Feather Huzzah ESP8266, Micropython needs to be installed on the board and Putty needs to be installed to communicate with the board over serial. See the previous post on how to install Micropython on the board and install Putty on a Windows 10 machine. Summary of Steps: Connect the Adafruit Feather Huzzah ESP8266 using a USB cable Determine which COM port the board is connected to using the Windows Device Manager Open Putty and connect to the board at 115200 baud Run commands at the prompt to turn the builtin LED on the Adafruit Feather Huzzah ESP8266 on and off 1. Connect the Adafruit Feather Huzzah ESP8266 board to the laptop Use a microUSB cable to connect the Feather Huzzah to the computer. Make sure that the microUSB cable is a full USB data cable and not just a simple power cable. The first cable I tried was just a charging mobile phones and I couldn't figure out why Putty wasn't working. Switching out the cable was all it took to get it to work. 2. Determine which serial port the Feather Huzzah is connected to Use Windows Device Manager to determine which serial port the Feather Huzzah is connected to. On my Windows 10 laptop, it usually comes up as COM4 . You can find the serial port by looking in the Ports (COM & LPT) category of the Windows Device Manager. Look for something like Silicon Labs CP210x USB to UART Bridge (COM4) in the Ports (COM & LPT) menu. It is the COM# that you are looking for. 3. Use Putty to connect to the Feather Huzzah Ensure the Feather Huzzah board is connected with a USB cable, then connect to it with Putty using the proper serial port (COM#) and 115200 baud. Remember to use the Serial radio button under Connection Type: to select serial communication or you will be trying to communicate with the Feather Huzzah over SSH which won't work. This should bring up the Micropython REPL prompt >>> . If you can't see the >>> prompt, try typing [Enter], Ctrl-D, pushing the RESET button on the Feather Huzzah or unplugging then replugging the USB cable. 4. Run commands at the prompt to turn the built-in LED on the Adafruit Feather Huzzah ESP8266 on and off At the micropython REPL (the Micropython command promt >>> ) try the following commands: >>> print('Micropython for Engineers!') Micropython for Engineers If we import the sys module, we can see the Micropython implementation and platform. >>> import sys >>> sys . implementation ( name = 'micropython' , version = ( 1 , 9 , 3 )) >>> sys . platform 'esp8266' If you see similar output, that means Micropython is working on the Feather Huzzah. We can also view the flash memory size of our Feather Huzzah and the size of the Micropyton firmware we installed. Try this at the Micropython prompt: >>> import port_diag We can see the flash memory size is 4 MB. Below the label Firmware checksum: we can see a line for size: 600872 . This means the size of our Micropythpon installation is about 600 KB or 0.6 MB. Just over half a megabyte and we are running a working version of Python! Now let's turn the Feather Huzzah's built-in LED on and off. The Feather Huzzah has a built-in red LED connected to Pin 0. We can access this LED with Micropython's machine module. First we use the machine module to create a Pin object. The first argument when we instantiate the Pin object is the pin number on the board (in this case 0 ). Pin zero on the Feather Huzzah is connected to the built-in red LED. The second argument is the pin type. We want Pin 0 to act as an ouput pin ( machine.Pin.OUT ). We are going to assign our pin the attribute .on() or .off() . This will cause the Feather board to output a positive voltage or no voltage to Pin 0 to turn the built-in red LED on and off. You can also connect Pin 0 to an LED through a resistor (then to ground) and have this LED turn on and off. >>> import machine >>> pin = machine . Pin ( 0 , machine . Pin . OUT ) Note that Pin 0 on the Adafruit Feather Huzzah is kind of wired \"backwards\". We call pin.off() and the built-in LED turns on and call pin.on() and the built-in LED turns off . >>> pin.on() >>> pin.off() >>> pin.on() >>> pin.off() Now let's see if we can make the LED blink. We'll do this with a simple for loop. At the micropython REPL, initiating a loop will automatically indent the next line, so a tab is not needed before the pin.on() statement. To run the loop, we type backspace on an empty line (to backspace from an indented line) and hit return. >>> import time >>> for i in range ( 10 ): ... pin . on () ... time . sleep ( 1 ) ... pin . off () ... time . sleep ( 1 ) ... This will blink the LED on and off for a total of 20 seconds. Next steps: In the next post, we'll connect to a I2C temperature sensor to the Adafruit Feather Huzzah and use Micropython to read the temperature.","tags":"micropython","url":"micropython-REPL.html"},{"title":"Installing Micropython on an Adafruit Feather Huzzah ESP8266","text":"This is the second part of a multipart series on Micropython. Micropython is a port of the Python programming language that runs on small, inexpensive microcontrollers. In this post, we will install Micropython on an Adafruit Feather Huzzah ESP8266 board using Python and a package called esptool. In subsequent posts we will build our Feather Huzzah microcontroller into a WiFi-enabled weather station. The posts in this series: What is Micropython? Installing Micropython on an Adafruit Feather Huzzah ESP8266 (This post) Blink an LED on an Adafruit Feather Huzzah ESP8266 using Micropython Read the temperature from a MCP9808 breakout board using Micropyton Use Micropython to connect an Adafruit Feather Huzzah to a WiFi network Upload Micropython code to turn an Adafruit Feather Huzzah into a WiFi-enabled IoT weather station To install Micropython on a microcontroller, like the Adafruit Feather Huzzah ESP8266, we need the following hardware: Hardware Purpose Windows 10 Laptop Used to download Micropython and install Micropython on the microcontroller Adafruit Feather Huzzah ESP8266 Microcontroller that will run Microphythonn microUSB Cable Used to connect the laptop to the microcontroller To install Micropython we will use the following software and tools: Software Purpose Windows 10 Download Micropython Anaconda distribution of Python Run the esptool that installs Micropython Anaconda Prompt Install the esptool package using pip esptool a pip installable package used to install Micropython firmware .bin file the version of Micropython will run on the Feather Huzzah board Summary of Steps: Install the Anaconda distribution of Python Create a new conda environment and pip install esptool Download the latest Micropython .bin firmware file Install the SiLabs driver for the Adafruit Feather Huzzah ESP8266 Connect the Adafruit Feather Huzzah ESP8266 board to the laptop using a microUSB cable Determine which serial port the Feather Huzzah is connected to Run the esptool to upload the .bin firmware file to the Feather Huzzah Download and install Putty , a serial monitor Use Putty to connect to the Feather Huzzah and run commands in the Micropython REPL 1. Install the Anaconda distribution of Python If you don't have Anaconda installed already, go to Anaconda.com/download and install the latest version. The Anaconda distribution of Python is the Python distribution I recommend for undergraduate engineers. You want to download and install the Python 3.6 Version (the Python 2.7 Version is legacy Python). Most laptops and desktops run a 64-bit version of Windows 10. If in doubt, you can check your Windows installation, or just go with the 64-bit version. 2. Create a new conda environment and install esptool.py It's best practice when using Python to work in virtual environments. We'll create a new virtual environment with conda to use with our Micropython projects. Open the Anaconda prompt and create a new virtual environment named micropython . Activate the environment with the conda activate command. After activating the virtual environment you should see (micropython) before the Anaconda Prompt. Once inside the virtual environment, use pip to install esptool . The esptool will be used to upload the Micropython .bin firmware file onto the Adafruit Feather Huzzah board. Confirm that esptool is installed in the (micropython) virtual environment with conda list . I also created a new directory in the Documents folder called micropython to store all the project files. conda create -n micropython python=3.6 conda activate micropython (micropython) pip install esptool (micropython) conda list (micropython) cd Documents (micropython) mkdir micropthon (micropython) cd micropython 3. Download the latest micropython firmware .bin file Go to github and download the latest .bin firmware file. Move the .bin firmware file to a new micropython directory. The .bin firmware file is the version of Micropython that will run on the Adafruit Feather Huzzah ESP8266. Straight from Adafruit, the little microcontroller does not have Micropyton installed, so we need to install Micropython ourselves. After installing the Micropython .bin firmware file onto the board, we will be able to bring up the Micropython REPL prompt, type commands into the Micropython REPL and run Micropython .py scripts on the board. 4. Install the SiLabs driver for the Adafruit Feather Huzzah ESP8266 Before we can connect the Adafruit Feather Huzzah to the computer, we need a specific driver installed. For my Windows 10 laptop to see the Adafruit Feather Huzzah board, the CP210x USB to UART Bridge VCP driver needs to be downloaded from SiLabs and installed. This is quick and easy, but does require admin privileges. 5. Connect the Adafruit Feather Huzzah ESP8266 board to the laptop Use a microUSB cable (the same kind of cable that charges many mobile phones) to connect the Feather Huzzah to the computer. Make sure that the microUSB cable is a full USB data cable and not just a simple power cable. I had trouble getting the Feather Huzzah to work, and it turned out the reason was the micoUSB cable was only a charging cable and could not transfer data. 6. Determine which serial port the Feather Huzzah is connected to Use Windows Device Manager to determine which serial port the Feather Huzzah board is connected to. We will need the serial port as one of the parameters when we upload the .bin firmware file on the board. Look for something like Silicon Labs CP210x USB to UART Bridge (COM4) in the Ports (COM & LPT) menu. The USB to UART bridge is actually the Feather Huzzah board. CP210x refers to the chip that handles serial communication on the Feather Huzzah, not the esp8266 chip itself. Make note of the number after (COM ) . It often comes up as (COM4) but it may be different on your computer. The first time I plugged the board into my laptop, Windows could't see the board. I looked through the Device Manager under the Ports menu and the Feather board just didn't show up. Turns out the first USB cable I used was just a charging only cable. When I switched this out for a microUSB data cable, the board came right up under Ports (COM & LPT) . 6. Run esptool to upload the .bin file to the Feather Huzzah Open the Anaconda Prompt and cd into the micropython directory with the .bin file. You can use the dir command to see the directory contents. Make sure the .bin firmware file is in the directory. It will be called something like esp8266-20171101-v1.9.3.bin . Activate the micropython environment with conda activate micropython . Run esptool --help to ensure esptool is installed properly. Note there is no .py extension after esptool . On my Windows laptop, the command esptool worked, but the command esptool.py did not (this is different than the commands shown on the Micropython docs ). If you try to run esptool and you are not in the (micropython) virtual environment, you will get an error. cd Documents cd micropython pwd Documents/micropython dir conda activate micropython (micropython) esptool --help Before we write the .bin firmware file to the board, we should first erase the flash memory on the Feather Huzzah using the esptool erase_flash command. Make sure to specify the --port . This is the COM port you found in the Windows Device Manager. In my case the port was COM4 . (micropython) esptool --port COM4 erase_flash Now it's time to write the .bin firmware file to the flash memory on the board using the esptool write_flash command. Make sure to use the exact .bin firmware file name you see sitting in the micropython directory. The port has to be set as the port you found in the Windows Device Manager. ---baud is the baud rate, or upload speed. I found that --baud 460800 worked, but you could also specify --baud 115200 which is slower. The upload time was a matter of seconds with either baud rate. The 0 after --flash_size=dectect means we want the firmware to be written at the start of the flash memory (the 0 th position) on the board. Again, make sure the .bin firmware file name is correct. It is easy to mistype. Another issue I ran into was that I tried to use the command esptool.py instead of esptool as shown on the Micropython docs . The documentation for Micropython on the ESP8266 specifies the command esptool.py (including the .py file extension). This did work on my Windows 10 machine. Omitting the .py file extension, and running esptool worked instead. (micropython) esptool --port COM4 --baud 460800 write_flash --flash_size=detect 0 esp8266-20171101-v1.9.3.bin 7. Download and install Putty, a serial monitor Now that Micropthon is installed on the board, we need to talk to our board over a serial connection. Windows 10 doesn't have a built-in serial monitor (like screen on OSX and Linux). So we need to download and install Putty . Putty is a lightweight SSH and serial client for Windows. Putty will allow us to communicate with the Adafruit Feather Huzzah board. Putty can be downloaded here . Putty is pretty small and the download and install should be pretty quick. 8. Use Putty to connect to the Feather Huzzah Ensure the Feather board is connected to the computer with a USB cable and ensure you can see the board in the Windows Device Manager. Then use Putty to connect to the board over serial. Make sure you specify the correct serial port in the Serial line box and 115200 baud in the Speed box. Micropython is set to run at 115200 baud , other baud rates will lead to junk characters in the serial monitor. I had trouble finding the serial connection option in Putty. When I opened Putty, the default was an SSH connection. We can't connect to the Feather Huzzah over SSH. You need to select the Serial radio button below the header Connection type: near the top of the Putty window. If you see >>> the Micropython REPL (the Micropython prompt) is running and the Adafruit Feather Huzzah ESP8266 is working! This version of Python isn't running on your computer, it's Micropython running on the little microcontroller! Sometimes I had to type [Enter] or Ctrl-D to get the >>> REPL prompt to show up. A few times I needed to close Putty, unplug then replug the board and try Putty again. The Feather Huzzah also has a tiny little black RESET button that can be pressed to restart the board. At the >>> Micropython REPL prompt try the following commands: >>> print ( 'Micropython for Engineers!' ) Micropython for Engineers >>> import sys >>> sys . platform 'esp8266' Next steps: In the next post of the series, we will use the Micropython REPL running on the Adafruit Feather Huzzah to blink an LED.","tags":"micropython","url":"micropython-install.html"},{"title":"What is Micropython?","text":"This is the first part of a multipart series on Micropython. In this post we'll review what Micropython is, what it is used for and how it is both similar and different from \"regular\" Python. We'll also discuss why Micropython is relevant to undergraduate engineers. 1. What is Micropython? Micropython is a port, or version of Python designed to run on small, inexpensive, low-power microcontrollers. Examples of microcontrollers that Micropython can run on include the pyboard , the WiPy and ESP8266 boards like the Adafruit Feather Huzzah . Normally, Python is run on a desktop or laptop computer (also on big servers at server farms). Compared to a desktop or laptop, microcontrollers are much smaller, cheaper and less powerful. A \"regular\" version of Python can't run on small, cheap microcontrollers because Python is too resouce intensive. Regular Python takes up too much hard disk space, runs on too much RAM and requires a more powerful processor than microcontrollers have. It is pretty amazing that a version of Python (Micropython) runs on these small, cheap microcontrollers like the ESP8266. To get Micropython to run at all on these small boards, Micropython only contains a subset of all the standard library modules inlcuded with \"regular\" Python. Some of the libraries that are included with Micropython don't have the full set of functions and classes that come with the full version of Python. This allows Micropython to be compact (around 600 kB for the ESP8266 port) and only use a small amount of RAM (down to 16k according to the Micropython main page ) You can try using Micropython online with this neat Micropython online emulator . The emulator allows you to run commands at a Micropyton Prompt and see the result on a virtual pyboard. 2. What is Micropyton used for? Micropython is installed on small, cheap microcontrollers like the ESP8266 . Anthing these small microcontrollers can do, Micropython can do. This includes using the microcontroller as a remote sensor to measure things like temperature, humidity and light level. Micropython can also be used to blink LED's, control arrays of LED's, or run small displays. Micropython can control servo motors, stepper motors and solenoids. Civil Engineers could use Micropython to monitor water levels. Michanical Engineers could use Micropython to drive robots. Electrical Engineers could use micropython to measure voltage levels in embedded systems. In the later posts in this series, we will use Micropython, running on a small cheap ESP8266 board, to create a remote internet-connected weather station. The last posts in the series will use Micropyton, running on a really cheap (around $2) ESP-01 module to turn on and off an LED from any computer connected to the internet anywhere in the world. 3. Why should undergraduate Engineers learn Mircopython? Using Python to solve engineering problems such as calculations, statistics, modeling and visulization is really useful for undergraduate Engineers. But Python on it's own is fairly limited in controlling devices outside the computer it's running on. You don't want to leave a laptop in a remote estuary to meausure water temperature, but you could leave a little microcontroller and low-cost temperature sensor. A small robot can't carry around a heavy laptop, but a small, light, low-power board could run a simple robot. You don't want to use a laptop for every small electrical measurement or embedded system control, but a $2 WiFi module would work. In addition, learning how to use Micropython on small, cheap microcontrller can help undergraduates Engineers understand how programming works. It is a different kind of feedback and excitment seeing a motor whirl around compared to seeing a picture of a motor with the speed displayed as text. There is a different kind of wonder seeing an array of LED's light up compared to a 2-D plot on a computer screen. Plus Micropython is just fun! It's as easy to program Micropython as it is to program regular Python. The little projects you can do with Micropython running on a small, low-cost board are almost unlimited. We could send Micropython to space in a micro-satalite, or burry Micropython underground in a small borring machine, or launch Micorpython into the sky on a weather ballon. Next steps: In the next post, we will install Micropython on a small, cheap ESP8266 microcontroller board called the Adafruit Feather Huzzah. Once Micropython is installed on the board, we will run a couple commands at the Micropython REPL running on the board.","tags":"micropython","url":"what-is-micropython.html"},{"title":"Plotting sine and cosine with matplotlib and Python","text":"Plotting is an essential skill for Engineers. Plots can reveal trends in data and outliers. Plots are a way to visually communicate results with your engineering team, supervisors and customers. In this post, we are going to plot a couple of trig functions using Python and matplotlib . Matplotlib is a plotting library that can produce line plots, bar graphs, histograms and many other types of plots using Python. Matplotlib is not included in the standard library. If you downloaded Python from python.org , you will need to install matplotlib and numpy with pip on the command line. > pip install matplotlib > pip install numpy If you are using the Anaconda distribution of Python (which is the distribution of Python I recommend for undergraduate engineers) matplotlib and numpy (plus a bunch of other libraries useful for engineers) are included. If you are using Anaconda , you do not need to install any additional packages to use matplotlib . In this post, we are going to build a couple of plots which show the trig functions sine and cosine . We'll start by importing matplotlib and numpy using the standard lines import matplotlib.pyplot as plt and import numpy as np . This means we can use the short alias plt and np when we call these two libraries. You could import numpy as wonderburger and use wonderburger.sin() to call the numpy sine function, but this would look funny to other engineers. The line import numpy as np has become a common convention and will look familiar to other engineers using Python. In case you are working in a Juypiter notebook, the %matplotlib inline command is also necessary to view the plots directly in the notebook. In [1]: import matplotlib.pyplot as plt import numpy as np # if using a jupyter notebook % matplotlib inline Next we will build a set of x values from zero to 4 in increments of 0.1 radians to use in our plot. The x-values are stored in a numpy array. Numpy's arange() function has three arguments: start , stop , step . We start at zero, stop at 4 and step by 0.1 radians. Then we define a variable y as the sine of x using numpy's sin() function. In [2]: x = np . arange ( 0 , 4 * np . pi , 0.1 ) # start,stop,step y = np . sin ( x ) To create the plot, we use matplotlib's plt.plot() function. The two arguments are our numpy arrays x and y . The line plt.show() will show the finished plot. In [3]: plt . plot ( x , y ) plt . show () Next let's build a plot which shows two trig functions, sine and cosine. We will create the same two numpy arrays x and y as before, and add a third numpy array z which is the cosine of x . In [4]: x = np . arange ( 0 , 4 * np . pi , 0.1 ) # start,stop,step y = np . sin ( x ) z = np . cos ( x ) To plot both sine and cosine on the same set of axies, we need to include two pair of x,y values in our plt.plot() arguments. The first pair is x,y . This corresponds to the sine function. The second pair is x,z . This correspons to the cosine function. If you try and only add three arguments as in plt.plot(x,y,z) , your plot will not show sine and cosine on the same set of axes. In [5]: plt . plot ( x , y , x , z ) plt . show () Let's build one more plot, a plot which shows the sine and cosine of x and also includes axis labels, a title and a legend. We build the numpy arrays using the trig functions as before: In [6]: x = np . arange ( 0 , 4 * np . pi - 1 , 0.1 ) # start,stop,step y = np . sin ( x ) z = np . cos ( x ) The plt.plot() call is the same as before using two pairs of x and y values. To add axis labels we will use the following methods: matplotlib method description example plt.xlabel() x-axis label plt.xlabel('x values from 0 to 4pi') plt.ylabel() y-axis label plt.ylabel('sin(x) and cos(x)') plt.title() plot title plt.title('Plot of sin and cos from 0 to 4pi') plt.legend([ ]) legend plt.legend(['sin(x)', 'cos(x)']) Note that plt.legend() method requires a list of strings (['string1', 'string2']), where the individual strings are enclosed with qutoes, then seperated by commas and finally inclosed in brackets to make a list. The first string in the list corresponds to the first x-y pair when we called plt.plot() , the second string in the list corresponds to the second x,y pair in the plt.plot() line. In [7]: plt . plot ( x , y , x , z ) plt . xlabel ( 'x values from 0 to 4pi' ) # string must be enclosed with quotes ' ' plt . ylabel ( 'sin(x) and cos(x)' ) plt . title ( 'Plot of sin and cos from 0 to 4pi' ) plt . legend ([ 'sin(x)' , 'cos(x)' ]) # legend entries as seperate strings in a list plt . show ()","tags":"Plotting","url":"plotting-sin-cos-with-matplotlib.html"},{"title":"Python Data Types","text":"Python has many useful built in data types. Python variables can store different types of data and can be created dynamically, without first defining a data type. It's useful for engineers to understand a couple of Python's core data types in order to write well constructed code. Below we will discuss a few different data types. Integers Integers are one of the Python data types. An integer is a whole number, negative, positive or zero. In Python, integer variables can be defined by simply assigning a whole number to a variable name. We can determine data type of a variable using the type() function. >>> a = 5 >>> type ( a ) < class ' int '> >>> b = - 2 >>> type ( b ) < class ' int '> >>> z = 0 >>> type ( z ) < class ' int '> Floating Point Numbers Floating point numbers or floats are another Python data type. Floats are decimals, positive, negative and zero. Floats can also be numbers in scientific notation which contain exponents. In Python, a float can be defined using a decimal point . when a variable is assigned. >>> c = 6.2 >>> type ( c ) < class ' float '> >>> d = - 0.03 >>> type ( d ) < class ' float '> >>> e = 6.02e23 >>> e 6.02e+23 >>> type ( e ) < class ' float '> To make sure a variable is a float instead of an integer even if it is a whole number, a trailing decimal point . is used. Note the difference when a decimal point comes after the a whole number: >>> g = 5 # no decimal point >>> type ( g ) < class ' int '> >>> g = 5. # decimal point >>> type ( g ) < class ' float '> Boolean The boolean data type is either True or False. In Python, boolean variables are defined by the True and False key words. Note that True and False must have an Upper Case first letter. Using a lowercase true returns an error. a = True type ( a ) < class ' bool '> b = False type ( b ) < class ' bool '> c = true Traceback ( most recent call last ): File \"<input>\" , line 1 , in < module > NameError : name 'true' is not defined d = false Traceback ( most recent call last ): File \"<input>\" , line 1 , in < module > NameError : name 'false' is not defined String Strings are sequences of letters, numbers, spaces and symbols. In Python, strings can be almost any length and can contain spaces. String variables are assigned in Python using quotation marks ' ' . string = 'z' type ( string ) < class ' str '> string = 'Engineers' type ( string ) < class ' str '> A numbers and decimals can be defined as strings too. If a decimal number is defined using quotes ' ' , it will be saved as a string rather than as a float. This is true of whole numbers as well. Whole numbers defined using quotes will become strings just like decimal numbers defined using quotes. num = '5.2' type ( num ) < class ' str '> num = '2' type ( num ) < class ' str '> Complex numbers One final data type useful to engineers are complex numbers. A complex number is defined in Python using a real component + imaginary component j . The letter j must be used in the imaginary component. Using the letter i will return an error. Note how imaginary numbers can be added to integers and floats. comp = 4 + 2j type ( comp ) < class ' complex '> comp2 = 4 + 2 i &#94; SyntaxError : invalid syntax intgr = 3 type ( intgr ) < class ' int '> comp_sum = comp + intgr print ( comp_sum ) ( 7 + 2j ) flt = 2.1 comp_sum = comp + flt print ( comp_sum ) ( 6.1 + 2j ) Converting between different data types The number five can be an integer, or a float or a string depending on how it is assigned. Python has built in functions to convert between data types. The int() float() and str() methods will convert our 5 from one Python data type to another. int_num = 5 type ( int_num ) < class ' int '> float_num = float ( int_num ) type ( float_num ) < class ' float '> str_num = str ( int_num ) type ( str_num ) < class ' str '> str_num '5' Summary Data Type Python Class Description Examples integer int whole numbers: negative positive and zero 5 -2 0 floating point number float decimal number: negative positive and zero. Can contain an exponent 2.3 -0.05 4.5e8 boolean bool True or False True False string str sequence of letters, numbers, spaces and symbols Gabby Engineering! 5 complex number comp number with both real and imaginary components 4+2j 0-2j 6+0j","tags":"Orientation","url":"python-data-types.html"},{"title":"Unit conversions with Python and Pint","text":"Units and unit conversions are BIG in engineering. Engineers solve the world's problems in teams. Any problem solved has to have a context. How heavy can a rocket be and still make it off the ground? What thickness bodypannels keep occupants save during a crash? In engineering, a number without a unit is like a fish without water. It just flops around hopelessly without context around it is useless. How can we get help using units? Programming is one way. We are going to complete some uit conversion problmes using Python and Pint. Pint is a Python package used for unit conversions. See the ( Pint documentation ) for more examples. I recommend that undergraduate engineers use Python 3 (Python 2.7 is legacy python) and the Anaconda distribution. To use Pint , we need to install pint in our wroking version of Python. Open up the Anaconda Prompt : > pip install pint I am working on a Windows 10 machine. You can check your operating system and Python version using the code below: In [1]: import platform print ( 'Operating System: ' + platform . system () + platform . release ()) print ( 'Python Version: ' + platform . python_version ()) Operating System: Windows10 Python Version: 3.6.3 Before we can complete a unit conversion with the Pint package, we need to import the Pint module and instantiate a UnitRegistry object. The new ureg object contains all the units used in the examples below. In [2]: import pint ureg = pint . UnitRegistry () For our first problem, we will complete the following converison: Convert 252 kW to Btu/day We'll create a variable called power with units of kilowatts (kW). To create the kW unit, we'll use our ureg object. In [3]: power = 252 * ureg . kW print ( power ) 252 kilowatt To convert power to Btu/day, we use Pint's .to() method. The .to() method does not change the units of power in place. We need to assign the output of the .to() method to another variable power_in_Btu_per_day In [4]: power_in_Btu_per_day = power . to ( ureg . Btu / ureg . day ) print ( power_in_Btu_per_day ) 20636632.5971578 btu / day Another probem: Convert 722 MPa to ksi In [5]: stress = 722 * ureg . MPa In [6]: stress_in_ksi = stress . to ( ureg . ksi ) print ( stress_in_ksi ) 104.71724664121106 kip_per_square_inch Next problem: Convert 1.620 m/s 2 to ft/min 2 In [7]: accel = 1.620 * ureg . m / ( ureg . s ** 2 ) print ( accel ) 1.62 meter / second ** 2 This time we will use the .ito() method. Using .ito() will convert the units of accel in place. In [8]: accel . ito ( ureg . ft / ( ureg . min ** 2 )) print ( accel ) 19133.85826771654 foot / minute ** 2 Convert 14.31 x 10 8 kJ kg mm -3 to cal lb m / in 3 In [9]: quant = 14.31e8 * ureg . kJ * ureg . kg * ureg . mm ** ( - 3 ) print ( quant ) 1431000000.0 kilogram * kilojoule / millimeter ** 3 In [10]: quant . ito ( ureg . cal * ureg . lb / ( ureg . inch ** 3 )) print ( quant ) 1.2356155557389996e+16 calorie * pound / inch ** 3","tags":"engineering","url":"unit-conversions-with-pint.html"},{"title":"Bar charts with error bars using Python and matplotlib","text":"Bar charts with error bars are useful in engineering to show the confidence or precision in a set of measurements or calculated values. Bar charts without error bars give the illusion that a measured or calculated value is known to high precision or high confidence. In this post we will build a bar plot using Python and matplotlib. The plot will show the coefficient of thermal expansion (CTE) for three different materials based on a small data set. We will then add error bars to this chart based on the standard deviation of the data. A bar chart with error bars is shown below. Note the labels on the x-axis and the error bars at the top of each bar. In order to build this plot, we need a couple of things Asset Description Python (version 3.6) Run the program Anaconda Prompt create the virtual environment and install packages numpy calculate the mean and standard deviation matplotlib build the plot data set data to plot To get going, we'll use the Anaconda Prompt to create a new virtual environment. Select Anaconda Prompt from the windows start menu. (If using OSX or Linux, the terminal could also be used) Once you have the Anaconda Prompt open, type the following command to create a new virtual environment: conda create -n errorbars Then to activate our new virtual environment, type the following into the Anaconda Prompt conda activate errorbars Now that the errorbars virtual environment is active, you should see (errorbars) in parenthesis before the Anaconda Prompt . Next install matplotlib and numpy using conda . (pip will work to install these packages as well) You can write both packages on the same line or use two different conda install lines. conda install matplotlib numpy Make sure that our (errorbars) virtual environment has matplotlib and numpy installed: conda list Now create a new Python script called errorbars.py . At the top of the script we need to import numpy and matplotlib . #errorbars.py import numpy as np import matplotlib.pyplot as plt Next we need to read in our data. The chart below shows the measured coefficient of thermal expansion (CTE) of three metals: Aluminum, Copper and Steel. The units for coefficient of thermal expansion is per degrees C ( / C) Coefficient of thermal expansion of three metals (units: / C) Sample Aluminum Copper Steel 1 6.4e-5 4.5e-5 3.3e-5 2 3.01e-5 1.97e-5 1.21e-5 3 2.36e-5 1.6e-5 0.9e-5 4 3.0e-5 1.97e-5 1.2e-5 5 7.0e-5 4.0e-5 1.3e-5 6 4.5e-5 2.4e-5 1.6e-5 7 3.8e-5 1.9e-5 1.4e-5 8 4.2e-5 2.41e-5 1.58e-5 9 2.62e-5 1.85e-5 1.32e-5 10 3.6e-5 3.3e-5 2.1e-5 We'll put this data into three different numpy arrays, one array for each metal. Notice the syntax np.array([ ... ]) has a parenthesis ( followed by a square bracket [ . We are passing a Python list, [ denoted with square brackets ] into a the numpy array function (argument enclosed in parenthesis). # Enter in the raw data aluminum = np . array ([ 6.4e-5 , 3.01e-5 , 2.36e-5 , 3.0e-5 , 7.0e-5 , 4.5e-5 , 3.8e-5 , 4.2e-5 , 2.62e-5 , 3.6e-5 ]) copper = np . array ([ 4.5e-5 , 1.97e-5 , 1.6e-5 , 1.97e-5 , 4.0e-5 , 2.4e-5 , 1.9e-5 , 2.41e-5 , 1.85e-5 , 3.3e-5 ]) steel = np . array ([ 3.3e-5 , 1.2e-5 , 0.9e-5 , 1.2e-5 , 1.3e-5 , 1.6e-5 , 1.4e-5 , 1.58e-5 , 1.32e-5 , 2.1e-5 ]) Now we need to calculate the mean (or average) for each of the three materials using numpy's np.mean() function. The means will be the height of each bar in our chart. # Calculate the average Aluminum_mean = np . mean ( aluminum ) Copper_mean = np . mean ( copper ) Steel_mean = np . mean ( steel ) Next we'll calculate the standard deviation for each metal using numpy's np.std() function. On the plot, we will use the standard deviation as the height of our error bars. The positive error will be show as +1 standard deviation above the mean and the negative error will be shown as -1 standard deviation below the mean. aluminum_std = np . std ( aluminum ) copper_std = np . std ( copper ) steel_std = np . std ( steel ) There are a couple more things needed to build the plot. We need the names of the metals to go along our x-axis, one name below each bar. We'll assign names as list of strings in a variable called materials . We also need a variable that contains the means of the coefficients of thermal expansion, the data we are going to plot. We'll put these into a python list called CTEs . Our standard deviations will be used for the height of the error bars. Those will go together in a list called error . Let's code all of these list into our errorbars.py script. # Create Arrays for the plot materials = [ 'Aluminum' , 'Copper' , 'Steel' ] x_pos = np . arange ( len ( materials )) CTEs = [ aluminum_mean , copper_mean , steel_mean ] error = [ aluminum_std , copper_std , steel_std ] Now it's time to build the plot. We are going to build a bar chart with three different bars, one bar for each material: Aluminum, Copper and Steel. First we will create a figure object called fig and an axis object in that figure called ax using matplotlib's plt.subplots() function. Everything in our plot will be added to the ax (axis) object. Next we put a bar chart on our ax (axis) with the ax.bar() method. Note the arguments that go into this method: (x_pos, CTEs, yerr=error) . x_pos is the array with the count of the number of bars. CTEs is our array which contains the means or heights of the bars. yerr=error sets the heights of the error bars and the standard deviations. The subsequent arguments (align='center', alpha=0.5, ecolor='black', capsize=10) styles the plot. We'll put a label on the y-axis with the title \"Coefficient of thermal expansion (C -1 )\" using ax.set_ylabel . We use ax.set_xticks() to feed in our number array to set the bars as numbers 1, 2, 3. Then we add labels to these numbered bars with ax.set_ticklabels() . ax.set_title() and ax.yaxis.grid(True) adds a title and horizontal grid lines. Finally, we we'll save the figure to a file called bar_plot_with_error_bars.png using matplotlib's plt.savefig() function. The plt.thight_layout() line ensures that the labels for our bars and axis don't get cut off and are visible. # Build the plot fig , ax = plt . subplots () ax . bar ( x_pos , CTEs , yerr = error , align = 'center' , alpha = 0.5 , ecolor = 'black' , capsize = 10 ) ax . set_ylabel ( 'Coefficient of Thermal Expansion ($\\degree C&#94;{-1}$)' ) ax . set_xticks ( x_pos ) ax . set_xticklabels ( materials ) ax . set_title ( 'Coefficent of Thermal Expansion (CTE) of Three Metals' ) ax . yaxis . grid ( True ) # Save the figure and show plt . tight_layout () plt . savefig ( 'bar_plot_with_error_bars.png' ) plt . show () The final plot looks like this:","tags":"matplotlib","url":"python-matplotlib-error-bars.html"},{"title":"Unicode characters for engineers in Python","text":"Unicode characters are very useful for engineers. A couple commonly used symbols in engineers include Omega and Delta. We can print these in python using unicode characters. From the Python interpreter we can type: >>> print('Omega: \\u03A9') Omega:  >>> print('Delta: \\u0394') Delta:  >>> print('sigma: \\u03C3') sigma:  >>> print('mu: \\u03BC') mu:  >>> print('epsilon: \\u03B5') epsilon:  >>> print('degree: \\u00B0') degree:  >>> print('6i\\u0302 + 4j\\u0302-2k\\u0302') 6i + 4j-2k All of these are unicode characters. Python has support for unicode characters built in. You can check if your system supports it by importing the sys module and calling the sys.getdefaultencoding() function >>> import sys >>> sys . getdefaulencoding () 'utf-8' If you see utf-8 , then your system supports unicode characters. To print any character in the Python interpreter, use a \\u to denote a unicode character and then follow with the character code. For instance, the code for  is 03B2, so to print  the command is print('\\u03B2') . There are a couple of special characters that will combine symbols. A useful one in engineering is the hat &#94; symbol. This is typically used to denote unit vectors. We can add a hat &#94; (also called a circumflex) by putting the unicode escape after the letter you want to add a hat to. For example to add a hat to i the command is print('i\\u0302') . Below is a list of symbols and greek letters and the corresponding unicode escape to produce the character in python. Useful unicode symbols in engineering unicode character description \\u0394  GREEK CAPITAL LETTER DELTA \\u03A9  GREEK CAPITAL LETTER OMEGA \\u03C0  GREEK SMALL LETTER PI \\u03F4  GREEK CAPITAL THETA SYMBOL \\u03BB  GREEK SMALL LETTER LAMDA \\u03B8  GREEK SMALL LETTER THETA \\u03B1  DEGREE SYMBOL i\\u0302 i i HAT j\\u0302 j j HAT k\\u0302 k k HAT u\\u0302 u u HAT Greek lower case letters unicode character description \\u03B1  GREEK SMALL LETTER ALPHA \\u03B2  GREEK SMALL LETTER BETA \\u03B3  GREEK SMALL LETTER GAMMA \\u03B4  GREEK SMALL LETTER DELTA \\u03B5  GREEK SMALL LETTER EPSILON \\u03B6  GREEK SMALL LETTER ZETA \\u03B7  GREEK SMALL LETTER ETA \\u03B8  GREEK SMALL LETTER THETA \\u03B9  GREEK SMALL LETTER IOTA \\u03BA  GREEK SMALL LETTER KAPPA \\u03BB  GREEK SMALL LETTER LAMDA \\u03BC  GREEK SMALL LETTER MU \\u03BD  GREEK SMALL LETTER NU \\u03BE  GREEK SMALL LETTER XI \\u03BF  GREEK SMALL LETTER OMICRON \\u03C0  GREEK SMALL LETTER PI \\u03C1  GREEK SMALL LETTER RHO \\u03C2  GREEK SMALL LETTER FINAL SIGMA \\u03C3  GREEK SMALL LETTER SIGMA \\u03C4  GREEK SMALL LETTER TAU \\u03C5  GREEK SMALL LETTER UPSILON \\u03C6  GREEK SMALL LETTER PHI \\u03C7  GREEK SMALL LETTER CHI \\u03C8  GREEK SMALL LETTER PSI \\u03C9  GREEK SMALL LETTER OMEGA Greek upper case letters unicode character description \\u0391  GREEK CAPITAL LETTER ALPHA \\u0392  GREEK CAPITAL LETTER BETA \\u0393  GREEK CAPITAL LETTER GAMMA \\u0394  GREEK CAPITAL LETTER DELTA \\u0395  GREEK CAPITAL LETTER EPSILON \\u0396  GREEK CAPITAL LETTER ZETA \\u0397  GREEK CAPITAL LETTER ETA \\u0398  GREEK CAPITAL LETTER THETA \\u0399  GREEK CAPITAL LETTER IOTA \\u039A  GREEK CAPITAL LETTER KAPPA \\u039B  GREEK CAPITAL LETTER LAMDA \\u039C  GREEK CAPITAL LETTER MU \\u039D  GREEK CAPITAL LETTER NU \\u039E  GREEK CAPITAL LETTER XI \\u039F  GREEK CAPITAL LETTER OMICRON \\u03A0  GREEK CAPITAL LETTER PI \\u03A1  GREEK CAPITAL LETTER RHO \\u03A3  GREEK CAPITAL LETTER SIGMA \\u03A4  GREEK CAPITAL LETTER TAU \\u03A5  GREEK CAPITAL LETTER UPSILON \\u03A6  GREEK CAPITAL LETTER PHI \\u03A7  GREEK CAPITAL LETTER CHI \\u03A8  GREEK CAPITAL LETTER PSI \\u03A9  GREEK CAPITAL LETTER OMEGA \\u03F4  GREEK CAPITAL THETA SYMBOL","tags":"python","url":"unicode-characters-in-python.html"},{"title":"Python Virtual Environments in OS X, Linux and Windows 10","text":"In this post, I'll review creating virtual environments on three different operating systems: Windows 10, Linux and Mac OSX. Using virtual environments is good programming practice when using Python. A virtual environment will separate the Python interpreter and installed modules from the main Python installation. I use three different operating systems on three different computers: Work: Windows 10 (no admin access) Home Office: Ubuntu 16.04 LTS Laptop: Mac OSX Setting up a Python virtual environment is different on each one of these operating systems. Let's see at what happens when we try and create a new virtual environment in each. Ubuntu 16.04 Linux should be the easiest to get a new virtualenv up and running. In Ubuntu 16.04, I have a terminal and admin access (can use sudo). But look what happens when I try to set up a new virtualenv without any flags or customization: $ mkvirtualenv webscrape $ source activate webscrape ( webscrape ) $ which python $ 2 .7.1 The default installation is legacy Python!? I don't want the legacy 2.7 version, I want at least Python 3.2 and would prefer Python 3.6. Let's delete that legacy Python environment. Make sure the virtual environment is deactivate d first. ( webscrape ) $ source deactivate $ rmvirtualenv webscrape Let's try to specify Python 3 with the -p python3 flag $ mkvirtualenv -p python3 webscrape $ error Now what? Path is too long? How is that possible? What happens when we see which Python version is the default python3? $ which python3 $ bin/usr/anaconda/python3 So that's the flag we need to use when the viruatlenv is created. The --python='which python3' flag will point virtualenvwrapper to the correct Python version. The new virtualenv is initiated with the full file path to our new environment ~/.virtualenvs/webscrape $ mkvirtualenv --python = 'which python3' ~/.virtualenvs/webscrape ( webscrape ) $ python Python 3 .6.2 Nice. Now we can pip install away. So what about creating a new virtual environment on a MacBook Air with OSX? Mac OSX Mac OSX has a terminal too. I get to it by going to the finder and clicking the search in the upper right or using [command] + [space] to bring up the spotlight search bar. Type terminal into the search bar. Setting up a virtualenv should be pretty easy right? $ mkvirtualenv webscrape $ source activate webscrape ( webscrape ) $ which python python 2 .7 Again!? More legacy Python?! Stop it already with the legacy Python. We want Python! Preferably 3.6. Gotta make sure that Python 3 is installed some where. rmvirtualenv that thing. ( webscrape ) $ source deactivate $ rmvirtualenv webscrape $ which python3 $ usr/bin/python $ virtualenv -p python3 webscrape $ source activate webscrape ( webscrape ) $ python python 3 .6.2 OK. Two down and one to go. Is this any easier on Windows 10? Especially with no admin access? Can't be right? Let's see. Windows 10 (no admin access) I have a Windows 10 machine at work with no admin access. I can't install any programs on my work computer that use the Windows active directory (which is most programs). The Python distribution that has worked out the best has been Anaconda . Besides coming with Python 3, and having a bunch of packages already installed, it also comes with a command line client call the Anaconda Prompt . The Anaconda Prompt operates a little like the terminal on Linux and Mac OSX, but some of the commands are a little different. To make a new virtual environment from the Anaconda Prompt type: $ conda create -n webscrape python = 3 .6 $ proceed ([ y ] /n ) ? $ conda activate webscrape ( webscrape ) $ python --version python 3 .6.3 :: Anaconda, Inc. Rockin' right? Turns out that the Windows 10 virtual environment was one of the easiest to set up. Who would have guessed that? That's one piece of magic from the Anaconda distribution. If you are using Windows, I think Anaconda is the way to go. End Note: Maybe it's best to use conda on both OSX and Linux too... You don't have to mess around with ~/.bash_profile or pointing virtualenv wrapper to the proper directory.","tags":"Orientation","url":"virtualenv-in-osx-linux-windows.html"},{"title":"How I Build This Site - Part 7","text":"This is the seventh part of a multi-part series on how I built this site. In last post , we added two new pages to our site. An About page and a Book page. In this post, we are going to deploy the site to github pages . Github pages is a place on github were documentation and static sites can be hosted. Steps in this post By the end of the post we are going to have a working static website hosted on github pages. To accomplish this, we will complete the following steps: Pull the most recent version of the site from gitub Run pelican's make html command to build the site and preview it with make serve Modify the publishconf.py file to include our github pages url and relative links Use pelican content -s publishconf.py to build a published version of the site in the output directory Add, commit and push the published contents to our master branch on github Make a gh-pages branch in our staticsite repo on github Use ghp-import output and git push origin gh-pages to push the output directory to the gh-pages branch View the freshly published site! OK, let's get started. Can't wait to see the published site live on github pages. Pull to the most recent version of the site from github Open a terminal and cd to the staticsite directory. Then activate the (staticsite) virtual environment with source activate staticsite . Once in the (staticsite) environment, pull the most recent version of the site down from github with git pull origin master . cd ~/Documents/staticsite source activate staticsite ( staticsite ) $ git pull origin master The staticsite directory should look something like this: staticsite  LICENSE  Makefile  README.md  __pycache__  _nb_header.html  content  develop_server.sh  fabfile.py  output  pelican-plugins  pelican-themes  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Now we use the make html command to build a demo version of the site. This will place the static files ( html , css , javascript ) that forms the site in the output folder. We preview the site with make serve . ( staticsite ) $ make html ( staticsite ) $ make serve The demo version of our site can now be viewed by pointing a browser to: localhost:8000 Press ctr-c to stop the server. Modify the publishconf.py file to use the github pages url We need to edit the publishconf.py file to add our github pages url to SITEURL and set RELATIVE_URLS to True. The lines to change are: #publishconf.py SITEURL = 'https://username.github.io/staticsite' RELATIVE_URLS = True Make sure to set username to your github user name. Setting RELATIVE_URLS = True is necessary for the links on the site to work and for the css and javascript files run on github pages. When I initially set RELATIVE_URLS = False , the site looked terrible, had no formatting or css and none of the links worked. Setting RELATIVE_URLS = True fixed the problem. Create a published version of the site Up to this point, we used the make html command to build a demo version of the site. Now we are ready to publish the site. We publish the site by running the command: pelican content -s publishconf.py This creates a published version of the site with relative url's in the output directory. Add, commit, push to the master branch on github Before we can put the published version of the site up on github pages, we need to push the current version of the site up to the master branch. ( staticsite ) $ git add . ( staticsite ) $ git commit -m \"first published version\" ( staticsite ) $ git push origin master Create a gh-pages branch in our staticsite repo on github Up to this point, we saved our work to the master branch of the staticsite repository on github. To host the site on github pages, we need to create a new branch in the staticsite repo called gh-pages . The master branch still houses the code,settings, markup files, notebooks, images, etc. to build the site. However, in the gh-pages branch of the staticsite repo any html, css and javascript files will be served like a regular website. To create the new branch, go the main staticsite repository page on github and click the [Branch: Master] drop down menu on the upper left hand side. Enter the name of the new branch: gh-pages Use ghp-import to post the contents of the output directory to the gh-pages branch As shown in the Pelican documentation , you can use a Python package called ghp-import to help posting the contents of the output directory to the gh-pages branch of our repo on github. If ghp-import isn't installed yet, use pip . Make sure you are in the (staticsite) virtual environment when you run pip . ( staticsite ) $ pip install ghp-import Now we'll use the ghp-import package to help us post the site. The command ghp-import output will assign the contents of the output directory to the gh-pages branch of our local git repository. The we push the contents of the local gh-pages branch up to the remote gh-pages branch on github. ( staticsite ) $ ghp-import output ( staticsite ) $ git push origin gh-pages I had trouble with this set of commands. Depending on which computer I was using, I would get the following error: ! [ rejected ] gh-pages -> gh-pages ( fetch first ) error: failed to push some refs to 'https://github.com/professorkazarinoff/staticsite.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: ( e.g., 'git pull ...' ) before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. I tried git stash and that didn't work. I also tried git pull origin gh-pages but this ended up putting everything from the output directory into my root staticsite directory which made a big old mess. The way I got around it was to use the -f (force) flag. I don't think this is the most elegant or preferred way to get the contents of the output directory up to the gh-pages branch. I just don't really understand how git works well enough to know how to get around the problem without a forced commit. If you get the error above try: pelican content - s publishconf . py git add . git commit - m \"published\" git push origin master ghp - import output git push - f origin gh - pages That has worked so far for me. View the site on github pages. Awesome! The site is now hosted for everyone to see on github pages! Pretty cool right? Point a browser to the github pages url and take a look: https://username.github.io/staticsite Change username to your github user name. My site (the one that you are reading) is hosted here: https://professorkazarinoff.github.io/staticsite Thanks for reading to the end! It was quite a bit of work to get the site up and running, but I am pleased with the results. Now I need to read up on git . . .","tags":"This site","url":"how-i-built-this-site-7.html"},{"title":"How I Build This Site - Part 6","text":"This is the sixth part in a multi-part series on how I built this site. In the last post , we put a search bar at the top right of each page and some css and javascript in order to make tables on the site look better. In this post we are going to add two new pages to our static site: an About page and a Book page. These new pages will have menu entries at the top of our site. Steps in this post We are going to accomplish the following in this post. By the end of the post we are going to have a site with two new pages and two new menu items. Activate our staticsite virtual environment Pull the most recent version of our site from github Add two new pages ( .md files) to our content folder Modify the pelicanconf.py file to use the new pages Build and preview the site with Pelican Add, commit and push the changes to github Let's get started. Activate our virtual environment and pull the most recent version of the site down from github $ cd ~/Documents/staticsite $ source activate staticsite ( staticsite ) $ git pull origin master The staticsite directory should look something like this: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   posts   first_post.md   second_post.md   third_post.md   fourth_post.md   code   sample_notebook.ipynb   extra   custom.css   custom.js  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb   tipue_search  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Create two new pages ( .md files) Up to this point, the top of our site has a menu item for [This site], which isn't very useful. We are going to add two new menu items that link to two new pages . These new menu items will be [About] and [Book]. First we'll create a pages folder in the staticsite/content directory (staticsite) $ cd ~/Documents/staticsite/content (staticsite) $ mkdir pages (staticsite) $ cd pages (staticsite) $ pwd Now we create two new .md files. Note that the header in this file is different from the other posts we've written. It has a very simple header; just one line: Title. We don't need to put in a date or a slug in the header of pages , just in the header of posts . The first markdown file is the about page about.md Title : About Why I started this blog My background My family My hobbies The second page is for the book I am writing, Python Programming for Undergraduate Engineers . The text for the page is going to look something like: book.md Title : Book ### Coming in Fall 2018 Book : ** Python Programming for Undergraduate Engineers ** This books is for undergraduate engineers learning programming . It is focused on using Python and programming to solve engineering problems . ### Chapter List * Preface * Chapter 1 : Orientation * Chapter 2 : Engineering 101 * Chapter 3 : Statics * Chapter 4 : Circuits I * Chapter 5 : Strengths * Chapter 6 : Circuits II * Chapter 7 : Dynamics * Chapter 8 : Circuits III * Chapter 9 : Capstone * Appendix ### Pre - order will be available soon After the pages are saved, our staticsite directory should look something like this: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   pages   about.md   book.md   posts   first_post.md   second_post.md   third_post.md   fourth_post.md   code   sample_notebook.ipynb   extra   custom.css   custom.js  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb   tipue_search  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Modify pelicanconf.py to include the two new pages. Pelican needs to know about the two \"pages\" files. Modify the pelicanconf.py file to include the lines: #pelicanconf.py # Paths PATH = 'content' PAGE_PATHS = ['pages'] ARTICLE_PATHS = ['posts'] # Top menus DISPLAY_CATEGORIES_ON_MENU = False DISPLAY_PAGES_ON_MENU = True Build and preview the site with Pelican With the two new page .md files created and the changes to pelicanconf.py in place, let's preview the site again. We build the site and serve up the contents in the output folder with: make html make serve To view the site, point a browser to localhost:8000 localhost:8000 Use ctrl-c to stop the server. Add and commit the changes then push them to github When we are done editing the the site, we add all of the changes to our local git repo using git add . . Commit those changes with git commit and add the -m \"added about and book pages\" flag to supply a commit message (make sure to use double quotes \"commit message\"). Push the changes up to github with git push origin master git add . git commit -m \"added about and book pages\" git push origin master In the next post we will publish the site to github pages. Once the site is published, it will be live and public. Available to any one with an internet connection. An actual, real, published, live static site!","tags":"This site","url":"how-i-built-this-site-6.html"},{"title":"How I Build This Site - Part 5","text":"This is the fifth part in a multi-part series on how I built this site. In the last post , we installed a couple of plugins to add extra functionality to the site. These plugins enabled embedded jupyter notebooks and posts in a series. In this post we'll put a search bar at the top right of each page and add some css and javascript in order to make tables on the site look better. Steps in this post We are going to accomplish the following in this post. By the end of the post we are going to have a site with a working search bar and nice looking tables. Activate our staticsite virtual environment Pull the most recent version of our site from github Modify the pelicanconf.py file to use the 'tipue_search' plugin Add custom css and javascript to make tables look better Build and preview the site with Pelican Add, commit and push the changes to github Let's get started. Activate our virtual environment and pull the most recent version of the site down from github $ cd ~/Documents/staticsite $ source activate staticsite ( staticsite ) $ git pull origin master The staticsite directory should look something like this: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   posts   first_post.md   second_post.md   third_post.md   code   sample_notebook.ipynb  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Add the 'tipue_search' plugin to the pelicanconf.py file Now we need to modify the pelicanconf.py file to use the 'tipue_search' plugin. This plugin will give us the ability to add a search bar to our site menu at the top right of each page. Add 'tipue_search' to the PLUGINS = [ ] list in the pelicanconf.py file. Make sure each plugin is separated with commas and surrounded by quotes . #pelicanconf.py PLUGINS = [ 'i18n_subsites', 'series', 'tag_cloud', 'liquid_tags.youtube', 'liquid_tags.notebook', 'liquid_tags.include_code', 'render_math', 'pelican-ipynb.markup', 'tipue_search' ] To use the 'tipue_search' plugin, we also need to add the following line to the pelicanconf.py file: #pelicanconf.py # for Tique Search Plugin DIRECT_TEMPLATES = ('index','tags', 'categories', 'authors', 'archives', 'search') Add some custom css and javascript to make tables look good. Even with a great theme like pelican-bootstrap3, there are some changes to make to the look of the site. One of these changes is to make tables look better, like the tables on github readme pages look. Let's make a new post in the content/posts directory. This post will contain a markdown table using the | (pipe) character and a header row with pipes separated by three dashes --- . fourth_post.md Title : Fourth Post - Part 4 Date : 2017 - 11 - 30 12 : 40 Modified : 2017 - 11 - 30 12 : 40 Status : published Category : example posts Tags : python , pelican , blog , tables Slug : fourth - post Authors : Peter D . Kazarinoff Series : example - post - series Series_index : 4 Summary : This is the fourth post of a series of posts . It will demonstrate tables . This is the fourth post of a series of posts . It will demonstrate tables . | Column Header | Column Header | | --- | ---| | Row 1 | Data 1 | | Row 2 | Data 2 | After the post is saved, our staticsite directory should look something like this: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   posts   first_post.md   second_post.md   third_post.md   fourth_post.md   code   sample_notebook.ipynb  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb   tipue_search  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid To change the way that tables are rendered, we will add some custom css and javascript that is not included with the pelican-bootstrap3 theme. First create a new folder in the staticsite/content directory called extra . cd ~/Dcouments/staticsite/content mkdir extra && cd extra Inside the extra folder, create a new .css file called custom.css . Insert the following style changes in custom.css : . table { width : inherit ; max-width : 100 % ; margin-bottom : 21 px ; padding : 6 px 13 px ; } Now create a new javascript file in the content/extra directory called custom.js . This file contains extra javascript that will be injected into pages when the .html is generated by Pelican. var tables, i; tables = document.getElementsByTagName('table'); for (i=0;i<tables.length;i++) { tables[i].className = 'table table-bordered table-hover table-striped table-responsive'; } With the addition of these two new files, the contents of the staticsite directory will look something like: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   posts   first_post.md   second_post.md   third_post.md   fourth_post.md   code   sample_notebook.ipynb   extra   custom.css   custom.js  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb   tipue_search  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Pelican needs to know about the two new \"custom\" files. Modify the pelicanconf.py file to include the lines: #pelicanconf.py CUSTOM_CSS = 'static/css/custom.css' CUSTOM_JS = 'static/js/custom.js' STATIC_PATHS = [ 'extra' ] EXTRA_PATH_METADATA = { 'extra/custom.css': {'path': 'static/css/custom.css'}, 'extra/custom.js': {'path': 'static/js/custom.js'} } When the site is built, Pelican will read in custom.css and custom.js siting in the extra folder. Pelican will then copy these two files in the appropriate places in the output directory (static/css/custom.css and static/css/custom.js) for the theme to use. Then the code from the css and javascript files will be used by the .html pages in the output directory along with the other css and javascript from the bootstrap3 theme. This will make tables look more like tables in github readme pages. Build and preview the site with Pelican With the search plugin configured, a new posts containing a table written, plus our custom.js and custom.css in place, let's preview the site again. We build the site and serve up the contents in the output folder with: make html make serve To view the site, point a browser to localhost:8000 localhost:8000 use ctrl-c to stop the server. Add and commit the changes then push them to github When we are done editing the the site, we add all of the changes to our local git repo using git add . . Commit those changes with git commit and add the -m \"added search and tables\" flag to supply a commit message (make sure to use double quotes \"commit message\"). Push the changes up to github with git push origin master git add . git commit -m \"added search and tables\" git push origin master In the next post we will add two new menu items across the top of our site. These new menu items will link to an [About] page and a [Book] page.","tags":"This site","url":"how-i-built-this-site-5.html"},{"title":"How I Build This Site - Part 4","text":"This is the fourth part in a multi-part series on how I built this site. In the last post , we installed the pelican-bootstrap3 theme and made our site mobile responsive. Now the site looks good on all devices. In this post we are going to install a couple of plugins to add extra functionality to our site. These plugins will allow our site to have a series of post that are linked together, create a working search bar, add youtube videos to posts, view LaTeX math and add embedded jupyter notebooks in posts. Steps in this post We are going to accomplish the following in this post. By the end of the post we are going to have a great looking website that contains a two-part series, a post with an embedded youtube video and a post that contains an embedded jupyter notebook. Activate our staticsite virtual environment Pull the most recent version of our site from github Install jupyter in our staticsite environment Modify the pelicanconf.py file to use new plugins Build some posts that will allow us to view the new plugins Build and preview the site with Pelican Add and commit the changes then push those changes to github Seems like a lot to do, so let's get started. Activate our virtual environment and pull the most recent version of the site down from github $ source activate staticsite ( staticsite ) $ cd ~/Documents/staticsite ( staticsite ) $ pwd ( staticsite ) $ git pull origin master Install the jupyter package in our staticsite virtual environment. I like using jupyter notebooks to build code and solve engineering problems with Python. A jupyter notebook can contain Python code, the output produced when this code is run and markup text (used for documentation). Jupyter notebooks can also easily display matplotlib plots and pandas data frames. These two Python packages are very useful for engineers solving problems in teams. As the jupyter package is in the main conda channel, we can install it into our virtual environment using the conda install command. (staticsite) $ conda install jupyter We can see all of the modules installed in our (staticsite) environment with: (staticsite) $ pip freeze The output should look something like: appnope==0.1.0 bleach==2.1.1 blinker==1.4 certifi==2017.11.5 decorator==4.1.2 docutils==0.14 entrypoints==0.2.3 feedgenerator==1.9 html5lib==0.999999999 ipykernel==4.6.1 ipython==6.2.1 ipython-genutils==0.2.0 ipywidgets==7.0.5 jedi==0.11.0 Jinja2==2.10 jsonschema==2.6.0 jupyter-client==5.1.0 jupyter-console==5.2.0 jupyter-core==4.4.0 Markdown==2.6.9 MarkupSafe==1.0 mistune==0.8.1 nbconvert==5.3.1 nbformat==4.4.0 notebook==5.2.2 pandocfilters==1.4.2 parso==0.1.0 pelican==3.7.1 pexpect==4.3.0 pickleshare==0.7.4 prompt-toolkit==1.0.15 ptyprocess==0.5.2 Pygments==2.2.0 python-dateutil==2.6.1 pytz==2017.3 pyzmq==16.0.3 qtconsole==4.3.1 simplegeneric==0.8.1 six==1.11.0 terminado==0.6 testpath==0.3.1 tornado==4.5.2 traitlets==4.3.2 Unidecode==0.4.21 wcwidth==0.1.7 webencodings==0.5.1 widgetsnbextension==3.0.8 Add new plugins to the pelicanconf.py file Now we need to modify the pelicanconf.py file to use a couple new plugins. The plugins we will add are: 'series' 'tag_cloud', 'liquid_tags.youtube' 'liquid_tags.notebook', 'liquid_tags.include_code', 'render_math' 'pelican-ipynb.markup' We add these to the PLUGINS = [ ] list in the pelicanconf.py file and separate them with commas. #pelicanconf.py PLUGINS = [ 'i18n_subsites', 'series', 'tag_cloud', 'liquid_tags.youtube', 'liquid_tags.notebook', 'liquid_tags.include_code', 'render_math', 'pelican-ipynb.markup' ] Build some posts that use the newly installed plugins Now we will build a couple of posts which use our newly installed plugins. First we'll modify our content directory with a new folder called posts . We'll keep all the posts in this directory. Then we'll make a couple of new .md files. I'll copy our first post to this new directory with the cp (copy) shell command and then remove the old .md file with the rm command. pwd cd content mkdir posts cp first_post.md posts/first_post.md rm first_post.md Now let's modify the first_post.md file with a couple new lines in the header. The Series: and Series_index: lines will put this first post in series of posts using the series plugin. first_post.md Title : First Post - Part 1 Date : 2017 - 11 - 30 12 : 40 Modified : 2017 - 11 - 30 12 : 40 Status : published Category : example posts Tags : python , pelican , blog Slug : first - post Authors : Peter D . Kazarinoff Series : example - post - series Series_index : 1 Summary : This is the first post of a series of demonstration posts . This is the first post of a series of demonstration posts . After the post is saved, we can go back to the terminal and copy it to create our second post. The contents of our staticsite directory should look something like this: staticsite/  LICENSE  Makefile  README.md  __pycache__  content   posts   first_post.md  develop_server.sh  fabfile.py  output  pelican-plugins   i18n_subsites   liquid_tags   pelican-ipynb   tipue_search  pelican-themes   pelican-bootstrap3  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Let's make two new posts in the content/posts directory. cd ~/Documents/staticsite/content/posts cp first_post.md second_post.md cp first_post.md third_post.md Now we'll edit second_post.md so it's part of a series and contains an embedded YouTube video. second_post.md Title : Second Post - Part 2 Date : 2017 - 11 - 30 12 : 40 Modified : 2017 - 11 - 30 12 : 40 Status : published Category : example posts Tags : python , pelican , blog Slug : second - post Authors : Peter D . Kazarinoff Series : example - post - series Series_index : 2 Summary : This is the second post of a series of posts . It will show series and an embeded youtube video . This is the second post of a series of posts . It will show series and an embedded youtube video . {% youtube https :// www . youtube . com / watch ? v = Qq - 5 frjUfK0 [ 560 ] [ 315 ] %} We'll create a sample jupyter notebook in a new folder called code in our content folder. The jupyter notebook will go in our third post. To create the code folder and bring up a new jupyter notebook in our web browser use: (staticsite) $ pwd (staticsite) $ mkdir content/code (staticsite) $ cd content/code (staticsite) $ jupyter notebook Let's put one markdown cell, one code cell and produce one output cell in our jupyter notebook . We'll save it as example_notebook.ipynb . My sample_notebook.ipynb looks like: Now we will put our example_notebook.ipynb into our third post. Including a juypter notebook requires the use of the line notebook path/to/notebook.ipynb surrounded with a { % % } . third_post.md Title : Third Post - Part 3 Date : 2017 - 11 - 30 12 : 40 Modified : 2017 - 11 - 30 12 : 40 Status : published Category : example posts Tags : python , pelican , blog Slug : third - post Authors : Peter D . Kazarinoff Series : example - post - series Series_index : 3 Summary : This is the third post of a series of posts . It will show series an jupyter notebook . This post contains a jupyter notebook . {% notebook ../ code / sample_notebook . ipynb %} Build and preview the site with Pelican With a couple new pelican-plugins configured and three new posts written, it is time to preview our site again. We can build the site and can view it with a web browser using: make html make serve To view the site, point a browser to localhost:8000 localhost:8000 use ctrl-c to stop the server. Add and commit the changes then push them to github When we are done editing the the site, we add all of the changes to our local git repo using git add . . Then we commit those changes with git commit and add the -m \"added plugins\" flag to give supply a commit message (make sure to use double quotes \"commit message\"). To push those changes up to github use git push origin master git add . git commit -m \"added plugins\" git push origin master In the next post we will customize the site. We'll add some css in order to make tables on the site look better and put a search bar at the top of the page.","tags":"This site","url":"how-i-built-this-site-4.html"},{"title":"How I Build This Site - Part 3","text":"This is the third part in a multi-part series on how I built this site. In the last post , we used pelican-quickstart to build the framework of the site and wrote a short first post, then viewed a demo version of the site on localhost:8000. In this post we will add a custom theme to the site called pelican-bootstrap3. The pelican-bootstrap3 theme looks great and is mobile reponsive so it will make the site look good on phones and tablets. Then we'll add some custom css to personalize the look of the theme. Steps in this post We are going to accomplish the following in this post. By the end of the post we are going to have a a static site with a great looking bootstrap3 theme which is mobile responsive and looks great on desktops, tablets and phones. Activate our staticsite virtual environment Pull the most recent version of our site from github Add a git submodule to our staticsite folder and bring in the pelican-themes repo from github Add a git submodule to our staticsite folder and bring in the pelican-plugins repo from github Modify the pelicanconf.py file to point to our new theme, and add a new plugin Build and preview the site with Pelican Add and commit the changes then push those changes to github Big steps, it's really going to look like a website when we are done. Activate our virtual environment and pull from github Open the Anaconda Prompt and activate the (staticsite) virtual environment $ source activate staticsite Then cd into the staticsite directory and bring in the most up to date version of the site stored on github. (staticsite) $ cd ~ (staticsite) $ cd Documents/staticsite (staticsite) $ git pull origin master Use git submodule add to pull all of the themes into our local staticsite/pelican-themes directory There are a bunch of different themes available for static site built with Pelican. The three I was most interested in were: material voidy_bootstrap pelican-bootstrap3 We can bring in all of the Pelican themes stored on github by creating a git submodule . A git submodule is a sub-repository within a git repository that is linked to another repository. It is a way to bring in something else from github within a local repository and not have to keep a local copy up to date. Each time we \"pull\" from the submodule, we get the newest version of the pelican-themes repo on github. We don't have to manually track any changes to these themes and incorporate them to our local version. When the changes are made to the themes on github, we just pull those changes down to our local version. The lines git submodule init and git submodule update --init --recursive are important to call. If those two commands are not moved we'll end up with empty folders within the staticsite/pelican-themes directory. A lot of the themes are submodules themselves so the --recursive option has to be used to pull all of these submodules down. Without --recursive you can end up with empty folders. To create the folder for our pelican-themes git submodule, ensure you are in the staticsite folder, then call: $ pwd $ git submodule add https://github.com/getpelican/pelican-themes.git $ git submodule init $ git submodule update --init --recursive Ensure that we are still in the staticsite directory and pull down the pelican-plugins repo form github. $ pwd $ git submodule add https://github.com/getpelican/pelican-plugins.git $ git submodule init $ git submodule update --init --recursive Now the contents of the staticsite folder should look something like: staticsite/  LICENSE  Makefile  README.md  __pycache__  content  develop_server.sh  fabfile.py  output  pelican-plugins  pelican-themes  pelican.pid  pelicanconf.py  publishconf.py  srv.pid Modify the pelicanconf.py file, so that we can use a new theme. So far our pelicanconf.py file contains only default lines of code that Pelican built for us. In order to use a new theme, we need to edit this configuration file. After we edit the configuration file, the make html command will produce new .html pages in the output directory. The relevant lines to add and modify in the pelicanconf.py are: #pelicanconf.py PLUGIN_PATHS = ['pelican-plugins'] THEME = 'pelican-themes/pelican-bootstrap3' BOOTSTRAP_THEME = 'flatly' PLUGIN_PATHS = ['/path/to/git/pelican-plugins'] JINJA_ENVIRONMENT = {'extensions': ['jinja2.ext.i18n']} PLUGINS = [ 'i18n_subsites'] I18N_TEMPLATES_LANG = 'en' Taken directly from the pelicin-bootstrap3 README.md ... this template can be translated (see Translations below). You also need to activate a plugin that initializes the i18n jinja extension. One possibility is an up to date version of the i18n_subsites plugin. So we need to make sure to include the i18n plugin in our pelicanonf.py file. Again from the from the pelicin-bootstrap3 README.md If you are using i18n_subsites and you are not using English as your default language, make sure to also correctly specify the default language of the theme. Otherwise the translations will not be used on your default site. Build and preview the site with Pelican With the pelican-plugins and pelican-themes repos downloaded and the pelicanconf.py file edited, we can build the site and take a look at it with a web browser: make html make serve To view the site, point a brower to localhost:8000 localhost:8000 Now, the site should look something like this: use ctrl-c to shut down the server. Add and commit the changes then push those changes to github When we are done editing the the site, we add all of the changes to our local git repo using git add . . Then we commit those changes with git commit and add the -m \"added pelican_bootstrap3 theme\" flag to give supply a commit message (make sure to use double quotes \"commit message\"). To push those changes up to github use git push origin master git add . git commit -m \"added pelican_bootstrap3 theme\" git push origin master In the next post we will add some additional pelican-plugins to bring extra functionality to the site and add the ability to include jupyter notebooks in posts.","tags":"This site","url":"how-i-built-this-site-3.html"},{"title":"How I Build This Site - Part 2","text":"This is the second part in a multi-part series on how I built this site. In the last post , we set up our development environment including Python, a new virtual environment, installed the pelican and markdown packages and set up git. In this post, we will use the pelican-quickstart command to get the blog off the ground. We are also going to create a first_post.md file and serve up the website locally so we can take a look at it. Steps in this post We are going to accomplish the following in this post. By the end of the post, we will have a working website with one blog post served up locally and an updated github repo with all the changes saved. Activate our staticsite virtual environment Pull the most recent version of our site from github Use the pelican-quickstart command to make a first version of the site Write a first post in markup language (.md) Build the site using the make html command Serve the site locally using make serve and view with a web browser Add and commit the changes then push those changes to github That's a lot to do, so let's get started. Activate our staticsite virtual environment I highly recommend installing the Anaconda distribution of python. If you followed along with the previous post , you already installed Anaconda and can pull up the Anaconda prompt. Open the Anaconda Prompt and see which virtual environments are available. $ conda info --envs You should see a list of all the virtual environments conda has created on your machine. It should look something like: staticsite C:\\Users\\user.name\\AppData\\Local\\Continuum\\Anaconda3\\envs\\staticsite root * C:\\Users\\user.name\\AppData\\Local\\Continuum\\Anaconda3 The staticsite virtual environment is the one we set up to create our site. Activate it with: $ source activate staticsite You should now see (staticsite) before the command prompt. This means we are operating in the staticsite virtual environment. View installed packages We installed pelican, markdown and fabric in the last post. Let's make sure they are installed in our (staticsite) virtual environment. (staticsite)$ pip freeze Make sure you see the following modules are installed: beautifulsoup4==4.6.0 Jinja2==2.9.6 Fabric==1.14.0 Markdown==2.6.9 pelican==3.7.1 Pygments==2.2.0 Pelican Quickstart - make the site! We are now going to build the site! Exciting stuff. With the virtual environment and packages in place, we just need to make sure we are in a directory where we want our site to live. (staticsite)$ cd ~ (staticsite)$ cd Documents (staticsite)$ cd staticsite You can confirm you are working in the staticsite directory by typing pwd which stands for print working directory : (staticsite)$ pwd Now we can spin up the settings and structure of our pelican build. Start the process with the command: (staticsite)$ pelican-quickstart Pelican will ask us a bunch of questions at the start. The initial settings I used are below. Make sure to change the title, author, URL prefix, and timezone: > Where do you want to create your new web site? [.] > What will be the title of this web site? Python Programming for Undergraduate Engineers > Who will be the author of this web site? Peter D. Kazarinoff > What will be the default language of this web site? [English] > Do you want to specify a URL prefix? e.g., http://example.com (Y/n) Y > What is your URL prefix? (see above example; no trailing slash) https://username.github.io/staticsite > Do you want to enable article pagination? (Y/n) Y > How many articles per page do you want? [10] 5 > What is your time zone? [Europe/Paris] America/Los_Angeles > Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) Y > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? (Y/n) Y > Do you want to upload your website using FTP? (y/N) N > Do you want to upload your website using SSH? (y/N) N > Do you want to upload your website using Dropbox? (y/N) N > Do you want to upload your website using S3? (y/N) N > Do you want to upload your website using Rackspace Cloud Files? (y/N) N > Do you want to upload your website using GitHub Pages? (y/N) y > Is this your personal page (username.github.io)? (y/N) N Now we can create and view the site. The command make html will create the site and the command make serve will spin up a local webserver that allows us to view the site. (staticsite)$ make html (staticsite)$ make serve Point a web browser to: localhost:8000 The site will look something like: We can shut down the server by typing ctrl-c Write a first post in markup language (.md) Time to write our first post. The contents of the staticsite directory should look something like: staticsite/  develop_server.sh  fabfile.py  Makefile  pelicanconf.py  publishconf.py  content output __pycache__ To make a new post, we need to add a markup file (.md file) to the content folder. Let's call our first post first_post.md . Depending on the computer I'm using, I create .md files with different programs. One Windows 10, I have been using Code Writer. On Mac OSX and Linux, I use PyCharm . The first_post.md file saved in the content folder needs to have the form: Title : First Post Date : 2017 - 10 - 13 12 : 40 Modified : 2017 - 10 - 13 12 : 40 Status : published Category : This site Tags : python , markdown , blog Slug : first - post Authors : Peter D . Kazarinoff Summary : This is the summary of the first post This is the very first post ! After saving the file first_post.md in the content folder, the static site folder should look something like: staticsite/  develop_server.sh  fabfile.py  Makefile  pelicanconf.py  publishconf.py  content  first_post.md  output __pycache__ Let's build the site again and take a look at our new post (staticsite)$ make html (staticsite)$ make serve Again, point a web browser to: localhost:8000 The site will look something like: It works! I can see the first post! We can shut down the server by typing ctrl-c Add and commit the changes then push the changes to github When we are done editing the posts and the site, we add all of the changes to our local git repo using git add . Then we commit these changes with git commit and use the -m \"created pelcian static site\" flag to supply a commit message (make sure to use double quotes \"commit message\"). Push those changes up to github with git push origin master git add . git commit -m \"created pelican static site\" git push origin master In the next post , we will customize the site with a pelican-theme called pelican-bootstrap3 .","tags":"This site","url":"how-i-built-this-site-2.html"},{"title":"How I Build This Site - Part 1","text":"This is the first part of a multi-part series on how I built this site. In this first post, we'll review the development environment, installing Python, virtual environments, installing the necessary packages and setting up git. Setting up the development environment What does that even mean? Before I started building this site I needed to make sure the necessary software, packages and settings were in place. This starts with setting up a development environment . To me, a development environment is simply the software on a computer that allows me to get work done. In this case the development environment means the tools necessary to build this site. A list of the tools and technologies are below: An operating system: This website was built on machines running Windows 10, Mac OSX and Linux (Ubuntu 16.04 LTS). Python: Python 3.6 (legacy Python is version 2.7) Anaconda: The Anaconda Prompt and the Anaconda Python distribution Github.com and git: Four different computers were used build the site. Github is the central place where the most up-to-date version of the site is kept. Pelican: A static site generator written in Python. Pelican will build the site from a settings file (pelicanconf.py) and posts written in markdown (.md files) Markdown: the posts on this website are written in markdown, or .md format fabric and make: fabric is a Python package to automate builds and tasks. Fabric works on Windows. Make is used on MacOSX and Linux to build the site and serve up a demo version of the site. Installing Python (the Anaconda distribution) I highly recommend installing the Anaconda distribution of Python. I have the Anaconda distribution installed on all the computers I used to build this site: Windows 10 Desktop at work Mac OSX laptop at work Mac OSX laptop at home Linux (Ubuntu 16.04 LTS) Desktop at home. An advantage of using the Anaconda distribution of Python is that each of these four computers will have the same stable version of Python. Download the latest release at: https://www.anaconda.com/download The Anaconda distribution of Python comes with an up-to-date and stable version of Python (Python 3.6) and the Anaconda Prompt . The Anaconda Prompt is useful because it allows me to run shell commands on my Windows 10 machine at work. These are the same commands I would run in the terminal on my Mac OSX or Linux boxes. The Anaconda Prompt makes creating and logging into virtual environments easy and allows me to use git on Windows 10. Follow this post to install the Anaconda on Windows 10. Create a new virtual environment Before downloading Pelican, we should create a new virtual environment. The same virtual environment on each of the computers I use means the same packages are installed on each computer. To create a new virtual environment, open up the Anaconda Prompt (on Windows) or a terminal (on Linux or Mac OSX). Let's call our new virtual environment staticsite . The conda create command creates the environment and the -n staticsite flag adds the name. $ conda create -n staticsite This creates a new virtual environment called staticsite . Becuase I use four different computers to work on the site, I need to make sure the same Python packages are installed on each computer. Using a virtual environment ensures this consistency. Once the virtual environment staticsite is created, we need to activate it and start using it with the command: On Mac OSX or Linux $ source activate staticsite or on Windows 10 activate staticsite We should now see (staticsite) before the terminal prompt. This means we are using the (staticsite) virtual environment. Install Pelican Time for some fun! Installing Pelican. Pelican is a Python package that creates static websites. Static sites are websites that only have html, css and javascript. A static site is not connected to a database and there is no code run on the server side. The server just serves static files (html, css, javascript) to the client when the client requests them. In order to install Pelican, we need to install pip first. On my Linux and Mac OSX machines, the command line tool git is already installed. If using Windows 10, git may not be available. To keep the development environments the same when using Windows 10, you will also need conda to install git . (staticsite)$ conda install pip (staticsite)$ conda install git Once pip is installed, we can install pelican and markdown . These are two of the core pieces we need to build the website. fabric3 is also installed because we'll use it to build and demo the site on Windows. bs4 is the beautiful soup package. Some of the pelican plugins to be installed later will depend on bs4 to function properly. (staticsite)$ pip install pelican (staticsite)$ pip install markdown (staticsite)$ pip install fabric3 (staticsite)$ pip install bs4 (staticsite)$ pip install ghp-import Create a github account and create a new repository While I was making the site, it became clear that I had to keep track of version control . I would make some changes to the site on my computer at work, then come home and make more changes to the site. Bringing a USB thumb drive back and forth was hard. I would forget the thumb drive at work or home and then could not edit the site. Or worse, I'd edit the site in both places and try to remember which changes were made where and which was the best version. Ah! The solution is to use git and github . Git is a command line utility that assists with version control. Using git means changes made to files on one computer can be synced with the same files on another computer. Github.com is the where the site content and settings are remotely stored and integrates easily with git. To sign up for a github.com account go here: https://github.com/join The account activation screen looks something like: Once the account is set up, log in and create a new repository. Use the + button on the upper right-hand menu: I named the new repository: staticsite and included both a README.md and a GNU General Public License v3.0 . Make a directory for the site and link it to github Once the github repo (short for repository, basically a folder with files on github.com) is set up, the last step to complete the development environment is to link the remote repo on github to the local version of the site on my computer. The local version is in a folder call staticsite in the Documents folder. The staticsite folder will contain all the files used to build the site and the output files created by Pelican that are the site. (staticsite)$ cd ~ (staticsite)$ cd Documents (staticsite)$ mkdir staticsite (staticsite)$ cd staticsite We can set up git to keep the contents of the local staticsite folder in sync with the contents of the staticsite repo on github.com. The command git init will initiate or create the local repository. The command git remote add origin followed by the url of our github repo links local folder to the remote repo on github. Note the web address ends in .git . If you are following along and want to build your own static site, make sure to change username to your github username and reponame to your github repo name. git init git remote add origin https://github.com/username/reponame.git Now for the git magic. On github.com we have a README.md file and a licence. But the local staticsite folder on the computer is empty. So the two folders aren't in sync. To make the contents of each folder identical, we pull the files from github onto the local computer. A pull \"pulls\" or gets the files from github and copies them to the local staticsite folder. (staticsite)$ git pull origin master If you look in the local staticsite folder you should now see the following two files: staticsite  LICENSE  README.md The development environment is set! On to building the site! Now each time I work on the site, I navigate to the staticsite folder on whatever computer I am using. Before any editing, I key in the command: git pull origin master After the pull , the staticsite folder is up-to-date with the newest version of all the files on github. Then I go about editing files, writing posts, changing settings, etc. After the edits, the last thing I do before shutting down the computer for the day is add all the changes to git with git add . (note there is a space between the add and the period . ). Then commit those changes locally with the line git commit -m \"commit message\" (note there are double quotes \"commit message\" used around the commit message), and finally push the changes up to github.com with git push origin master . Now the version of the site up on github.com is the same as the version of the site on my local machine. git add . git commit -m \"commit message\" git push origin master This ensures all of my computers and the github repo contain the same version of the site. In the next post we will use the pelican-quickstart command to get the blog off the ground, write our first post and view a demo version of the site.","tags":"this site","url":"how-i-built-this-site-1.html"},{"title":"Create a new virtual environment with Conda","text":"To start a new Python project, it is best practice to create a new virtual environment. I have the Anaconda distribution of Python installed on my Windows 10 machine at work. When you install Anaconda , it comes with the very useful Anaconda Prompt . Using the Anaconda Prompt is a bit like using the terminal on a Mac OSX or Linux. To start the Anaconda Prompt on Windows 10, go to the Windows start button on the lower left and select Anaconda Prompt . To create the new environment, we need to issue the following command at the prompt: conda create --name webscrape python=3.6 The conda create command builds the new virtual environment. The --name webscrape flag gives our new virtual environment the name webscrape . I like to name my virtual environments the same name as the project that I will use that environment for, or after the name of the package that will be used the most. Including python=3.6 ensures that your virtual environment has an up to date version of python. Conda will tell us: The following NEW packages will be INSTALLED: certifi: 2016.2.28-py36_0 pip: 9.0.1-py36_1 python: 3.6.2-0 setuptools: 36.4.0-py36_0 vs2015_runtime: 14.0.25420-0 wheel: 0.29.0-py36_0 wincertstore: 0.2-py36_0 Proceed ([y]/n)? y Type y to confirm that you want to create the new virtual environment. To use the new virtual environment webscrape you need to activate it by typing: activate webscrape You know you are in your virtual environment webscrape when (webscrape) is in parenthesis at the start of the prompt: (webscrape) tribilium@Den-PC:~$ To deactivate an active environment, use: deactivate For power-users using the terminal on Mac or Linux, you must use source activate to enter into the virtual environment. source activate webscrape and source deactivate webscrape If you see the (webscrape) in parenthesis before the command prompt, that means you set up the new virtual environment and are now using it. You can view a list of your virtual environments using the conda info --envs or conda env list command. conda info --envs # conda environments: # matplotlib /home/tribilium/anaconda3/envs/matplotlib webscrape * /home/tribilium/anaconda3/envs/pelican root /home/tribilium/anaconda3 Notice the * asterisk on the line with webscrape . The virtual environment with the * is currently active. To exit the virtual environment, use the command deactivate . If you run conda env list again, you'll see there is no * in front of webscrape . That's because you are not currently in that virtual environment. conda env list # conda environments: # matplotlib /home/tribilium/anaconda3/envs/matplotlib webscrape /home/tribilium/anaconda3/envs/pelican root * /home/tribilium/anaconda3 We can spin up the virtual environment again with the activate command. Want to use pip and virtualenv / virtuelenv wrapper instead? Great. Those will work too. For windows and computers with locked down active directories, I prefer conda . It just seems to make things easier. For a great post about the differences between conda and pip see this post by Jake VanderPlas.","tags":"Orientation","url":"new-virtual-environment-with-conda.html"},{"title":"Installing Anaconda on Windows","text":"In this post, we will run through installing the Anaconda distribution of Python on Windows 10. I think the Anaconda distribution of Python is the the best option for undergraduate engineers who want to use Python. Anaconda is free (although the download is large which can take time) and can be installed on school or work computers where you don't have administrator access or the ability to install new programs. Steps: Visit Anaconda.com/downloads Select Windows Download the .exe installer Open and run the .exe installer Open the Anaconda Prompt and run some Python code 1. Visit the Anaconda downloads page Go to the following link: Anaconda.com/downloads The Anaconda Downloads Page will look something like this: 2. Select the Windows Select Windows where the three opperating systems are listed. 3. Download Download the Python 3.6 distribution. Python 2.7 is legacy Python. For undergraduate engineers, select the Python 3.6 version. If you are unsure about a 32-bit version vs a 64-bit version, most Windows installations are 64-bit. You may be prompted to enter your email. You can still download Anaconda if you click [No Thanks] and don't enter your Work Email address. The download is quite large (over 500 MB) so it may take a while for the download to complete. 4. Open and run the installer Once the download completes, open and run the .exe installer At the beginning of the install you will need to click [Next] to confirm the installation, and agree to the license. At the Advanced Installation Options screen, I recommend that you do not check \"Add Anaconda to my PATH environment variable\" 5. Open the Anaconda Prompt from the Windows start menu After the Anaconda install is complete, you can go to the Windows start menu and select the Anaconda Prompt This will open up the Anaconda Prompt, which is often called the Conda prompt . Anaconda is the Python distribution and the Conda prompt is a command line tool (a program where you type in your commands instead of using a mouse). It doesn't look like much, but it is really helpful for an undergraduate engineer using Python. At the Anaconda Prompt, type python . This will start the Python interperater. Note the Python version. You should see something like Python 3.6.1 . With the interperter running, you will see a set of greater-than symbols >>> before the cursor. Now you can type Python commands. Try typing import this . You should see the Zen of Python by Tim Peters To close the Python interperater, type exit() at the interperator prompt >>> . Note the double parenthesis at the end of the command. The () is needed to stop the Python interperator and get back out to the Conda prompt. To close the Conda prompt, you can either close the window with the mouse, or type exit . Congratulations! You installed the Anaconda distribution on your Windows computer! When you want to use the Python interperater again, just click the Windows Start button and select the Anaconda Prompt and type python .","tags":"Orientation","url":"installing-anaconda-on-windows.html"}]}